[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estatística Computacional I",
    "section": "",
    "text": "Preface\nEste material foi criado para a disciplina Estatística Computacional I, do Curso de Bacharelado em Estatística, da Universidade Federal do Amazonas.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html",
    "href": "solucoes_caso_real.html",
    "title": "1  Solução de equações",
    "section": "",
    "text": "1.1 Método da bisseção\nO método da bisseção é uma ferramenta versátil para encontrar raízes. Antes de apresentá-lo, é importante conhecer o Teorema do Valor Intermediário (ou Teorema de Bolzano).\nObserve que a condição do Teorema do Valor Intermediário pode ser reescrita para como \\[g(a)=f(a)-d \\leq 0 \\leq g(b)=f(b)-d,\\] (ou \\(g(b)\\leq 0\\leq g(a)\\)), o que implica que \\(c\\) é raíz de \\(g(x)\\). Temos o seguinte corolário.\nPortanto, se conhecemos um intervalo \\([a,b]\\) tal que \\(f(a)\\) e \\(f(b)\\) tem sinais opostos, então temos a certeza de que há pelo menos uma raíz de \\(f(x)\\) dentro do intervalo \\([a,b]\\). A Figure 1.1 ilustra o Teorema do Valor Intermediário, considerando o intervalo $[a_0,b_0]$$ e a raiz \\(x^\\star\\).\nFigure 1.1: Ilustração do Teorema do Valor Médio\nO método da bisseção é baseado em aplicações sucessivas do Teorema do Valor Intermediário. Suponha que conhecemos um intervalo \\([a_0,b_0]\\) tal que \\(f(a_0)\\) e \\(f(b_0)\\) possuem sinais opostos e que contém a única raiz \\(x^\\star\\). Considere como candidato à solução o valor\n\\[x_1=\\frac{a_0+b_0}{2},\\]\nou seja, o ponto médio do intervalo. Portanto, podemos escrever \\([a_0,b_0]=[a_0,x_1]\\cup[x_1,b_0]\\). Ao calcular \\(f(x_1)\\), três coisas podem acontecer:\nA figura abaixo mostra a Figure 1.1 novamente, agora com o ponto \\(x_1\\) em vermelho. Também em vermelho está representado o novo intervalo, com metade do comprimento do anterior, que contém a raiz e satisfaz as condições do Teorema do Valor Intermediário.\nFigure 1.2: Obtenção de um novo intervalo para o Teorema do Valor Intermediário\nComo ilustrado acima, se o procedimento não encontra a solução, ele ao menos elimina metade do espaço de busca. Podemos então definir o novo candidato a solução \\[x_2=\\frac{a_1+b_1}{2},\\] isto é, ponto médio do novo intevalo e verificar se a raiz está no intervalo \\([a_1,x_2]\\) ou \\([x_2,b_1]\\). Ao realizar esse procedimento \\(n\\) vezes, teremos a sequência\n\\[[a_0,b_0]\\supset [a_1,b_1]\\supset\\cdots\\supset [a_{n},b_{n}],\\] onde o intervalo \\([a_n,b_n]\\) contém a raiz \\(x^\\star\\). O comprimento deste intervalo é \\[\\frac{b_0-a_0}{2^n}\\] o que implica que \\[|x_{n}-x^\\star|&lt;\\frac{b_0-a_0}{2^n}=\\varepsilon_n,\\] onde \\(\\varepsilon_n\\) é o erro máximo da aproximação. É imediato que \\(\\lim_{n\\rightarrow \\infty}\\varepsilon_n=0\\), o que implica que o método converge para a solução. Abaixo, apresentamos o algoritmo do método da bisseção.\nO método da bisseção consiste em realizar as sucessivas divisões de intervalos até a iteração \\(n\\) que satifaz o erro máximo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-da-bisseção",
    "href": "solucoes_caso_real.html#método-da-bisseção",
    "title": "1  Solução de equações",
    "section": "",
    "text": "Teorema do Valor Intermediário. Seja \\(f\\) uma função real definida no intervalo \\([a,b]\\). Então, se \\(f(a)\\leq d \\leq f(b)\\) ou \\(f(b)\\leq d \\leq f(a)\\), existe \\(c\\in[a,b]\\) tal que \\(f(c)=d\\).\n\n\n\nCorolário. Seja \\(f\\) uma função real definida no intervalo \\([a,b]\\). Então, se \\(f(a)\\) e \\(f(b)\\) possuem sinais opostos, então existe \\(x^\\star\\in[a,b]\\) tal que \\(x^\\star\\) é raíz de \\(f(x)\\).\n\n\n\n\n\n\n\n\\(f(x_1)=0\\). Nesse caso, \\(x_1\\) é a solução do problema.\n\\(f(x_1)\\) e \\(f(a_0)\\) tem o mesmo sinal. Isso implica que \\(f(x_1)\\) e \\(f(b_0)\\) possuem sinais opostos e, portanto, a solução deve estar em \\([x_1,b_0]\\). Podemos então definir o novo intervalo de busca \\([a_1,b_1]\\) onde \\(a_1=x_1\\) e \\(b_1=b_0\\).\n\\(f(x_1)\\) e \\(f(b_0)\\) tem o mesmo sinal. Isso implica que \\(f(x_1)\\) e \\(f(a_0)\\) possuem sinais opostos e, portanto, a solução deve estar em \\([a_0,x_1]\\). Podemos então definir o novo intervalo de busca \\([a_1,b_1]\\) onde \\(a_1=a_0\\) e \\(b_1=x_1\\).\n\n\n\n\n\n\nAlgoritmo 1 (Método da Bisseção).\nPara encontrar uma raiz de \\(f(x)\\) no intervalo \\([a,b]\\) com uma tolerância de \\(\\varepsilon\\):\nPré-condição: \\(f(a)f(b)&lt;0.\\)\n\nDefina a tolerância \\(\\varepsilon&gt;0.\\) e faça \\(i=1\\)\nEnquanto \\(∣b−a|\\geq\\varepsilon\\) repita os seguintes passos:\n\nCalcule \\(x_i=(a+b)/2\\)\nSe \\(f(x_i)=0\\), pare e retorne \\(x^\\star=x_i\\).\nAtualização do intervalo\n\n\nSe \\(f(x_i)f(a)&lt;0\\), a raiz está o intervalo \\([a,x_i]\\). Faça \\(b=x_i\\)\nSenão, a raiz está no intervalo \\([x_i,b]\\). Faça \\(a=x_i\\)\nFaça \\(i=i+1\\)\n\n\n\n\n\nExample 1.1 Seja \\[f(x)=x^2−x-1\\] Vamos encontrar a raiz de \\(f\\) utilizando o método da bisseção. Observe que \\(f(1)=-1\\) e \\(f(2)=5\\) e, pelo Teorema do Valor Intermediário, há pelo menos uma raiz no intervalo [1,2]. O erro máximo na \\(n\\)-ésima iteração é\n\\[\\varepsilon_n=\\frac{1}{2^n}.\\] Escolhendo \\(n=15\\) temos \\(\\varepsilon_{15}=0.00003\\) (ou seja, uma precisão de quatro casas decimais). A tabela abaixo apresenta os intervalos e os pontos médios (soluções) para cada iteração.\n\n\n [1] 1.000000 1.500000 1.500000 1.500000 1.562500 1.593750 1.609375 1.617188\n [9] 1.617188 1.617188 1.617188 1.617676 1.617920 1.617920 1.617981 1.618011\n\n\n\n\n\nIteração\nSolução\nErro\n[a\nb]\n\n\n\n\n1\n1.500000\nInf\n1.500000\n2.000000\n\n\n2\n1.750000\n0.250000\n1.500000\n1.750000\n\n\n3\n1.625000\n0.125000\n1.500000\n1.625000\n\n\n4\n1.562500\n0.062500\n1.562500\n1.625000\n\n\n5\n1.593750\n0.031250\n1.593750\n1.625000\n\n\n6\n1.609375\n0.015625\n1.609375\n1.625000\n\n\n7\n1.617188\n0.007812\n1.617188\n1.625000\n\n\n8\n1.621094\n0.003906\n1.617188\n1.621094\n\n\n9\n1.619141\n0.001953\n1.617188\n1.619141\n\n\n10\n1.618164\n0.000977\n1.617188\n1.618164\n\n\n11\n1.617676\n0.000488\n1.617676\n1.618164\n\n\n12\n1.617920\n0.000244\n1.617920\n1.618164\n\n\n13\n1.618042\n0.000122\n1.617920\n1.618042\n\n\n14\n1.617981\n0.000061\n1.617981\n1.618042\n\n\n15\n1.618011\n0.000031\n1.618011\n1.618042",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-newton-raphson",
    "href": "solucoes_caso_real.html#método-newton-raphson",
    "title": "1  Solução de equações",
    "section": "1.2 Método Newton-Raphson",
    "text": "1.2 Método Newton-Raphson\nConsidere o problma de encontrar uma raiz de \\(f(x)\\). Seja \\(x_0\\) um ponto próximo da raiz \\(x^\\star\\). A equação da reta tangente ao ponto \\(x_0\\) é dada por\n\\[f(x)=f(x_0)+(x-x_0)f'(x_0).\\] A Figure 1.3 ilustra a reta tangente (em vermelho) ao ponto \\(x_0\\) para uma certa função. Observe que a raiz da reta tangente, marcada como \\(x_1\\) no gráfico, está mais próxima de \\(x^\\star\\) do que o ponto \\(x_0\\).\n\n\n\n\n\n\n\n\nFigure 1.3: Obtenção de um novo intervalo para o Teorema do Valor Intermediário\n\n\n\n\n\nA raiz \\(x_1\\) é calculada do seguinte modo: \\[0=f(x_0)+(x_1-x_0)f'(x_0)\\Rightarrow x_1=x_0-\\frac{f(x_0)}{f'(x_0)}.\\]\nPodemos então encontrar a reta tangente ao ponto \\(x_1\\). Por sua vez, a raiz desta reta, denominada por \\(x_2\\), estará mais próxima de \\(x^\\star\\) do que \\(x_1\\). Após realizar o mesmo procedimento \\(n\\) vezes teremos\n\\[x_n=x_{n-1}-\\frac{f(x_{n-1})}{f'(x_{n-1})}\\]\nIntuitivamente, temos que \\(x_n\\) deve estar mais próximo de \\(x^\\star\\) na medida que \\(n\\rightarrow\\infty\\). Abaixo, segue o algoritmo do método de Newton-Raphson.\n\nAlgoritmo 2. (Método de Newton-Raphson)\nPara encontrar a raiz \\(x^\\star\\) de \\(f(x)\\) com um erro \\(\\varepsilon\\).\nPré-condição: \\(f(x)\\) é diferenciável com \\(f'(x)\\neq 0\\) para qualquer \\(x\\) na vizinhança de \\(x^\\star\\). É necessário um valor \\(x_0\\) próximo de \\(x^\\star\\)\nDefina \\(x_{atual}=x_0\\) e erro=\\(+\\infty\\)\n\nEnquanto o erro for maior que \\(\\varepsilon\\) repita os seguintes passos:\n\nCalcule \\(x_m=x_{atual}-\\frac{f(x_{atual})}{f'(x_{atual})}\\)\nCalcule erro\\(=|x_m-x_{atual}|\\) e faça \\(x_{atual}=x_m\\).\n\nRetorne \\(x^\\star=x_m\\).\n\n\nO método\n\nTeorema de Kantorovich (simplificado) Seja \\(x_0\\) um ponto inicial para o método Newton-Raphson. A convergência para a raiz de \\(f(x)\\) é garantida se:\n\n\\(f'(x_0)\\neq 0\\)\nExiste uma constante \\(L\\) que limita a segunda derivada, ou seja \\(|f''(x_0)|\\leq L\\) em algum intervalo contento \\(x_0\\).\nA seguinte condição é satisfeita: \\[h_0=\\frac{|f(x_0)|}{f'(x_0)^2}L\\leq \\frac{1}{2}.\\]\n\n\n\nMétodos de quase-Newton.\nUma das desvantagens do método de Newton-Raphson é a necessidade de obter uma expressão analítica para a derivada de \\(f(x)\\). Qualquer método que utilize a mesma estrutura do Newton-Raphson mas troca a forma analítica de \\(f'(x)\\) por uma aproximação pertence à classe de métodos de quase-Newton.\n\n.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-da-secante",
    "href": "solucoes_caso_real.html#método-da-secante",
    "title": "1  Solução de equações",
    "section": "1.3 Método da secante",
    "text": "1.3 Método da secante\nSeja \\(x^\\star\\) a raiz de \\(f(x)\\) e sejam \\(x_0\\) e \\(x_1\\) dois valores próximos de \\(x^\\star\\). A reta secante aos pontos \\((x_0,f(x_0))\\) e \\((x_1,f(x_1))\\) é dada por \\[f(x)=f(x_1)+ (x-x_1)\\frac{f(x_1)-f(x_0)}{x_1-x_0}.\\]\nAssim como no métod Newton-Raphson, a raiz da reta secante acima tende a estar mais próxima de \\(x^\\star\\) do que \\(x_0\\) e \\(x_1\\). A Figure 1.4 ilustra essa afirmação, onde o ponto \\(x_2\\) é a raiz da reta secante (em vermelho).\n\n\n\n\n\n\n\n\nFigure 1.4: Obtenção de um novo pontos utilizando a reta secante formada a partir de dois pontos iniciais.\n\n\n\n\n\nO ponto \\(x_2\\) é obtido do seguinte modo:\n\\[0=f(x_1)+(x_2-x_1)\\frac{f(x_1)-f(x_)}{x_1-x_0}\\Rightarrow x_2=x_1-f(x_1)\\frac{x_1-x_0}{f(x_1)-f(x_0)}.\\] De modo análogo, podemos construir a reta secante aos pontos \\((x_1,f(x_1))\\) e \\((x_2,f(x_2))\\) e encontrar sua raiz \\(x_3\\). Ao realizar esse procedimento diversas vezes, teremos que o ponto \\(x_n\\) será dado por\n\\[ x_n=x_{n-1}-f(x_{n-1})\\frac{x_{n-1}-x_{n-2}}{f(x_{n-1})-f(x_{n-2})}.\\] Podemos continuar a iteração até obter \\(|x_n-x_{n-1}|&lt;\\varepsilon\\).\n\nAlgoritmo 3. (Método da secante)\nPara encontrar a raiz \\(x^\\star\\) de \\(f(x)\\) com um erro \\(\\varepsilon\\).\nPré-condição: São necessários valores \\(x_0\\) e \\(x_1\\) próximos de \\(x^\\star\\)\nDefina erro=\\(+\\infty\\) e \\(n=1\\)\n\nEnquanto o erro for maior que \\(\\varepsilon\\) repita os seguintes passos:\n\n\\(n=n+1\\)\nCalcule \\(x_n=x_{n-1}-f(x_{n-1})\\frac{x_{n-1}-x_{n-2}}{f(x_{n-1})-f(x_{n-2})}\\)\nCalcule erro\\(=|x_n-x_{n-1}|\\).\n\nRetorne \\(x^\\star=x_n\\).\n\n\n\nExemplo - (W de Lambert) Considere o problema de encontrar a raiz de \\(f(x)= e^{-x}-x\\). O resultado, conhecido como constante de \\(W\\) de Lambert, é um número irracional e é aproximadamente 0,567143.\nVamos escolher os pontos iniciais \\(x_0=0\\) e \\(x_1=1\\), onde \\[\\begin{align}f(0)&=e^{-0}-0=1\\\\f(1)&=e^{-1}-1\\approx-0,6321.\\end{align}\\] A fórmula de iteração é dada por\n\\[x_n=x_{n-1}-(e^{-x_{n-1}}-1)\\frac{x_{n-1}-x_{n-2}}{e^{-x_{n-1}}-x_{n-1}-e^{-x_{n-2}}+x_{n-2}}.\\]\nVamos fixar o erro em \\(10^{-7}\\) para obter uma precisão de 6 casas decimais.\n\n\n\n\n\nIterações\nx\nErro\n\n\n\n\n1\n0.612700\n0.387300\n\n\n2\n0.563838\n0.048861\n\n\n3\n0.567170\n0.003332\n\n\n4\n0.567143\n0.000027\n\n\n5\n0.567143\n0.000000\n\n\n\n\n\n\nFica como exercício provar que o método da secante pertence à classe dos métodos de quase-Newton. Assim como o método de Newton-Raphson, esse método tem boas propriedades para funções duas vezes continuamente diferenciáveis, como mostra o teorema abaixo.\n\nTeorema da convergência do método da secante. Seja \\(f\\) uma função real, contínua e duas vezes diferenciável no intervalo \\([a,b]\\), onde \\(x^\\star\\in(a,b)\\) é a única raiz nesse intervalo e \\(f'(x^\\star)\\neq 0\\)\nSe as soluções iniciais \\(x_0\\) e \\(x_1\\) forem escolhidas suficientemente próximas de \\(x^\\star\\) então a sequência \\(x_2,\\ldots,x_n\\) produzida pelo método da secante irá convergir para \\(x^\\star\\).\n\n\nAtenção. O teorema acima mostra que não basta escolher dois valores dentro do intervalo \\([a,b]\\) que contém \\(x^\\star\\). Ou seja, diferente do método da bisseção, essa escolha não é garantia de convergência. Isso ocorre porque, dependendo da função, a escolha dos pontos iniciais pode levar a geração de candidatos para fora do intervalo \\([a,b]\\) fazendo com que a convergência demore ou mesmo falhe.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-da-interpolação-quadrática-inversa-método-de-muller",
    "href": "solucoes_caso_real.html#método-da-interpolação-quadrática-inversa-método-de-muller",
    "title": "1  Solução de equações",
    "section": "1.4 Método da interpolação quadrática inversa (Método de Muller)",
    "text": "1.4 Método da interpolação quadrática inversa (Método de Muller)\nO método da interpolação quadrática inversa segue a mesma ideia do método da secante. Considere três pontos, \\(x_1,x_2,x_3\\) próximos de \\(x^\\star\\). Na \\(i\\)-ésima iteração, o próximo candidato à solução é a raiz da parábola que passa pelos pontos \\(x_{i-1},x_{i-2},x_{i-3}\\), dada por\n\\[x_{i} = x_{i-1} - \\frac{2f(x_{i-1})}{\\omega \\pm \\sqrt{\\omega^2 - 4f(x_{i-1})\\delta_{i-1}}}\\] onde \\[\\begin{align}\n\\delta_{i-1} &= \\frac{f(x_{i-1})-f(x_{i-2})}{x_{i-1}-x_{i-2}} + \\frac{f(x_{i-1})-f(x_{i-3})}{x_{i-1}-x_{i-3}} - \\frac{f(x_{i-2})-f(x_{i-3})}{x_{i-2}-x_{i-3}}\\\\\n\\omega &= \\frac{f(x_{i-1})-f(x_{i-2})}{x_{i-1}-x_{i-2}} + \\delta_{i-1}(x_{i-1}-x_{i-2})\n\\end{align}\\]\nEmbora esse método possa convergir rapidamente para a raiz, os pontos iniciais devem estar próximos da raiz. Além disso, o algoritmo pode falhar se em algum momento os valores de \\(f(x_i),f(x_{i-1})\\) ou \\(f(x_{i-2})\\) concidirem. Portanto, é recomendado que esse método seja utilizado em conjunto com outro, conforme discutido na seção Métodos Híbridos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-da-falsa-posição-regula-falsi",
    "href": "solucoes_caso_real.html#método-da-falsa-posição-regula-falsi",
    "title": "1  Solução de equações",
    "section": "1.5 Método da falsa posição (regula falsi)",
    "text": "1.5 Método da falsa posição (regula falsi)\nO método da falsa posição é uma combinação do método da secante com o método da bisseção. Assim como neste último, é necessário começar com um intervalo \\([x_a,x_b]\\) no qual \\(f(x_a)\\) e \\(f(x_b)\\) têm sinais opostos.\nPrimeiro, calcula-se a raiz da reta secante que passa pelos pontos \\((x_a,f(x_a))\\) e \\((x_b,f(x_b))\\), dada por\n\\[\nx_c=x_b-f(x_b)\\frac{x_b-x_a}{f(x_b)-f(x_a)}.\n\\] Em seguida, verifica-se qual dos intervalos \\([x_a,x_c]\\) ou \\([x_c,x_b]\\) contém a raiz. Então, repete-se a busca.\nAssim como ocorre no método da bisseção, o método da falsa posição cria uma sequência de intervalos encaixados que contém a raiz, convergindo portanto para a verdadeira solução. Espera-se que a escolha de \\(x_c\\) esteja mais próxima da raiz do que o ponto médio do intervalo.\n\nAlgoritmo 4 (Método da Falsa Posição).\nPara encontrar uma raiz de \\(f(x)\\) no intervalo \\([a,b]\\) com uma tolerância de \\(\\varepsilon\\):\nPré-condição: \\(f(a)f(b)&lt;0.\\)\n\nDefina a tolerância \\(\\varepsilon&gt;0.\\) e faça \\(i=1\\)\nEnquanto \\(∣b−a|\\geq\\varepsilon\\) repita os seguintes passos:\n\nCalcule \\(x_i=b - f(b)\\frac{b-a}{f(b)-f(a)}\\)\nSe \\(f(x_i)=0\\), pare e retorne \\(x^\\star=x_i\\).\nAtualização do intervalo\n\n\nSe \\(f(x_i)f(a)&lt;0\\), a raiz está o intervalo \\([a,x_i]\\). Faça \\(b=x_i\\)\nSenão, a raiz está no intervalo \\([x_i,b]\\). Faça \\(a=x_i\\)\nFaça \\(i=i+1\\)\n\n\n\n\nExemplo Seja \\[f(x)=x^2−x-1\\] Anteriormente, encontramos a raiz de \\(f\\), iniciando com o intervalo [1,2] e fixando erro de \\(0,00003\\), que foi obtido em 15 iterações. A tabela abaixo apresenta os intervalos e os pontos médios (soluções) para o método da falsa posição até a obtenção do mesmo erro fixado no método da bisseção.\n\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\n\n\n\nIteração\nSolução\nErro\n[a\nb]\n\n\n\n\n1\n1.800000\n1.000000\n1\n1.800000\n\n\n2\n1.712871\n0.087129\n1\n1.712871\n\n\n3\n1.669659\n0.043212\n1\n1.669659\n\n\n4\n1.646784\n0.022875\n1\n1.646784\n\n\n5\n1.634245\n0.012539\n1\n1.634245\n\n\n6\n1.627238\n0.007007\n1\n1.627238\n\n\n7\n1.623280\n0.003958\n1\n1.623280\n\n\n8\n1.621031\n0.002249\n1\n1.621031\n\n\n9\n1.619748\n0.001283\n1\n1.619748\n\n\n10\n1.619015\n0.000733\n1\n1.619015\n\n\n11\n1.618596\n0.000419\n1\n1.618596\n\n\n12\n1.618356\n0.000240\n1\n1.618356\n\n\n13\n1.618218\n0.000137\n1\n1.618218\n\n\n14\n1.618140\n0.000079\n1\n1.618140\n\n\n15\n1.618094\n0.000045\n1\n1.618094",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#métodos-híbridos",
    "href": "solucoes_caso_real.html#métodos-híbridos",
    "title": "1  Solução de equações",
    "section": "1.6 Métodos híbridos",
    "text": "1.6 Métodos híbridos\nOs métodos discutidos até o momento possuem vantagens e desvantagens, conforme sumarizado na tabela abaixo.\n\n\n\n\n\n\n\n\n\nMétodo\nOrdem de Convergência\nVantagem\nDesvantagem\n\n\n\n\nBisseção\nLinear\nGarante a convergência. Não exige o cálculo da derivada.\nA convergência é lenta.\n\n\nFalsa Posição\nLinear\nGeralmente converge mais rápido que a bisseção. Não exige o cálculo da derivada.\nPode ter convergência lenta se a função for muito côncava ou convexa.\n\n\nSecante\nSuperlinear\nConvergência rápida, sem precisar calcular a derivada.\nPode divergir se a aproximação inicial for ruim.\n\n\nInterpolação Quadrática Inversa\nSuperlinear\nGeralmente mais rápido que o método da secante. Pode encontrar raízes complexas.\nExige três pontos iniciais. Pode ser instável e falhar se os pontos forem colineares.\n\n\nNewton-Raphson\nQuadrática\nConvergência extremamente rápida (se as condições forem atendidas).\nExige o cálculo da derivada. Pode divergir se a aproximação inicial for ruim.\n\n\n\nOs métodos híbridos são algoritmos que decidem em cada iteração qual método utilizar. Os objetivos dessas decisões são acelerar e garantir a convergência. Vamos apresentar a ideia por trás de dois métodos: Dekker e Brent.\nO método de Dekker (1969) combina o método da bisseção com o da secante. sua ideia básica é utilizar o método da secante sempre que possível, criando verificações lógicas que evitem que o candidato à solução se afaste da raiz. Ele começa com um intervalo \\([a,b]\\) que contém a raiz \\(x^\\star\\) (ou seja, \\(f(a)\\) e \\(f(b)\\) tem sinais opostos). Na sua \\(i\\)-ésima iteração:\n\nGeração dos candidatos. Dois candidatos são construídos, utilizando o método da bisseção e o da secante:\n\n\\[\\begin{align}x_m&=\\frac{a_{i-1}+b_{i-1}}{2}\\\\x_s&=b_{i-1}-f(b_{i-1})\\frac{b_{i-1}-a_{i-1}}{f(b_{i-1})-f(a_{i-1})} \\end{align}\\] 2. Freio da secante. Se \\(x_s\\in[x_m,b_{i-1}]\\), então o ponto da secante não se afastou da solução e fazemos \\(b_{i}=x_s\\). Senão, escolhemos \\(b_i=x_m\\) por segurança. O objetivo desse passo é evitar utilizar um ponto \\(x_s\\) afastado da solução.\n\nDefinindo o novo intervalo. Se \\(f(a_{i-1})\\) e \\(f(b_i)\\) tem sinais oposto, então \\(a_i=a_{i-1}\\). Em caso contrário, \\(a_i=b_{i-1}\\)\nPermuta Se \\(|f(a_i)|&lt;|f(b_i)|\\), então é provável que \\(a_i\\) seja um palpite melhor que \\(b_i\\). Então os valores \\(a_i\\) e \\(b_i\\) são permutados. Isso garante que a solução será dada pel convergência da sequência \\(b_1,b_2,\\ldots\\)\n\nO método de Brent (1973) é uma modificação do método de Dekker, començando também com um intervalo \\([a,b]\\) que contém a raiz. A principal diferença é que Brent adiciona mais verificações para garantir que a interpolação não apenas se aproxime da raiz, mas o faça de forma eficiente. Ele utiliza a interpolação quadrática inversa (que é mais rápida que a secante) e recorre à bisseção quando a interpolação não faz um progresso significativo.\nEm geral, o método itera nos seguintes passos:\n\nGeração do candidato: O algoritmo tenta encontrar um novo ponto, \\(x_s\\), usando a interpolação quadrática inversa. Se essa interpolação não for possível, ele recorre ao método da secante.\nTestes de segurança: Antes de aceitar \\(x_s\\), ele é submetido a uma série de testes que o comparam com a garantia de progresso da bisseção. Se \\(x_s\\) não for significativamente melhor do que um passo de bisseção, o algoritmo descarta \\(x_s\\) e, em vez disso, usa o ponto médio \\(x_m\\) como candidato.\nAtualização do intervalo. O novo intervalo é atualizado com base no candidato aceito (\\(x_s\\) ou \\(x_m\\)), garantindo que a raiz continue delimitada e que o ponto com o menor valor de \\(f(x)\\) seja sempre a melhor estimativa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#funções-para-encontrar-raízes-no-r",
    "href": "solucoes_caso_real.html#funções-para-encontrar-raízes-no-r",
    "title": "1  Solução de equações",
    "section": "1.7 Funções para encontrar raízes no R",
    "text": "1.7 Funções para encontrar raízes no R\nO R oferece várias funções para encontrar raízes de equações não-lineares, tanto no pacote base (stats) quanto em pacotes externos.\n\n1.7.1 Pacote stats\nA função uniroot é a implementação do método de Brent. Seus principais argumentos são:\n\nf: a função da qual se deseja obter a raiz\ninterval: vetor contendo os pontos extremos do intervalo de busca\ntol: tolerância desejada (o valor padrão é 10^{-10})\nmaxiter: número máximo de tentativas (o valor padrão é 1000)\n\n\n\n1.7.2 Pacote pracma\nO pacote pracma traz diversas rotinas de cálculo numérico, incluindo os métodos clássicos de busca de raízes.\nOs métodos da bisseção, secante e falsa posição estão implementados nas funções bisect, secant e regulaFalsi, respectivamente. Seus argumentos são\n\nf: a função da qual se deseja obter a raiz\na: limite inferior do intervalo para a bisseção e falsa posição; um ponto perto da raiz para a secante\nb: limite superior do intervalo para a bisseção e falsa posição; um ponto perto da raiz para a secante\ntol: tolerância desejada (o valor padrão é \\(10^{-10}\\))\nmaxiter: número máximo de tentativas (o valor padrão é 100)\n\nO método da interpolação quadrática inversa (Muller) está implementado na função muller, cujos argumentos são\n\nf: a função da qual se deseja obter a raiz\np0,p1,p2: três pontos iniciais, próximos da raiz\ntol: tolerância desejada (o valor padrão é .0001)\nmaxiter: número máximo de tentativas (o valor padrão é \\(10^{-10}\\))\n\nO método de Brent está implementado na função brent e possui os argumentos\n\nf: a função da qual se deseja obter a raiz\na,b: intervalo que contém a raiz (as imagens nas extremidades do intervalo devem ter sinais opostos)\ntol: tolerância desejada (o valor padrão é \\(10^{-12}\\))\nmaxiter: número máximo de tentativas (o valor padrão é 500)\n\nA função newtonRaphson (ou de modo equivalente, newton) encontra a raiz da função utilizando o método Newton-Raphson. Os argumentos são\n\nfun: a função da qual se deseja obter a raiz\nx0: valor inicial, próximo da raiz\ndfun: a função da derivada de \\(f\\). Se dfun=NULL, o método da secante será utilizado\ntol: tolerância desejada (o valor padrão é \\(10^{-8}\\))\nmaxiter: número máximo de tentativas (o valor padrão é 500)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#exercícios",
    "href": "solucoes_caso_real.html#exercícios",
    "title": "1  Solução de equações",
    "section": "1.8 Exercícios",
    "text": "1.8 Exercícios\n\nExercício Seja \\(f:[a,b]\\rightarrow\\mathbb{R}\\) uma função contínua tal que \\(f(a)f(b)&lt;0\\). O método da bisseção gera uma sequência de intervalos encaixados \\([a_k,b_k]\\) onde a raiz \\(x^\\star\\in[a_k,b_k]\\) para todo \\(k\\in\\mathbb{N}\\).\n\nDiscuta a unicidade da raiz encontrada por esse método. Sob quais condições o Método da Bisseção garante que a raiz encontrada é única no intervalo inicial \\([a,b]?\\)\nSe a função tiver múltiplas raízes no intervalo, o que podemos afirmar sobre a raiz que o método converge?\n\n\n\nExercício Implemente o método da bisseção para encontrar a raiz da função \\[f(x)=\\log(x)+x^2-4.\\]\nUtilize o intervalo inicial \\([1,2]\\). (Verifique previamente se \\(f(1)\\) e \\(f(2)\\) possuem sinais opostos). O critério de parada deve ser a obtenção de um erro absoluto máximo inferior a \\(10^{−6}\\). Ao final, imprima:\n\nA raiz aproximada encontrada.\nO número total de iterações realizadas.\nUma tabela com os valores \\(x_n\\), \\(f(x_n)\\) e \\(\\varepsilon_n\\).\n\n\n\nExemplo. Seja \\(X\\) uma variável aleatória contínua, cuja função distribuição é dada por \\(F(x)\\). O quantil \\(100p\\%\\) é o valor \\(x_p\\) que satisfaz \\[F(x_p)=p.\\] Explique porque existe um único \\(x_p\\) que satifaz a equação acima e utilize esse fato para descrever como obter \\(x_p\\) a partir do método da bisseção.\n\n\nExercício. Seja \\(X\\) uma variável aleatória com a seguinte função distribuição \\[F(x)=1-\\frac{1}{5}(x^2+4x+5) e^{-x},\\] com \\(x&gt;0\\). Encontre a mediana de \\(X\\) utilizando o método da bisseção.\n\n\nExercício Considere o problema de resolver \\(\\sqrt{x}=3\\). Prove que o método de Newton-Raphson vai convergir para a verdadeira solução se o valor inicial \\(x_0\\) satisfaz\n\\[\\frac{|x_0^2-7|}{2x_0^2}\\leq \\frac{1}{2}.\\]\n\n\nExercício. Neste exercício você vai demonstrar o Teorema da Convergência do método da secante. Considere que as condições do teorema estão satisfeitas. É fato que, para qualquer ponto \\(x\\) próximo de \\(x^\\star\\), a função \\(f(x)\\) pode ser aproximada por\n\\[f(x)\\approx f(x^\\star) +(x-x^\\star)f'(x^\\star)+\\frac{1}{2}(x-x^\\star)f''(x^\\star)\\] onde a aproximação é obtida por expansão em séries de Taylor. Considere então que \\(x_n\\) e \\(x_{n-1}\\) estão próximas de \\(x^\\star\\). Seja \\(\\xi_n=x_n-x^\\star\\).\n\nProve que \\[\\begin{align}f(x_n)\\approx \\xi_nf'(x^\\star)+\\frac{1}{2}\\xi_n^2 f''(x^\\star)\\end{align}\\]\nMostre que\n\n\\[\\begin{align}f(x_n)-f(x_{n-1})\\approx (\\xi_n-\\xi_{n-1})\\left[f'(x^\\star)+\\frac{1}{2}f''(x^\\star)(\\xi_n+\\xi_{n-1})\\right]\\end{align}\\]\n\nMostre que a fórmula de iteração do método da secante pode ser reescrita como\n\n\\[\\xi_{n+1}=\\xi_n-f(x_n)\\frac{\\xi_n-\\xi_{n-1}}{f(x_n)-f(n-1)}\\]\n\nA partir dos passos 2 e 3, prove que\n\n\\[\\xi_{n+1}=\\xi_n-\\frac{f(x_n)}{f'(x^\\star)+\\frac{1}{2}f''(x^\\star)(\\xi_n+\\xi_{n-1})}\\]\n\nUtilize a aproximação para \\(f(x_n)\\) para mostrar que\n\n\\[\\xi_{n+1}\\approx\\xi_n-\\frac{\\xi_nf'(x^\\star)+\\frac{1}{2}\\xi_n^2 f''(x^\\star)}{f'(x^\\star)+\\frac{1}{2}f''(x^\\star)(\\xi_n+\\xi_{n-1})}=\\xi_n-\\frac{\\xi_n+\\frac{1}{2}\\xi_n^2 \\frac{f''(x^\\star)}{f'(x^\\star)} }{1+\\frac{1}{2}\\frac{f''(x^\\star)}{f'(x^\\star)}(\\xi_n+\\xi_{n-1})}\\]\n\nSe \\(u\\) é próximo de zero, é verdade que \\[\\frac{1}{1+u}\\approx 1-u\\] Como \\((\\xi_n+\\xi_{n-1})\\) é próximo de zero faça \\[u=\\frac{1}{2}\\frac{f''(x^\\star)}{f'(x^\\star)}(\\xi_n+\\xi_{n-1})\\] e mostre que \\[\\xi_{n+1}\\approx\\xi_n-\\frac{\\xi_nf'(x^\\star)+\\frac{1}{2}\\xi_n^2 f''(x^\\star)}{f'(x^\\star)+\\frac{1}{2}f''(x^\\star)(\\xi_n+\\xi_{n-1})}=\\xi_n-\\left(\\xi_n+\\frac{1}{2}\\xi_n^2 \\frac{f''(x^\\star)}{f'(x^\\star)}\\right)\\left(1-\\frac{1}{2}\\frac{f''(x^\\star)}{f'(x^\\star)}(\\xi_n+\\xi_{n-1})\\right)\\]\nComo \\(\\varepsilon_n\\) é pequeno, o valor de \\(\\varepsilon_n^2\\approx 0\\). Utilize essa informação para mostrar que\n\n\\[\\xi_{n+1}\\approx \\frac{1}{2}\\frac{f''(x^\\star)}{f'(x^\\star)}\\xi_n\\xi_{n-1}\\] Isto implica que \\(|\\xi_{n+1}|&lt;|\\xi_n|\\)\n\nConclua que, se \\(x_0,x_1\\) estão próximos o suficiente da raiz, então o método da secante é convergente.\n\n\n\nExercício. Dizemos que \\(X\\) tem distribuição Lindley(\\(\\theta\\)) se sua função densidade é dada por \\[f(x|\\theta)=\\frac{\\theta^2}{1+\\theta}(1+x)e^{-\\theta x},\\] onde \\(x,\\theta&gt;0\\). Pode-se provar que\n\\[F(x|\\theta)=1-\\frac{1+\\theta(1+x)}{1+\\theta}e^{-\\theta x}\\] Para \\(\\theta=1\\) e considerando um erro de \\(10^{-4}\\), encontre a mediana desse modelo considerando os métodos\n\nBisseção\nSecante\nFalsa Posição\nMuller\nBrent\nNewton-Raphson\n\nVocê pode utilizar as funções ja implementadas no R. Para cada método, guarde o número de iterações até a convergência. Qual método convergiu mais rápido?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "otimizacao.html",
    "href": "otimizacao.html",
    "title": "2  Métodos de otimização",
    "section": "",
    "text": "2.1 Introdução\nSeja \\(f(x)\\) uma função contínua. O problema de otimização tem por objetivo determinar o valor de \\(x^\\star\\) qual que \\[f(x)\\geq f(x^\\star)\\] para todo \\(x\\) no domínio da função. No contexto de otimização, \\(f(x)\\) é denominada função objetivo, enquanto que o ponto \\(x^\\star\\) é denominado ponto de mínimo global.\nEm certas situações, estamos interessados apenas em determinar o valor \\(x^\\star\\in D\\) tal que \\[f(x)\\geq f(x^\\star)\\] para todo \\(x\\in D\\). Nesse caso, \\(x^\\star\\) é denominado ponto de mínimo local.\nDentro da otimização, o problema de encontrar um mínimo local é escrito como\n\\[\\begin{align}&\\min f(x)\\\\\n&\\hbox{s.t.}\\\\\n&x\\in D\n\\end{align}\\]\nEm geral, os métodos computacionais para otimização são implementados apenas para encontrar mínimos locais. Isso ocorre porque\n\\[\\max f(x)=\\min -f(x).\\] Observe que pontos ótimos, tanto mínimos quanto máximos, satisfazem \\(f'(x^\\star)=0\\). A Figure 2.1 apresenta a reta tangente a um ponto de máxmimo, no qual o valor da derivada (inclinação da reta) é zero.\nFigure 2.1: Em vermelho, a reta tangente ao ponto de máximo\nPortanto, o problema de encontrar um mínimo local em um intervalo \\(D\\) se reduz ao problema de encontrar \\(x^\\star\\) tal que\n\\[f'(x^\\star)=0,\\] Isto implica que podemos utilizar dos os todos os métodos vistos no capítulo anterior para encontrar mínimos locais.\nOs métodos vistos até o momento não são suficientes para resolver os problemas gerais de otimização, especialmente pela exigência da forma analítica da derivada da função.\nAlém disso, os problemas de otmização que lidamos na estatística e nas principais áreas da matemática aplicada envolvem funções do tipo \\(f:\\mathbb{R}^n\\rightarrow \\mathbb{R}\\), com \\(n&gt;1\\). Deste modo, métodos de confinamento, como o da bisseção, são difícies de generalizar. O exemplo abaixo ilustra uma aplicação comum na estatística.\nPara o caso \\(n\\)-dimensional, o teste da derivada se mantém, conforme o teorema a seguir.\nPara o próximo resultado, relembre que a matriz \\(B\\) é posivita definida se \\(\\boldsymbol{x}^TB\\boldsymbol{x}&gt;0\\) para todo \\(\\boldsymbol{x}\\neq \\textbf{0}\\) e positiva semidefinida se \\(\\boldsymbol{x}^TB\\boldsymbol{x}\\geq 0\\) para todo \\(\\boldsymbol{x}.\\)\nO seguinte teorema é fundamental para a construção dos principais algoritmos de otimização.\nNessas notas de aula, vamos considerar que \\(f\\) é uma função suave, o que significa que existe a segunda derivada e ela é contínua.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "otimizacao.html#introdução",
    "href": "otimizacao.html#introdução",
    "title": "2  Métodos de otimização",
    "section": "",
    "text": "Example 2.1 Considere o problema de encontrar o máximo da função \\[f(x)=x^2 e^{-x},\\] com \\(x&gt;0\\), cuja solução analítica é \\(x^\\star=2\\). A derivada de \\(f\\) é dada por\n\\[f'(x)=2xe^{-x}-x^2e^{-x}=(2x-x^2)e^{-x}\\] O gráfico da derivada da função, entre os pontos 1 e 3 é dado abaixo. Observe que as imagens nesses pontos têm sinais opostos.\n\ndfunc &lt;- function(x) (2*x - x^2) * exp(-x)\ncurve( dfunc(x) , 1, 3)\nabline(h=0, lty =2)\n\n\n\n\n\n\n\n\nUtilizando a função uniroot, obtemos o ponto desejado.\n\nuniroot( dfunc, c(1,3) )\n\n$root\n[1] 2\n\n$f.root\n[1] -7.437827e-08\n\n$iter\n[1] 6\n\n$init.it\n[1] NA\n\n$estim.prec\n[1] 6.709322e-05\n\n\n\n\nExercício. Considere a função\n\\[f(x) = \\sin(x) +\\cos(x\\sqrt{2}))+\\sin(\\pi x).\\] 1. Implemente a função e faça o seu gráfico para o intervalo \\((-10,10)\\).\n\nDetermine o mínimo global, sabendo que este está no intervalo \\((-10,10)\\)\n\n\n\nImportante A função logaritmo é monótona crescente. Portanto, é verdade que\n\\[f(x)\\geq f(x^\\star)\\Leftrightarrow \\log f(x) \\geq \\log f(x^\\star).\\] Deste modo, encontrar o mínimo de \\(\\log f\\) é equivalente a encontrar \\(x^\\star\\).\n\n\n\n\nExample 2.2 Os dados abaixo apresentam as máximas anuais do Rio Negro entre 2004 e 2014.\n\nmaximas &lt;- c(25.42, 28.76, 29.05, 27.16, 28.54, 28.96, 27.58, 29.30, 28.62, 28.21, 28.91, 28.27, 27.13, 28.10, 28.84, 28.18, 28.62, 29.77, 27.96, 28.62, 29.97, 29.33, 29.50)\n\nAssim como o Teorema Central do Limite dá uma distribuição aproximada para a média amostral, o Teorema de Fisher-Tippet diz que a distribuição dos máximos tende a uma distribuição denominada Valores Extremos. Nesse caso em particular, a distribuição será a Weibull, cuja densidade é dada por\n\\[f(x|\\alpha,\\beta) =\n\\begin{cases}\n\\frac{\\alpha}{\\beta}\\left(\\frac{x}{\\beta}\\right)^{\\alpha-1}e^{-(x/\\beta)^\\alpha} & \\text{para } x \\ge 0 \\\\\n0 & \\text{para } x &lt; 0\n\\end{cases}\\]\ncom \\(\\alpha,\\beta&gt;0\\). O valor de \\(\\alpha,\\beta\\) que maximiza a função \\[L(\\alpha,\\beta)=\\prod_{i=1}^n f(x_i|\\alpha,\\beta)\\] é denominado estimativa de máxima verossimilhança. Abaixo, mostramos a implementação de \\(-L(\\alpha,\\beta)\\) e algumas representações gráficas da superfície a ser otimizada\n\nL &lt;- function(a,b) -prod(dweibull(maximas, shape = a, scale = b, log = F))\na &lt;- seq(30,46, length.out = 50)\nb &lt;- seq(28,30, length.out = 50)\nz &lt;- outer(a, b, Vectorize(L))\n\npersp(a,b,z, theta = 45, phi = 45)\n\n\n\n\n\n\n\nimage(a,b,z, col=terrain.colors(20))\ncontour(a,b,z, add = T)\n\n\n\n\n\n\n\n\nÉ importante ressaltar que, por motivos de estabilidade numérica, \\(\\log L(\\alpha,\\beta)\\) deve ser implementada no lugar de \\(L(\\alpha,\\beta)\\)\n\n\n\nTheorem 2.1 (Condição necessária de primeira ordem) Se \\(\\boldsymbol{x}^\\star\\) é um ponto de mínimo, então \\[\\nabla f(\\boldsymbol{x}^\\star)=\\left(\\frac{\\partial f}{\\partial x_1},\\ldots,\\frac{\\partial f}{\\partial x_n}\\right)^T=\\textbf{0}_n.\\]\n\n\n\nTheorem 2.2 (Condição necessária de segunda ordem) Se \\(\\boldsymbol{x}^\\star\\) é um ponto de mínimo local de \\(f\\) e se existe \\(\\nabla f\\) contínua na vizinhança de \\(\\boldsymbol{x}^\\star\\), então \\(\\nabla f(\\boldsymbol{x}^\\star)=\\textbf{0}\\) e \\(\\mathcal{H}f(\\boldsymbol{x}^\\star)\\) é positiva semidefinida, onde \\(\\mathcal{H}f\\), denominada Hessiana, é a matriz das segundas derivadas parciais da função, definida como:\n\\[ \\mathcal{H} f(\\mathbf{x}) = \\left(\\begin{array}{ccc}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{array}\\right) \\]\n\n\n\nTheorem 2.3 (Teorema de Taylor) Suponha que \\(f:\\mathbb{R}^n\\rightarrow\\mathbb{R}\\) é contínua e diferenciável. Então, para qualquer \\(\\boldsymbol{x}\\) na vizinhança de \\(\\boldsymbol{x}_0\\), valem as aproximações:\n\nPrimeira ordem:\n\n\\[f(\\boldsymbol{x})\\approx f(\\boldsymbol{x}_0)+(\\boldsymbol{x}-\\boldsymbol{x}_0)^T\\nabla f(\\boldsymbol{x}_0)\\] 2. Segunda ordem:\n\\[f(\\boldsymbol{x})\\approx f(\\boldsymbol{x}_0)+(\\boldsymbol{x}-\\boldsymbol{x}_0)^T\\nabla f(\\boldsymbol{x}_0)+\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{x}_0)^T\\mathcal{H}(\\boldsymbol{x}_0)(\\boldsymbol{x}-\\boldsymbol{x}_0)\\]\n\n\nCorolário. Suponha que \\(f:\\mathbb{R}^n\\rightarrow\\mathbb{R}\\) é contínua e diferenciável. Então, para qualquer \\(\\boldsymbol{x}\\) na vizinhança de \\(\\boldsymbol{x}_0\\),\n\\[\\nabla f(\\boldsymbol{x})\\approx \\nabla f(\\boldsymbol{x}_0)+\\mathcal{H}f(\\boldsymbol{x}_0)(\\boldsymbol{x}-\\boldsymbol{x}_0).\\]\n\n\nTheorem 2.4 (Condição suficiente de segunda ordem) Suponha que \\(\\mathcal{H}f(\\boldsymbol{x})\\) é contínua na vizinhança de \\(\\boldsymbol{x}^\\star\\), \\(\\nabla f(\\boldsymbol{x}^\\star)=\\textbf{0}\\) e \\(\\mathcal{H}f(\\boldsymbol{x}^\\star)\\) é positiva definida. Então \\(\\boldsymbol{x}^\\star\\) é um minimizador local.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "otimizacao.html#método-newton-raphson",
    "href": "otimizacao.html#método-newton-raphson",
    "title": "2  Métodos de otimização",
    "section": "2.2 Método Newton-Raphson",
    "text": "2.2 Método Newton-Raphson\nEsse método consiste na aplicação direra do método dNewton-Raphson para encontrar raizes. Considere o problema de encontrar o zero da função \\(f'(x)\\). Então, a equação de iteração do método é\n\\[x_n=x_{n-1}-\\frac{f'(x_{n-1})}{f''(x_{n-1})}.\\] A vantagem deste método é que ele pode ser generalizado para \\(f:\\mathbb{R}^n\\rightarrow \\mathbb{R}\\). Para um ponto \\(\\boldsymbol{x}\\) próximo de \\(\\boldsymbol{x}_n\\), pelo corolário do Teorema de Taylor temos que\n\\[\\nabla f(\\boldsymbol{x})\\approx \\nabla f(\\boldsymbol{x}_{n}) +\\mathcal{H}f(\\boldsymbol{x}_{n})(\\boldsymbol{x}-\\boldsymbol{x}_{n})\\] Vamos encontrar a raiz a aproximação acima: \\[\\textbf{0}_n= \\nabla f(\\boldsymbol{x}_{n}) +\\mathcal{H}f(\\boldsymbol{x}_{n})(\\boldsymbol{x}-\\boldsymbol{x}_{n})\\Rightarrow \\boldsymbol{x}=\\boldsymbol{x}_n-[\\mathcal{H}f(\\boldsymbol{x}_n)]^{-1}\\nabla f(\\boldsymbol{x}_{n})\\] logo, a equação de iteração é dada por em torno de \\(x_0\\) 'e dada por\n\\[\\boldsymbol{x}_{n+1}=\\boldsymbol{x}_n-[\\mathcal{H}f(\\boldsymbol{x}_n)]^{-1}\\nabla f(\\boldsymbol{x}_{n})\\]\ne o critério de parada é \\(||\\boldsymbol{x}_n-\\boldsymbol{x}_{n-1}||\\leq \\varepsilon\\).\nVamos resscrever a iteração do método Newton-Raphson do seguinte modo:\n\\[\\boldsymbol{x}_{n}=\\boldsymbol{x}_{n-1}+\\underbrace{[\\mathcal{H}f(\\boldsymbol{x}_{n-1})]^{-1}}_{\\alpha_{n-1}}\\underbrace{(-\\nabla f(\\boldsymbol{x}_{n-1}))}_{p_{n-1}}=\\boldsymbol{x}_{n-1}+\\alpha_{n-1}p_{n-1}\\] Na equação acima, o termo \\(p_{n-1}\\) indica a direção de maior inclinação, enquato \\(\\alpha_{n-1}\\) indica o tamanho do salto (quanto mais plana for a função no ponto, maior é o valor de \\([\\mathcal{H}f]^{-1}\\) e maior será o salto). Essa estrutura de direção e salto é utilizada em outros métodos de otimização.\nComo a direção e o tamanho do salto são computados diretamente da função objetivo, temos que o metódo Newton-Rapshon é rápido quando o ponto inicial é escolhido próximo do ótimo e o custo de computar o gradiente e o inverso da Hessiana é baixo.\n\nAlgoritmo - Newton-Raphson\n\nInicialização. Comece com \\(\\boldsymbol{x}_0\\) próximo de um mínimo local. Fixe um erro \\(\\varepsilon\\) e faça ERRO\\(=+\\infty\\) e comece com \\(i=1\\).\nIteração. Enquanto ERRO\\(&gt;\\varepsilon\\).\n\nCalcule \\[\\boldsymbol{x}_i=\\boldsymbol{x}_{i-1}-[\\mathcal{H}f(\\boldsymbol{x}_{i-1})]^{-1}\\nabla f(\\boldsymbol{x}_{i-1})\\] e faça \\(i=i+1\\).\n\nb.Calcule ERRO\\(=|\\boldsymbol{x}_i-\\bolsymbol{x}_{i-1}|=\\sqrt{\\sum_{j=1}^n (x_{i,j}-x_{i-1,j})^2}\\)\nFinalizando. Retorne \\(x_i\\)\n\n\n\nExercício 3\nConsidere a função \\[f(x,y)=x^4+2y^4-4xy\\] Começando no ponto \\((2,1)\\), implemente o método Newton-Raphon para encontrar um mínimo local para esta função.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "otimizacao.html#método-da-descida-do-gradiente",
    "href": "otimizacao.html#método-da-descida-do-gradiente",
    "title": "2  Métodos de otimização",
    "section": "2.3 Método da Descida do Gradiente",
    "text": "2.3 Método da Descida do Gradiente\nUm dos principais problemas do método Newton-Raphson é a inversão da matriz Hessiana a cada iteração.\nIniciando em um ponto \\(\\boldsymbol{x}_0\\) próximo da raiz, a \\(n\\)-ésima iteração do método da Descida do Gradiente é dado por\n\\[\\boldsymbol{x}_{n}=\\boldsymbol{x}_{n-1}-\\alpha_n\\nabla f(\\boldsymbol{x}_{n-1})\\] onde \\(\\alpha_n\\) é denominada taxa de aprendizado, sendo geralmente uma constante. Observe que esse método substitui a inversa da matriz Hessiana por um escalar. Essa simplificação abre mão da precisão do método de Newton em favor da eficiência computacional.\n\nAlgoritmo - Descida do Gradiente\n\nInicialização. Comece com \\(\\boldsymbol{x}_0\\) próximo de um mínimo local. Fixe um erro \\(\\varepsilon\\), faça ERRO\\(=+\\infty\\) e comece com \\(i=1\\). Escolha um valor para \\(\\alpha\\).\nIteração. Enquanto ERRO\\(&gt;\\varepsilon\\).\n\nCalcule \\[\\boldsymbol{x}_i=\\boldsymbol{x}_{i-1}-\\alpha\\nabla f(\\boldsymbol{x}_{i-1})\\] e faça \\(i=i+1\\).\n\nb.Calcule ERRO\\(=|\\boldsymbol{x}_i-\\bolsymbol{x}_{i-1}|=\\sqrt{\\sum_{j=1}^n (x_{i,j}-x_{i-1,j})^2}\\)\nFinalizando. Retorne \\(x_i\\)\n\n\n\nExercício 4\nConsidere novamente a função \\[f(x,y)=x^4+2y^4-4xy\\] Começando no ponto \\((2,1)\\), implemente o método da Descida do Gradiente um mínimo local para esta função.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "otimizacao.html#o-método-bfgs-broyden-fletcher-goldfarb-and-shanno",
    "href": "otimizacao.html#o-método-bfgs-broyden-fletcher-goldfarb-and-shanno",
    "title": "2  Métodos de otimização",
    "section": "2.4 O Método BFGS (Broyden, Fletcher, Goldfarb, and Shanno)",
    "text": "2.4 O Método BFGS (Broyden, Fletcher, Goldfarb, and Shanno)\nSeja \\(f\\) uma função real e suave. Ao colocar o ponto de mínimo como a raiz de \\(f'(x)\\). Sabemos que a iteração do método de Newton Raphson é dado por \\[x_n=x_{n-1}+\\frac{f'(x_{n-1})}{f''(x_{n-1})}\\]\nenquanto que a iteração do método da secante é dado por\n\\[x_n=x_{n-1}+f'(x_{n-1})\\frac{x_{n-1}-x_{n-2}}{f'(x_{n-1})-f'(x_{n-2})}\\] onde \\[\\frac{f'(x_{n-1})-f'(x_{n-2})}{x_{n-1}-x_{n-2}}\\approx f''(x_{n-1})\\].\nPortanto, o método da secante, sendo um método de quase-Newton, depende apenas do gradiente para aproximar a segunda derivada.\nConsidere agora que \\(f:\\mathbb{R}^n\\rightarrow \\mathbb{R}\\), com \\(n&gt;1\\). A expansão do gradiente em série de Taylor de primeira ordem em torno de \\(\\boldsymbol{x}_n\\) é\n\\[\\nabla f(x)=\\nabla f(\\boldsymbol{x}_n)+\\mathcal{H}f(\\boldsymbol{x}_n)(\\boldsymbol{x}-\\boldsymbol{x}_n).\\] Deste modo, a matriz Hessiana em \\(\\boldsymbol{x}_n\\) deve satisfazer\n\\[\\nabla f(\\boldsymbol{x})-\\nabla f(\\boldsymbol{x}_n)=\\mathcal{H}f(\\boldsymbol{x}_n)(\\boldsymbol{x}-\\boldsymbol{x}_n).\\] O método BFGS procura criar uma sequência de matrizes positivas definidas \\(\\textbf{B}_1,\\ldots,\\textbf{B}_n\\) tais que\n\\[\\nabla f(\\boldsymbol{x}_{n+1})-\\nabla f(\\boldsymbol{x}_n)=\\textbf{B}_{n}(\\boldsymbol{x}-\\boldsymbol{x}_n).\\] de tal forma que \\(\\textbf{B}_n\\approx \\mathcal{H}(\\boldsymbol{x}_n)\\). A obtenção exata de \\(\\textbf{B}_n\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "otimizacao.html#método-de-nelder-e-mead",
    "href": "otimizacao.html#método-de-nelder-e-mead",
    "title": "2  Métodos de otimização",
    "section": "2.5 Método de Nelder e Mead",
    "text": "2.5 Método de Nelder e Mead\nUm \\(n\\)-simplex é a forma com o menor número de vértices possível em uma dimensão \\(n\\), que seriam \\(n+1\\) vértices.\n\n0-Simplex: um ponto.\n1-Simplex: um segmento de reta (ligando dois pontos).\n2-Simplex: É um triângulo (ligando três pontos não-colineares).\n3-Simplex: É um tetraedro (uma pirâmide com quatro faces triangulares).\n\nO métodos de Nelder Mead procura a direção do ótimo sem a necessidade do cálculo de derivadas, criando a busca através de uma sequência de simplex. Vamos explicar o algoritmo em detalhes para o caso geral, mas vamos ilustrar seu funcionamento para uma função \\(f:\\mathbb{R}^2\\rightarrow\\mathbb{R}\\). A ?fig-nelder apresenta o \\(n\\)-simplex inicial (um triângulo).\nPasso 1. Para o conjunto com \\(n+1\\) soluções, rotule as soluções como \\(x_0,\\ldots,x_{n-1}\\) de modo que \\(f(x_0)\\leq \\cdots\\leq f(x_n)\\). Determine o conjunto restante, dado por \\(x_0,\\ldots,x_{n-1}\\). Vamos traçar a reta que passa pela pior solução e a média dos pontos no conjunto restante \\(x_M\\) (reta em vermelho). Observe que essa reta indica a direção do salto, que é oposta à pior solução.\n\n\n\nInício de um iteração de Nelder Mead. Um simplex é contruído e uma reta é traçada entre a pior solução e o baricentro das demais para determinar a direção do salto.\n\n\nAgora que a direção do salto foi determinada, o método utiliza uma série de estratégias para determinar o tamanho do salto.\nPasso 2. Sua primeira tentativa é fazer uma reflexão, ou seja, se vale a pena procurar por uma solução utilizando o \\(n\\)-simplex refletido, calculando o ponto oposto ao pior na direção já determinada, dado por\n\\[x_r = x_M+(x_M-x_n)\\] Se \\(f(x_r)\\) é melhor que pelo menos uma das soluções no conjunto restante (ou seja, \\(f(x_r)&lt;f(x_{n-1})\\)), então, o método obteve sucesso.\n\nCaso \\(A\\): \\(f(x_r)&gt;f(x_0)\\), então a solução encontrada não é a melhor de todas, mas pode substituir a pior. Um novo simplex é construído considerando. os pontos do conjunto restante e \\(x_r\\). Volte para o Passo 1\nCaso \\(B\\): \\(f(x_r)&lt;f(x_0)\\) então uma solução melhor que as demais foi encontrada. Então o método realiza uma expansão, para determinar se vale a pena procurar por uma solução em um triângulo maior, além de \\(x_r\\), utilizando a direção já estabelecida. O ponto da expansão é dado por\n\n\\[x_e=x_M+2(x_M-x_n)\\] Se \\(f(x_e)&lt;f(x_r)\\), então a expansão apresentou uma solução melhor e um novo simplex é construído unindo o conjunto restante com o ponto \\(x_e\\). Caso contrário, o novo simplex é construído considerando o ponto \\(x_r\\). Volte para o Passo 1.\n\n\n\n\n\n\n\n\nFigure 2.2: Esquerda: reflexão. Direita: expansão\n\n\n\n\n\nPasso 3. Quando a reflexão falha em gerar um novo ponto, o método tenta uma contração.\n\n(Caso C). A contração externa ocorre quando a solução \\(x_r\\) é pior que todas as soluções no conjunto restante mas é melhor que \\(x_n\\). Isso significa que a direção da reflexão deveria estar certa mas a reflexão foi longe demais. Um ponto mais próximo de \\(x_M\\) é gerado\n\n\\[x_c=x_M+\\frac{1}{2}(x_r-x_M)\\] * (Caso D)A contração interna ocorre quando \\(x_r\\) é a pior de todas as soluções. Nesse caso o método assume que o ponto de mínimo está no interior do simplex, na direção do pior ponto. O ponto interno é dado por\n\\[x_c=x_M-\\frac{1}{2}(x_n - x_M).\\]\nSe \\(f(x_c)&lt; f(x_n)\\), então a contração teve êxito. Um novo simplex é construído considerando o ponto \\(x_c\\) e o conjunto restante. Volte para o Passo 1.\n\nPasso 4. O encolhimento é o último recurso. Ele ocorre quando todas as tentativas de melhorar o pior ponto falharam. O algoritmo reduz o tamanho do simplex encolhendo” todos os vértices (exceto \\(x_0\\)) em direção de \\(x_0\\). Todos os vértices são substituídos por\n\n\\[y_i=x_0+\\frac{1}{2}(x_i-x_0).\\] Um novo simplex será construído com os pontos \\(y_1,\\ldots,y_n\\). Retorne ao Passo 1.\nCritério de parada. Podemos parar o procedimento quando \\(|x_0-x_n|&lt;\\varepsilon\\)\nAbaixo, segue o algoritmo Nelder-Mead\n\nAlgoritmo - Método de Nelder Mead\n\nOrdenação dos Pontos: Ordene os pontos do simplex com base nos valores da função, do melhor para o pior: \\(x_0, x_1, \\ldots, x_n\\).\nCálculo do Centroide: Calcule o centroide (\\(x_M\\)) dos pontos, excluindo o pior (\\(x_n\\)). \\[x_M = \\frac{1}{n} \\sum_{i=0}^{n-1} x_i\\]\nReflexão: Calcule o ponto de reflexão (\\(x_r\\)) na direção oposta ao ponto \\(x_n\\). \\[x_r = x_M + (x_M - x_n)\\]\nAvaliação de \\(x_r\\):\n\nSucesso Extremo (\\(f(x_r) &lt; f(x_0)\\)): O ponto refletido é o melhor de todos. O método tenta uma expansão, calculando \\[x_e = x_M + 2(x_M - x_n)\\] Se \\(f(x_e) &lt; f(x_r)\\), substitua \\(x_n\\) por \\(x_e\\). Senão, substitua \\(x_n\\) por \\(x_r\\).\nSucesso Moderado (\\(f(x_{n-1}) &gt; f(x_r)\\))}: O ponto refletido é melhor que o segundo pior. Substitua \\(x_n\\) por \\(x_r\\).\nFracasso da Reflexão (\\(f(x_r) \\ge f(x_{n-1})\\)): O ponto refletido não é uma melhoria significativa. O método tenta uma contração.\n\n{Contração Externa} (\\(f(x_{n-1}) \\le f(x_r) &lt; f(x_n)\\)): Calcule o ponto de contração (\\(x_c\\)). \\[x_c = x_M + \\frac{1}{2}(x_r - x_M)\\] Se \\(f(x_c) &lt; f(x_r)\\), substitua \\(x_n\\) por \\(x_c\\). Senão, vá para o Encolhimento.\nContração Interna (\\(f(x_r) \\ge f(x_n)\\)): Calcule o ponto de contração (\\(x_c\\)). \\[x_c = x_M - \\frac{1}{2}(x_n - x_M)\\] Se \\(f(x_c) &lt; f(x_n)\\), substitua \\(x_n\\) por \\(x_c\\). Senão, vá para o Encolhimento.\n\n\nEncolhimento (Shrink): Se todas as outras tentativas falharem, encolha o simplex em direção ao melhor ponto (\\(x_0\\)). Para cada ponto \\(x_i\\) (\\(i=1, \\ldots, n\\)), substitua-o por \\(y_i\\). \\[y_i = x_0 + \\frac{1}{2}(x_i - x_0)\\]\n\n\n\nExercício 4\nConsidere novamente a função \\[f(x,y)=x^4+2y^4-4xy\\] Começando nos pontos \\((0,0)\\), \\((1,2)\\) e \\((2,1)\\), implemente o método de Nelder Mead para encontrar um mínimo local para esta função.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "otimizacao.html#a-função-optim",
    "href": "otimizacao.html#a-função-optim",
    "title": "2  Métodos de otimização",
    "section": "2.6 A função optim",
    "text": "2.6 A função optim\nA função optim agrega diversos otimizadores, sendo a principal função de otimização do R. Seus principais argumentos são:\n\npar: vetor com ponto inicial\nfn: a função a ser minimizada. O seu primeiro argumento deve ser um vetor com a mesma dimensão de par\nmethod: são os métodos implementados. Em especial destacamos as opções Nelder-Mead (padrão) e BFGS.\n\n\nExercício 5\nConsidere novamente a função \\[f(x,y)=x^4+2y^4-4xy\\] e o ponto inicial \\((0,0)\\), utilize a função optim para encontrar o mínimo global.\n\n\nExercício 6\nUtilizand a função optim, encontre as estimativas de máxima verossimilhança para o problema dado no Exemplo 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "otimizacao.html#exercícios",
    "href": "otimizacao.html#exercícios",
    "title": "2  Métodos de otimização",
    "section": "2.7 Exercícios",
    "text": "2.7 Exercícios\n\nExercício.\nProve que\n\nTodo minimizador global é um minizador local.\nNem todo minimizador local é um minimizador global\n\n\n\nExercício.\nProve que o problema \\(\\min f(x)\\) é equivalente a \\(\\max -f(x)\\).\n\n\nExercício\nUma condição \\(C\\) é dita ser suficiente para o resultado \\(A\\) quando a ocorrência de \\(C\\) implica em \\(A\\). Por outro lado, dizemos que uma condição \\(C\\) é necessária para \\(A\\) quando o resultado \\(A\\) implica na ocorrência de \\(C\\).\nVimos no Teorema da Condição Suficiente de Segunda Ordem que a condição:\n \\(\\mathcal{H}f(\\boldsymbol{x})\\) é contínua na vizinhança de \\(\\boldsymbol{x}^\\star\\), \\(\\nabla f(\\boldsymbol{x}^\\star)=\\textbf{0}\\) e \\(\\mathcal{H}f(\\boldsymbol{x}^\\star)\\) é positiva definida  é suficiente para que \\(\\boldsymbol{x}^\\star\\). Nesse exercício, vamos mostrar que ela não é necessária, ou seja, é possível que exista um minimizador local que não satifaz a condição.\nSeja \\(f(x)=x^4\\).\n\nProve que \\(x^\\star=0\\) é um minimizador global.\nProve que a condição do Teorema da Condição Suficiente de Segunda Ordem não está satisfeita.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "integral.html",
    "href": "integral.html",
    "title": "3  Métodos de quadratura",
    "section": "",
    "text": "3.1 Quadratura Gaussiana\nSeja \\(P_n(x)\\) um polinômio de grau \\(n\\). Dizemos que os polinômios \\(P_0(x),P_1(x),\\ldots,\\) são ortonormais para o intervalo \\([a,b]\\) e para a função de peso \\(w(x)\\geq 0\\) se \\[\\begin{align}\\int_a^b w(x)P_i(x)P_j(x)dx&=0\\\\\\int_a^b w(x)P_i(x)^2dx&=1\\end{align}\\] para todo \\(i\\neq j\\). Esse polinômios podem ser escritos como \\[P_n(x)=k_n\\prod_{i=1}^n(x-t_i),\\] onde \\(k_n&gt;0\\) e \\(a&lt;t_1&lt;\\cdots&lt;t_n&lt;b\\) são suas raízes reais.\nO seguinte teorema relaciona a integral de uma função \\(f\\) contínua em \\([a,b]\\) e os polinômios ortonormais.\nO teorema acima mostra que a integral \\(\\int_a^b f(x)w(x)dx\\) é bem aproximada por \\[\\sum_{j=1}^n w_j f(t_j),\\] uma vez que o termo restante decresce para zero rapidamente. Além disso, \\(f^{(2n)}(x)=0\\) se \\(f(x)\\) for um polinômio de grau \\(2n+1\\), o que implica que o resultado é exato. Podemos então definir a quadratura gaussiana.\nEm geral, tal quadratura costuma vir acompanhada com o nome do polinômio ortogonal utilizado. A tabela abaixo apresenta as quadraturas usuais.\n\\[\\begin{array}{lll}\\hline\n\\hbox{Quadratura} & \\hbox{Intervalo} & \\hbox{Função peso} \\\\ \\hline\n\\hbox{Gauss-Legendre} & [-1,1] & w(x)=1 \\\\\n\\hbox{Gauss-Chebyshev} & [-1,1] & w(x)=(1-x^2)^{1/2} \\\\\n\\hbox{Gauss-Jacobi} & [-1,1] & w(x)=(1-x)^\\alpha (1+x)^\\beta, \\alpha,\\beta&gt;1 \\\\\n\\hbox{Gauss-Laguerre} & [0,\\infty) & w(x)=e^{-x}x^\\alpha, \\alpha&gt;-1 \\\\\n\\hbox{Gauss-Hermite} & (-\\infty,\\infty) & w(x)=e^{-x^2} \\\\\n\\end{array}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de quadratura</span>"
    ]
  },
  {
    "objectID": "integral.html#quadratura-gaussiana",
    "href": "integral.html#quadratura-gaussiana",
    "title": "3  Métodos de quadratura",
    "section": "",
    "text": "Theorem 3.1 Teorema Seja \\(f(x)\\) uma função contínua em \\([a,b]\\). Para qualquer polinômio ortonormal \\(P_n(x)\\) sejam \\(t_1,\\ldots,t_n\\) suas raízes. Então,\n\\[\\int_a^b f(x)w(x)dx=\\sum_{j=1}^n w_j f(t_j)+\\frac{f^{(2n)}(\\xi)}{k_n^2 (2n)!},\\] onde \\(\\xi\\in[a,b]\\), \\(f^{(2n)}\\) é a derivada de ordem \\(2n\\) da função e \\[w_j=-\\frac{k_{n+1}}{k_n}\\frac{1}{P_{n+1}(t_j)P_n'(t_j)}.\\]\n\n\n\nDefinition 3.2 (Quadratura gaussiana) Seja \\(f(x)\\) uma função contínua em \\([a,b]\\) e sejam \\(P_0(x),P_1(x),\\ldots,\\) polinômios ortogonais no mesmo intervalo considerando a função peso \\(w(x)\\leq\\). Então, a integral \\[\\int_a^b f(x)w(x)dx\\] então a quadratura gaussiana com \\(n\\) nós é dada por \\[\\hat{I}_n=\\sum_{j=1}^n w_j f(t_j).\\]\n\n\n\n\nExercício. Para a quadratura de Gauss-Legendre com 5 nós, temos\n\\[\\begin{array}{cc}\\hline \\hbox{Nó}(t_j)& \\hbox{Peso}(w_j) \\\\ \\hline\n-\\frac{1}{3}\\sqrt{5+2\\sqrt{\\frac{10}{7}}}\n& \\frac{322-13\\sqrt{70}}{900} \\\\\n-\\frac{1}{3}\\sqrt{5-2\\sqrt{\\frac{10}{7}}}\n& \\frac{322+13\\sqrt{70}}{900} \\\\\n0 & \\frac{128}{225} \\\\\n+\\frac{1}{3}\\sqrt{5-2\\sqrt{\\frac{10}{7}}}\n& \\frac{322+13\\sqrt{70}}{900} \\\\\n+\\frac{1}{3}\\sqrt{5+2\\sqrt{\\frac{10}{7}}}\n& \\frac{322-13\\sqrt{70}}{900} \\\\ \\hline\n\\end{array}\\]\nResolva a integral\n\\[\\int_{-1,96}^{1,96} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}dx\\] utilizando essa quadratura.\n\n\nRegra de Gauss-Kronrod\nConsidere a quadratura gaussiana \\(\\hat{I}_n\\). Sabemos que os nós desta quadratura são \\(t_1,\\ldots,t_n\\), relacionadas com as raízes do polinômio ortogonal \\(P_n(x)\\).\nAregra de Gauss-Kronrod consiste em criar o conjunto de nós \\(y_1,\\ldots,y_{2n+1}\\) contendo os nós \\(t_1,\\ldots,t_n\\) da regra de Gauss e um novo conjunto com \\(n+1\\) nós obtidos através do polinômio \\(Q_{n+1}(x)\\) ortogonal ao polinômio de Gauss \\(P_n(x)\\) e a todos os polinômios de grau inferior ou igual a \\(n-1\\). Deste modo, a regra de Gauss-Kronrod é dada por\n\\[\\hat{K}_{2n+1}=\\sum_{i=1}^{2n+1} \\tilde{w}_i f(y_i)\\] Observe que, na prática, tanto os pesos quanto os nós das regras de Gauss ou de Gauss-Kronrod já estão pré-calculados. A real vantagem do método de Gauss-Kronrod está no número de cálculos para estimar o erro, conforme vemos abaixo:\n\nRegra de Gauss. Para comparar o erro da integral utilizando a difereça \\(|\\hat{I}_{2n+1}-\\hat{I}_n|\\), a função \\(f(.)\\) deve ser avaliada \\(3n+1\\) vezes, porque existem \\(3n+1\\) nós distintos.\nRegra de Gauss-Kronrod. Para comparar o erro da integral, pode-se utilizar a difereça \\(|\\hat{K}_{2n+1}-\\hat{I}_n|\\). Nesse caso, a função \\(f(.)\\) deve ser avaliada apenas \\(2n+1\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de quadratura</span>"
    ]
  },
  {
    "objectID": "integral.html#quadratura-adaptativa-e-a-função-integrate",
    "href": "integral.html#quadratura-adaptativa-e-a-função-integrate",
    "title": "3  Métodos de quadratura",
    "section": "3.2 Quadratura adaptativa e a função integrate",
    "text": "3.2 Quadratura adaptativa e a função integrate\nA quadratura adaptativa é um método de integração numérica que ajusta dinamicamente os nós para maximizar a precisão onde a função é mais difícil de ser integrada.\nA ideia central é um processo de subdivisão recursiva. Utilizando a regra de Gauss-Kronrod como exemplo, temos os seguintes passos:\n\nAproximação Inicial: A integral é calculada em todo o intervalo \\([a,b]\\) usando a regra de Gauss-Kronrod. O resultado é a aproximação \\(\\hat{K}_{2n+1}\\) com uma estimatvida do erro dada por \\(\\hbox{erro}=|\\hat{I}_n-\\hat{K}_{2n+1}|\\)\nVerificação do Erro: O erro é comparado com uma tolerância \\(\\varepsilon\\).\n\nSe o erro &lt;\\(\\varepsilon\\), a aproximação é considerada suficientemente precisa. A integração para esse intervalo é concluída e o resultado é retornado.\nSe o erro \\(\\geq \\varepsilon\\), a aproximação não é precisa o suficiente. O método precisa de mais avaliações para melhorar o resultado.\n\nSubdivisão do Intervalo: O intervalo original \\([a,b]\\) é dividido em dois subintervalos, \\([a,m]\\) e \\([m,b]\\), onde \\(m\\) é o ponto médio. A ideia é que, ao subdividir o intervalo, a função em cada pedaço se torne mais suave, permitindo que as regras de quadratura funcionem melhor.\nProcesso Recursivo: O método se aplica novamente a cada um dos novos subintervalos, com uma nova tolerância para cada um (por exemplo, metade da tolerância original para cada subintervalo).\nCombinação dos Resultados: A aproximação final para a integral original é a soma das aproximações de cada subintervalo que foram aceitas.\n\nA função integrate, do pacote stats do R utiliza uma quadratura adaptativa com a regra de Gauss-Kronrod para aproximar a integral de uma função real. Seus argumentos principais são:\n\nf: função cujo primeiro argumento é um vetor numérico e que retorne um vetor numérico de mesmo comprimento.\nlower,upper: os limites de integração. É possível fazer lower = -Inf e upper = Inf\n\n\nExercício.\nCalcule analiticamente as integrais:\n\n\\(\\int_0^2 x^2dx\\)\n\n2 \\(\\int_0^\\pi \\sin(x)dx\\)\n\n\\(\\int_0^{10} e^{-x}dx\\)\n\nEm seguida, calcule as mesmas integrais utilizando a função integrate.\n\n\nExercício\nConsidere a função densidade\n\\[f(x)=\\frac{2}{5\\sqrt{\\pi}}(2/5)e^{-x^2}+ \\frac{2}{15}x^3 e^{-x},\\] para \\(x&gt;0\\). Combine as funções integrate e uniroot para encontrar a mediana deste modelo.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de quadratura</span>"
    ]
  },
  {
    "objectID": "integral.html#exercícios",
    "href": "integral.html#exercícios",
    "title": "3  Métodos de quadratura",
    "section": "3.3 Exercícios",
    "text": "3.3 Exercícios\n\nExercício Considerando a regra do trapézio prove que \\[\\hat{I}_n=\\sum_{i=0}^n w_i f(x_i),\\] onde \\(0=x_0,\\ldots,x_n=1\\) é o conjunto de nós e \\[w_i=\\left\\{\\begin{array}{ll}\\frac{1}{2n},&\\hbox{ se }i=0\\\\\n\\frac{1}{n},&i=1,\\ldots,n-1\\\\\\frac{1}{2n},&\\hbox{ se }i=n\\end{array}\\right.\\]\n\n\nExercício Considerando a regra do trapézio prove que \\[\\hat{I}_n=\\sum_{i=0}^n w_i f(x_i),\\] onde \\(0=x_0,\\ldots,x_n=1\\) é o conjunto de nós e \\[w_i=\\left\\{\\begin{array}{ll}\\frac{1}{2n},&\\hbox{ se }i=0\\\\\n\\frac{1}{n},&i=1,\\ldots,n-1\\\\\\frac{1}{2n},&\\hbox{ se }i=n\\end{array}\\right.\\]\n\n\nExercício Resolva analiticamente a integral\n\\[\\int_0^1 (a+bx)dx\\]\nAgora, considerando \\(n=1\\), resolva essa integral utilizando a regra do ponto médio. Qual é a sua conclusão?\n\n\nExercício Considere a regra de Simpson para aproximar a integral de uma função \\(f(x)\\) no intervalo \\([0,1]\\) utilizando \\(n\\) subintervalos. A fórmula da aproximação é dada por:\n\\[\\hat{I}_n=\\frac{1}{3n}\\sum_{i=1}^n[f(x_{i-1})+4f(\\tilde{x}_i)+f(x_i)]\\] Mostre que essa mesma aproximação pode ser escrita como \\[\\hat{I}_n=\\sum_{j=0}^{2n}w_jf(y_j)\\] onde \\(y_j=j/2n\\) para \\(j=0,\\ldots 2n\\) e\n\\[w_j=\\left\\{\\begin{array}{ll} \\frac{1}{3n},&j=0\\\\\\frac{4}{3n},&j \\hbox{ é ímpar}\\\\\\frac{2}{3n},&j\\hbox{ é par diferente de } 2n\\\\ \\frac{1}{3n},&j=2n\\end{array}\\right.\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de quadratura</span>"
    ]
  }
]