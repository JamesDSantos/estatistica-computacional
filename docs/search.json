[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estatística Computacional I",
    "section": "",
    "text": "Preface\nEste material foi criado para a disciplina Estatística Computacional I, do curso de bacharelado em Estatística, da Universidade Federal do Amazonas.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html",
    "href": "solucoes_caso_real.html",
    "title": "1  Solução de equações",
    "section": "",
    "text": "1.1 Método da bisseção\nO método da bisseção é uma ferramenta versátil para encontrar raízes. Antes de apresentá-lo, é importante conhecer o Teorema do Valor Intermediário (ou Teorema de Bolzano).\nObserve que a condição do Teorema do Valor Intermediário pode ser reescrita para como \\[g(a)=f(a)-d \\leq 0 \\leq g(b)=f(b)-d,\\] (ou \\(g(b)\\leq 0\\leq g(a)\\)), o que implica que \\(c\\) é raíz de \\(g(x)\\). Temos o seguinte corolário.\nPortanto, se conhecemos um intervalo \\([a,b]\\) tal que \\(f(a)\\) e \\(f(b)\\) tem sinais opostos, então temos a certeza de que há pelo menos uma raíz de \\(f(x)\\) dentro do intervalo \\([a,b]\\). A Figure 1.1 ilustra o Teorema do Valor Intermediário, considerando o intervalo \\([a_0,b_0]\\) e a raiz \\(x^\\star\\).\nFigure 1.1: Ilustração do Teorema do Valor Médio\nO método da bisseção é baseado em aplicações sucessivas do Teorema do Valor Intermediário. Suponha que conhecemos um intervalo \\([a_0,b_0]\\) tal que \\(f(a_0)\\) e \\(f(b_0)\\) possuem sinais opostos e que contém a única raiz \\(x^\\star\\). Considere como candidato à solução o valor\n\\[x_1=\\frac{a_0+b_0}{2},\\]\nou seja, o ponto médio do intervalo. Portanto, podemos escrever \\([a_0,b_0]=[a_0,x_1]\\cup[x_1,b_0]\\). Ao calcular \\(f(x_1)\\), três coisas podem acontecer:\nA figura abaixo mostra a Figure 1.1 novamente, agora com o ponto \\(x_1\\) em vermelho. Também em vermelho está representado o novo intervalo, com metade do comprimento do anterior, que contém a raiz e satisfaz as condições do Teorema do Valor Intermediário.\nFigure 1.2: Obtenção de um novo intervalo para o Teorema do Valor Intermediário\nComo ilustrado acima, se o procedimento não encontra a solução, ele ao menos elimina metade do espaço de busca. Podemos então definir o novo candidato a solução \\[x_2=\\frac{a_1+b_1}{2},\\] isto é, ponto médio do novo intevalo e verificar se a raiz está no intervalo \\([a_1,x_2]\\) ou \\([x_2,b_1]\\). Ao realizar esse procedimento \\(n\\) vezes, teremos a sequência\n\\[[a_0,b_0]\\supset [a_1,b_1]\\supset\\cdots\\supset [a_{n},b_{n}],\\] onde o intervalo \\([a_n,b_n]\\) contém a raiz \\(x^\\star\\). O comprimento deste intervalo é \\[\\frac{b_0-a_0}{2^n}\\] o que implica que \\[|x_{n}-x^\\star|&lt;\\frac{b_0-a_0}{2^n}=\\varepsilon_n,\\] onde \\(\\varepsilon_n\\) é o erro máximo da aproximação. É imediato que \\(\\lim_{n\\rightarrow \\infty}\\varepsilon_n=0\\), o que implica que o método converge para a solução. Abaixo, apresentamos o algoritmo do método da bisseção.\nO método da bisseção consiste em realizar as sucessivas divisões de intervalos até a iteração \\(n\\) que satifaz o erro máximo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-da-bisseção",
    "href": "solucoes_caso_real.html#método-da-bisseção",
    "title": "1  Solução de equações",
    "section": "",
    "text": "Teorema do Valor Intermediário. Seja \\(f\\) uma função real definida no intervalo \\([a,b]\\). Então, se \\(f(a)\\leq d \\leq f(b)\\) ou \\(f(b)\\leq d \\leq f(a)\\), existe \\(c\\in[a,b]\\) tal que \\(f(c)=d\\).\n\n\n\nCorolário. Seja \\(f\\) uma função real definida no intervalo \\([a,b]\\). Então, se \\(f(a)\\) e \\(f(b)\\) possuem sinais opostos, então existe \\(x^\\star\\in[a,b]\\) tal que \\(x^\\star\\) é raíz de \\(f(x)\\).\n\n\n\n\n\n\n\n\\(f(x_1)=0\\). Nesse caso, \\(x_1\\) é a solução do problema.\n\\(f(x_1)\\) e \\(f(a_0)\\) tem o mesmo sinal. Isso implica que \\(f(x_1)\\) e \\(f(b_0)\\) possuem sinais opostos e, portanto, a solução deve estar em \\([x_1,b_0]\\). Podemos então definir o novo intervalo de busca \\([a_1,b_1]\\) onde \\(a_1=x_1\\) e \\(b_1=b_0\\).\n\\(f(x_1)\\) e \\(f(b_0)\\) tem o mesmo sinal. Isso implica que \\(f(x_1)\\) e \\(f(a_0)\\) possuem sinais opostos e, portanto, a solução deve estar em \\([a_0,x_1]\\). Podemos então definir o novo intervalo de busca \\([a_1,b_1]\\) onde \\(a_1=a_0\\) e \\(b_1=x_1\\).\n\n\n\n\n\n\nAlgoritmo 1 (Método da Bisseção).\nPara encontrar uma raiz de \\(f(x)\\) no intervalo \\([a,b]\\) com uma tolerância de \\(\\varepsilon\\):\nPré-condição: \\(f(a)f(b)&lt;0.\\)\n\nDefina a tolerância \\(\\varepsilon&gt;0.\\) e faça \\(i=1\\)\nEnquanto \\(∣b−a|\\geq\\varepsilon\\) repita os seguintes passos:\n\nCalcule \\(x_i=(a+b)/2\\)\nSe \\(f(x_i)=0\\), pare e retorne \\(x^\\star=x_i\\).\nAtualização do intervalo\n\n\nSe \\(f(x_i)f(a)&lt;0\\), a raiz está o intervalo \\([a,x_i]\\). Faça \\(b=x_i\\)\nSenão, a raiz está no intervalo \\([x_i,b]\\). Faça \\(a=x_i\\)\nFaça \\(i=i+1\\)\n\n\n\n\n\nExemplo Seja \\[f(x)=x^2−x-1\\] Vamos encontrar a raiz de \\(f\\) utilizando o método da bisseção. Observe que \\(f(1)=-1\\) e \\(f(2)=5\\) e, pelo Teorema do Valor Intermediário, há pelo menos uma raiz no intervalo [1,2]. O erro máximo na \\(n\\)-ésima iteração é\n\\[\\varepsilon_n=\\frac{1}{2^n}.\\] Escolhendo \\(n=15\\) temos \\(\\varepsilon_{15}=0.00003\\) (ou seja, uma precisão de quatro casas decimais). A tabela abaixo apresenta os intervalos e os pontos médios (soluções) para cada iteração.\n\n\n [1] 1.000000 1.500000 1.500000 1.500000 1.562500 1.593750 1.609375 1.617188\n [9] 1.617188 1.617188 1.617188 1.617676 1.617920 1.617920 1.617981 1.618011\n\n\n\n\n\nIteração\nSolução\nErro\n[a\nb]\n\n\n\n\n1\n1.500000\nInf\n1.500000\n2.000000\n\n\n2\n1.750000\n0.250000\n1.500000\n1.750000\n\n\n3\n1.625000\n0.125000\n1.500000\n1.625000\n\n\n4\n1.562500\n0.062500\n1.562500\n1.625000\n\n\n5\n1.593750\n0.031250\n1.593750\n1.625000\n\n\n6\n1.609375\n0.015625\n1.609375\n1.625000\n\n\n7\n1.617188\n0.007812\n1.617188\n1.625000\n\n\n8\n1.621094\n0.003906\n1.617188\n1.621094\n\n\n9\n1.619141\n0.001953\n1.617188\n1.619141\n\n\n10\n1.618164\n0.000977\n1.617188\n1.618164\n\n\n11\n1.617676\n0.000488\n1.617676\n1.618164\n\n\n12\n1.617920\n0.000244\n1.617920\n1.618164\n\n\n13\n1.618042\n0.000122\n1.617920\n1.618042\n\n\n14\n1.617981\n0.000061\n1.617981\n1.618042\n\n\n15\n1.618011\n0.000031\n1.618011\n1.618042",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-newton-raphson",
    "href": "solucoes_caso_real.html#método-newton-raphson",
    "title": "1  Solução de equações",
    "section": "1.2 Método Newton-Raphson",
    "text": "1.2 Método Newton-Raphson\nConsidere o problma de encontrar uma raiz de \\(f(x)\\). Seja \\(x_0\\) um ponto próximo da raiz \\(x^\\star\\). A equação da reta tangente ao ponto \\(x_0\\) é dada por\n\\[f(x)=f(x_0)+(x-x_0)f'(x_0).\\] A Figure 1.3 ilustra a reta tangente (em vermelho) ao ponto \\(x_0\\) para uma certa função. Observe que a raiz da reta tangente, marcada como \\(x_1\\) no gráfico, está mais próxima de \\(x^\\star\\) do que o ponto \\(x_0\\).\n\n\n\n\n\n\n\n\nFigure 1.3: Obtenção de um novo intervalo para o Teorema do Valor Intermediário\n\n\n\n\n\nA raiz \\(x_1\\) é calculada do seguinte modo: \\[0=f(x_0)+(x_1-x_0)f'(x_0)\\Rightarrow x_1=x_0-\\frac{f(x_0)}{f'(x_0)}.\\]\nPodemos então encontrar a reta tangente ao ponto \\(x_1\\). Por sua vez, a raiz desta reta, denominada por \\(x_2\\), estará mais próxima de \\(x^\\star\\) do que \\(x_1\\). Após realizar o mesmo procedimento \\(n\\) vezes teremos\n\\[x_n=x_{n-1}-\\frac{f(x_{n-1})}{f'(x_{n-1})}\\]\nIntuitivamente, temos que \\(x_n\\) deve estar mais próximo de \\(x^\\star\\) na medida que \\(n\\rightarrow\\infty\\). Abaixo, segue o algoritmo do método de Newton-Raphson.\n\nAlgoritmo 2. (Método de Newton-Raphson)\nPara encontrar a raiz \\(x^\\star\\) de \\(f(x)\\) com um erro \\(\\varepsilon\\).\nPré-condição: \\(f(x)\\) é diferenciável com \\(f'(x)\\neq 0\\) para qualquer \\(x\\) na vizinhança de \\(x^\\star\\). É necessário um valor \\(x_0\\) próximo de \\(x^\\star\\)\nDefina \\(x_{atual}=x_0\\) e erro=\\(+\\infty\\)\n\nEnquanto o erro for maior que \\(\\varepsilon\\) repita os seguintes passos:\n\nCalcule \\(x_m=x_{atual}-\\frac{f(x_{atual})}{f'(x_{atual})}\\)\nCalcule erro\\(=|x_m-x_{atual}|\\) e faça \\(x_{atual}=x_m\\).\n\nRetorne \\(x^\\star=x_m\\).\n\n\nO método\n\nTeorema de Kantorovich (simplificado) Seja \\(x_0\\) um ponto inicial para o método Newton-Raphson. A convergência para a raiz de \\(f(x)\\) é garantida se:\n\n\\(f'(x_0)\\neq 0\\)\nExiste uma constante \\(L\\) que limita a segunda derivada, ou seja \\(|f''(x_0)|\\leq L\\) em algum intervalo contento \\(x_0\\).\nA seguinte condição é satisfeita: \\[h_0=\\frac{|f(x_0)|}{f'(x_0)^2}L\\leq \\frac{1}{2}.\\]\n\n\n\nMétodos de quase-Newton.\nUma das desvantagens do método de Newton-Raphson é a necessidade de obter uma expressão analítica para a derivada de \\(f(x)\\). Qualquer método que utilize a mesma estrutura do Newton-Raphson mas troca a forma analítica de \\(f'(x)\\) por uma aproximação pertence à classe de métodos de quase-Newton.\n\n.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-da-secante",
    "href": "solucoes_caso_real.html#método-da-secante",
    "title": "1  Solução de equações",
    "section": "1.3 Método da secante",
    "text": "1.3 Método da secante\nSeja \\(x^\\star\\) a raiz de \\(f(x)\\) e sejam \\(x_0\\) e \\(x_1\\) dois valores próximos de \\(x^\\star\\). A reta secante aos pontos \\((x_0,f(x_0))\\) e \\((x_1,f(x_1))\\) é dada por \\[f(x)=f(x_1)+ (x-x_1)\\frac{f(x_1)-f(x_0)}{x_1-x_0}.\\]\nAssim como no métod Newton-Raphson, a raiz da reta secante acima tende a estar mais próxima de \\(x^\\star\\) do que \\(x_0\\) e \\(x_1\\). A Figure 1.4 ilustra essa afirmação, onde o ponto \\(x_2\\) é a raiz da reta secante (em vermelho).\n\n\n\n\n\n\n\n\nFigure 1.4: Obtenção de um novo pontos utilizando a reta secante formada a partir de dois pontos iniciais.\n\n\n\n\n\nO ponto \\(x_2\\) é obtido do seguinte modo:\n\\[0=f(x_1)+(x_2-x_1)\\frac{f(x_1)-f(x_)}{x_1-x_0}\\Rightarrow x_2=x_1-f(x_1)\\frac{x_1-x_0}{f(x_1)-f(x_0)}.\\] De modo análogo, podemos construir a reta secante aos pontos \\((x_1,f(x_1))\\) e \\((x_2,f(x_2))\\) e encontrar sua raiz \\(x_3\\). Ao realizar esse procedimento diversas vezes, teremos que o ponto \\(x_n\\) será dado por\n\\[ x_n=x_{n-1}-f(x_{n-1})\\frac{x_{n-1}-x_{n-2}}{f(x_{n-1})-f(x_{n-2})}.\\] Podemos continuar a iteração até obter \\(|x_n-x_{n-1}|&lt;\\varepsilon\\).\n\nAlgoritmo 3. (Método da secante)\nPara encontrar a raiz \\(x^\\star\\) de \\(f(x)\\) com um erro \\(\\varepsilon\\).\nPré-condição: São necessários valores \\(x_0\\) e \\(x_1\\) próximos de \\(x^\\star\\)\nDefina erro=\\(+\\infty\\) e \\(n=1\\)\n\nEnquanto o erro for maior que \\(\\varepsilon\\) repita os seguintes passos:\n\n\\(n=n+1\\)\nCalcule \\(x_n=x_{n-1}-f(x_{n-1})\\frac{x_{n-1}-x_{n-2}}{f(x_{n-1})-f(x_{n-2})}\\)\nCalcule erro\\(=|x_n-x_{n-1}|\\).\n\nRetorne \\(x^\\star=x_n\\).\n\n\n\nExemplo - (W de Lambert) Considere o problema de encontrar a raiz de \\(f(x)= e^{-x}-x\\). O resultado, conhecido como constante de \\(W\\) de Lambert, é um número irracional e é aproximadamente 0,567143.\nVamos escolher os pontos iniciais \\(x_0=0\\) e \\(x_1=1\\), onde \\[\\begin{align}f(0)&=e^{-0}-0=1\\\\f(1)&=e^{-1}-1\\approx-0,6321.\\end{align}\\] A fórmula de iteração é dada por\n\\[x_n=x_{n-1}-(e^{-x_{n-1}}-1)\\frac{x_{n-1}-x_{n-2}}{e^{-x_{n-1}}-x_{n-1}-e^{-x_{n-2}}+x_{n-2}}.\\]\nVamos fixar o erro em \\(10^{-7}\\) para obter uma precisão de 6 casas decimais.\n\n\n\n\n\nIterações\nx\nErro\n\n\n\n\n1\n0.612700\n0.387300\n\n\n2\n0.563838\n0.048861\n\n\n3\n0.567170\n0.003332\n\n\n4\n0.567143\n0.000027\n\n\n5\n0.567143\n0.000000\n\n\n\n\n\n\nFica como exercício provar que o método da secante pertence à classe dos métodos de quase-Newton. Assim como o método de Newton-Raphson, esse método tem boas propriedades para funções duas vezes continuamente diferenciáveis, como mostra o teorema abaixo.\n\nTeorema da convergência do método da secante. Seja \\(f\\) uma função real, contínua e duas vezes diferenciável no intervalo \\([a,b]\\), onde \\(x^\\star\\in(a,b)\\) é a única raiz nesse intervalo e \\(f'(x^\\star)\\neq 0\\)\nSe as soluções iniciais \\(x_0\\) e \\(x_1\\) forem escolhidas suficientemente próximas de \\(x^\\star\\) então a sequência \\(x_2,\\ldots,x_n\\) produzida pelo método da secante irá convergir para \\(x^\\star\\).\n\n\nAtenção. O teorema acima mostra que não basta escolher dois valores dentro do intervalo \\([a,b]\\) que contém \\(x^\\star\\). Ou seja, diferente do método da bisseção, essa escolha não é garantia de convergência. Isso ocorre porque, dependendo da função, a escolha dos pontos iniciais pode levar a geração de candidatos para fora do intervalo \\([a,b]\\) fazendo com que a convergência demore ou mesmo falhe.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-da-interpolação-quadrática-inversa-método-de-muller",
    "href": "solucoes_caso_real.html#método-da-interpolação-quadrática-inversa-método-de-muller",
    "title": "1  Solução de equações",
    "section": "1.4 Método da interpolação quadrática inversa (Método de Muller)",
    "text": "1.4 Método da interpolação quadrática inversa (Método de Muller)\nO método da interpolação quadrática inversa segue a mesma ideia do método da secante. Considere três pontos, \\(x_1,x_2,x_3\\) próximos de \\(x^\\star\\). Na \\(i\\)-ésima iteração, o próximo candidato à solução é a raiz da parábola que passa pelos pontos \\(x_{i-1},x_{i-2},x_{i-3}\\), dada por\n\\[x_{i} = x_{i-1} - \\frac{2f(x_{i-1})}{\\omega \\pm \\sqrt{\\omega^2 - 4f(x_{i-1})\\delta_{i-1}}}\\] onde \\[\\begin{align}\n\\delta_{i-1} &= \\frac{f(x_{i-1})-f(x_{i-2})}{x_{i-1}-x_{i-2}} + \\frac{f(x_{i-1})-f(x_{i-3})}{x_{i-1}-x_{i-3}} - \\frac{f(x_{i-2})-f(x_{i-3})}{x_{i-2}-x_{i-3}}\\\\\n\\omega &= \\frac{f(x_{i-1})-f(x_{i-2})}{x_{i-1}-x_{i-2}} + \\delta_{i-1}(x_{i-1}-x_{i-2})\n\\end{align}\\]\nEmbora esse método possa convergir rapidamente para a raiz, os pontos iniciais devem estar próximos da raiz. Além disso, o algoritmo pode falhar se em algum momento os valores de \\(f(x_i),f(x_{i-1})\\) ou \\(f(x_{i-2})\\) concidirem. Portanto, é recomendado que esse método seja utilizado em conjunto com outro, conforme discutido na seção Métodos Híbridos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-da-falsa-posição-regula-falsi",
    "href": "solucoes_caso_real.html#método-da-falsa-posição-regula-falsi",
    "title": "1  Solução de equações",
    "section": "1.5 Método da falsa posição (regula falsi)",
    "text": "1.5 Método da falsa posição (regula falsi)\nO método da falsa posição é uma combinação do método da secante com o método da bisseção. Assim como neste último, é necessário começar com um intervalo \\([x_a,x_b]\\) no qual \\(f(x_a)\\) e \\(f(x_b)\\) têm sinais opostos.\nPrimeiro, calcula-se a raiz da reta secante que passa pelos pontos \\((x_a,f(x_a))\\) e \\((x_b,f(x_b))\\), dada por\n\\[\nx_c=x_b-f(x_b)\\frac{x_b-x_a}{f(x_b)-f(x_a)}.\n\\] Em seguida, verifica-se qual dos intervalos \\([x_a,x_c]\\) ou \\([x_c,x_b]\\) contém a raiz. Então, repete-se a busca.\nAssim como ocorre no método da bisseção, o método da falsa posição cria uma sequência de intervalos encaixados que contém a raiz, convergindo portanto para a verdadeira solução. Espera-se que a escolha de \\(x_c\\) esteja mais próxima da raiz do que o ponto médio do intervalo.\n\nAlgoritmo 4 (Método da Falsa Posição).\nPara encontrar uma raiz de \\(f(x)\\) no intervalo \\([a,b]\\) com uma tolerância de \\(\\varepsilon\\):\nPré-condição: \\(f(a)f(b)&lt;0.\\)\n\nDefina a tolerância \\(\\varepsilon&gt;0.\\) e faça \\(i=1\\)\nEnquanto \\(∣b−a|\\geq\\varepsilon\\) repita os seguintes passos:\n\nCalcule \\(x_i=b - f(b)\\frac{b-a}{f(b)-f(a)}\\)\nSe \\(f(x_i)=0\\), pare e retorne \\(x^\\star=x_i\\).\nAtualização do intervalo\n\n\nSe \\(f(x_i)f(a)&lt;0\\), a raiz está o intervalo \\([a,x_i]\\). Faça \\(b=x_i\\)\nSenão, a raiz está no intervalo \\([x_i,b]\\). Faça \\(a=x_i\\)\nFaça \\(i=i+1\\)\n\n\n\n\nExemplo Seja \\[f(x)=x^2−x-1\\] Anteriormente, encontramos a raiz de \\(f\\), iniciando com o intervalo [1,2] e fixando erro de \\(0,00003\\), que foi obtido em 15 iterações. A tabela abaixo apresenta os intervalos e os pontos médios (soluções) para o método da falsa posição até a obtenção do mesmo erro fixado no método da bisseção.\n\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\n\n\n\nIteração\nSolução\nErro\n[a\nb]\n\n\n\n\n1\n1.800000\n1.000000\n1\n1.800000\n\n\n2\n1.712871\n0.087129\n1\n1.712871\n\n\n3\n1.669659\n0.043212\n1\n1.669659\n\n\n4\n1.646784\n0.022875\n1\n1.646784\n\n\n5\n1.634245\n0.012539\n1\n1.634245\n\n\n6\n1.627238\n0.007007\n1\n1.627238\n\n\n7\n1.623280\n0.003958\n1\n1.623280\n\n\n8\n1.621031\n0.002249\n1\n1.621031\n\n\n9\n1.619748\n0.001283\n1\n1.619748\n\n\n10\n1.619015\n0.000733\n1\n1.619015\n\n\n11\n1.618596\n0.000419\n1\n1.618596\n\n\n12\n1.618356\n0.000240\n1\n1.618356\n\n\n13\n1.618218\n0.000137\n1\n1.618218\n\n\n14\n1.618140\n0.000079\n1\n1.618140\n\n\n15\n1.618094\n0.000045\n1\n1.618094",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#métodos-híbridos",
    "href": "solucoes_caso_real.html#métodos-híbridos",
    "title": "1  Solução de equações",
    "section": "1.6 Métodos híbridos",
    "text": "1.6 Métodos híbridos\nOs métodos discutidos até o momento possuem vantagens e desvantagens, conforme sumarizado na tabela abaixo.\n\n\n\n\n\n\n\n\n\nMétodo\nOrdem de Convergência\nVantagem\nDesvantagem\n\n\n\n\nBisseção\nLinear\nGarante a convergência. Não exige o cálculo da derivada.\nA convergência é lenta.\n\n\nFalsa Posição\nLinear\nGeralmente converge mais rápido que a bisseção. Não exige o cálculo da derivada.\nPode ter convergência lenta se a função for muito côncava ou convexa.\n\n\nSecante\nSuperlinear\nConvergência rápida, sem precisar calcular a derivada.\nPode divergir se a aproximação inicial for ruim.\n\n\nInterpolação Quadrática Inversa\nSuperlinear\nGeralmente mais rápido que o método da secante. Pode encontrar raízes complexas.\nExige três pontos iniciais. Pode ser instável e falhar se os pontos forem colineares.\n\n\nNewton-Raphson\nQuadrática\nConvergência extremamente rápida (se as condições forem atendidas).\nExige o cálculo da derivada. Pode divergir se a aproximação inicial for ruim.\n\n\n\nOs métodos híbridos são algoritmos que decidem em cada iteração qual método utilizar. Os objetivos dessas decisões são acelerar e garantir a convergência. Vamos apresentar a ideia por trás de dois métodos: Dekker e Brent.\nO método de Dekker (1969) combina o método da bisseção com o da secante. sua ideia básica é utilizar o método da secante sempre que possível, criando verificações lógicas que evitem que o candidato à solução se afaste da raiz. Ele começa com um intervalo \\([a,b]\\) que contém a raiz \\(x^\\star\\) (ou seja, \\(f(a)\\) e \\(f(b)\\) tem sinais opostos). Na sua \\(i\\)-ésima iteração:\n\nGeração dos candidatos. Dois candidatos são construídos, utilizando o método da bisseção e o da secante:\n\n\\[\\begin{align}x_m&=\\frac{a_{i-1}+b_{i-1}}{2}\\\\x_s&=b_{i-1}-f(b_{i-1})\\frac{b_{i-1}-a_{i-1}}{f(b_{i-1})-f(a_{i-1})} \\end{align}\\] 2. Freio da secante. Se \\(x_s\\in[x_m,b_{i-1}]\\), então o ponto da secante não se afastou da solução e fazemos \\(b_{i}=x_s\\). Senão, escolhemos \\(b_i=x_m\\) por segurança. O objetivo desse passo é evitar utilizar um ponto \\(x_s\\) afastado da solução.\n\nDefinindo o novo intervalo. Se \\(f(a_{i-1})\\) e \\(f(b_i)\\) tem sinais oposto, então \\(a_i=a_{i-1}\\). Em caso contrário, \\(a_i=b_{i-1}\\)\nPermuta Se \\(|f(a_i)|&lt;|f(b_i)|\\), então é provável que \\(a_i\\) seja um palpite melhor que \\(b_i\\). Então os valores \\(a_i\\) e \\(b_i\\) são permutados. Isso garante que a solução será dada pel convergência da sequência \\(b_1,b_2,\\ldots\\)\n\nO método de Brent (1973) é uma modificação do método de Dekker, començando também com um intervalo \\([a,b]\\) que contém a raiz. A principal diferença é que Brent adiciona mais verificações para garantir que a interpolação não apenas se aproxime da raiz, mas o faça de forma eficiente. Ele utiliza a interpolação quadrática inversa (que é mais rápida que a secante) e recorre à bisseção quando a interpolação não faz um progresso significativo.\nEm geral, o método itera nos seguintes passos:\n\nGeração do candidato: O algoritmo tenta encontrar um novo ponto, \\(x_s\\), usando a interpolação quadrática inversa. Se essa interpolação não for possível, ele recorre ao método da secante.\nTestes de segurança: Antes de aceitar \\(x_s\\), ele é submetido a uma série de testes que o comparam com a garantia de progresso da bisseção. Se \\(x_s\\) não for significativamente melhor do que um passo de bisseção, o algoritmo descarta \\(x_s\\) e, em vez disso, usa o ponto médio \\(x_m\\) como candidato.\nAtualização do intervalo. O novo intervalo é atualizado com base no candidato aceito (\\(x_s\\) ou \\(x_m\\)), garantindo que a raiz continue delimitada e que o ponto com o menor valor de \\(f(x)\\) seja sempre a melhor estimativa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#funções-para-encontrar-raízes-no-r",
    "href": "solucoes_caso_real.html#funções-para-encontrar-raízes-no-r",
    "title": "1  Solução de equações",
    "section": "1.7 Funções para encontrar raízes no R",
    "text": "1.7 Funções para encontrar raízes no R\nO R oferece várias funções para encontrar raízes de equações não-lineares, tanto no pacote base (stats) quanto em pacotes externos.\n\n1.7.1 Pacote stats\nA função uniroot é a implementação do método de Brent. Seus principais argumentos são:\n\nf: a função da qual se deseja obter a raiz\ninterval: vetor contendo os pontos extremos do intervalo de busca\ntol: tolerância desejada (o valor padrão é 10^{-10})\nmaxiter: número máximo de tentativas (o valor padrão é 1000)\n\n\n\n1.7.2 Pacote pracma\nO pacote pracma traz diversas rotinas de cálculo numérico, incluindo os métodos clássicos de busca de raízes.\nOs métodos da bisseção, secante e falsa posição estão implementados nas funções bisect, secant e regulaFalsi, respectivamente. Seus argumentos são\n\nf: a função da qual se deseja obter a raiz\na: limite inferior do intervalo para a bisseção e falsa posição; um ponto perto da raiz para a secante\nb: limite superior do intervalo para a bisseção e falsa posição; um ponto perto da raiz para a secante\ntol: tolerância desejada (o valor padrão é \\(10^{-10}\\))\nmaxiter: número máximo de tentativas (o valor padrão é 100)\n\nO método da interpolação quadrática inversa (Muller) está implementado na função muller, cujos argumentos são\n\nf: a função da qual se deseja obter a raiz\np0,p1,p2: três pontos iniciais, próximos da raiz\ntol: tolerância desejada (o valor padrão é .0001)\nmaxiter: número máximo de tentativas (o valor padrão é \\(10^{-10}\\))\n\nO método de Brent está implementado na função brent e possui os argumentos\n\nf: a função da qual se deseja obter a raiz\na,b: intervalo que contém a raiz (as imagens nas extremidades do intervalo devem ter sinais opostos)\ntol: tolerância desejada (o valor padrão é \\(10^{-12}\\))\nmaxiter: número máximo de tentativas (o valor padrão é 500)\n\nA função newtonRaphson (ou de modo equivalente, newton) encontra a raiz da função utilizando o método Newton-Raphson. Os argumentos são\n\nfun: a função da qual se deseja obter a raiz\nx0: valor inicial, próximo da raiz\ndfun: a função da derivada de \\(f\\). Se dfun=NULL, o método da secante será utilizado\ntol: tolerância desejada (o valor padrão é \\(10^{-8}\\))\nmaxiter: número máximo de tentativas (o valor padrão é 500)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#exercícios",
    "href": "solucoes_caso_real.html#exercícios",
    "title": "1  Solução de equações",
    "section": "1.8 Exercícios",
    "text": "1.8 Exercícios\n\nExercício Seja \\(f:[a,b]\\rightarrow\\mathbb{R}\\) uma função contínua tal que \\(f(a)f(b)&lt;0\\). O método da bisseção gera uma sequência de intervalos encaixados \\([a_k,b_k]\\) onde a raiz \\(x^\\star\\in[a_k,b_k]\\) para todo \\(k\\in\\mathbb{N}\\).\n\nDiscuta a unicidade da raiz encontrada por esse método. Sob quais condições o Método da Bisseção garante que a raiz encontrada é única no intervalo inicial \\([a,b]?\\)\nSe a função tiver múltiplas raízes no intervalo, o que podemos afirmar sobre a raiz que o método converge?\n\n\n\nExercício Implemente o método da bisseção para encontrar a raiz da função \\[f(x)=\\log(x)+x^2-4.\\]\nUtilize o intervalo inicial \\([1,2]\\). (Verifique previamente se \\(f(1)\\) e \\(f(2)\\) possuem sinais opostos). O critério de parada deve ser a obtenção de um erro absoluto máximo inferior a \\(10^{−6}\\). Ao final, imprima:\n\nA raiz aproximada encontrada.\nO número total de iterações realizadas.\nUma tabela com os valores \\(x_n\\), \\(f(x_n)\\) e \\(\\varepsilon_n\\).\n\n\n\nExemplo. Seja \\(X\\) uma variável aleatória contínua, cuja função distribuição é dada por \\(F(x)\\). O quantil \\(100p\\%\\) é o valor \\(x_p\\) que satisfaz \\[F(x_p)=p.\\] Explique porque existe um único \\(x_p\\) que satifaz a equação acima e utilize esse fato para descrever como obter \\(x_p\\) a partir do método da bisseção.\n\n\nExercício. Seja \\(X\\) uma variável aleatória com a seguinte função distribuição \\[F(x)=1-\\frac{1}{5}(x^2+4x+5) e^{-x},\\] com \\(x&gt;0\\). Encontre a mediana de \\(X\\) utilizando o método da bisseção.\n\n\nExercício Considere o problema de resolver \\(\\sqrt{x}=3\\). Prove que o método de Newton-Raphson vai convergir para a verdadeira solução se o valor inicial \\(x_0\\) satisfaz\n\\[\\frac{|x_0^2-7|}{2x_0^2}\\leq \\frac{1}{2}.\\]\n\n\nExercício. Neste exercício você vai demonstrar o Teorema da Convergência do método da secante. Considere que as condições do teorema estão satisfeitas. É fato que, para qualquer ponto \\(x\\) próximo de \\(x^\\star\\), a função \\(f(x)\\) pode ser aproximada por\n\\[f(x)\\approx f(x^\\star) +(x-x^\\star)f'(x^\\star)+\\frac{1}{2}(x-x^\\star)f''(x^\\star)\\] onde a aproximação é obtida por expansão em séries de Taylor. Considere então que \\(x_n\\) e \\(x_{n-1}\\) estão próximas de \\(x^\\star\\). Seja \\(\\xi_n=x_n-x^\\star\\).\n\nProve que \\[\\begin{align}f(x_n)\\approx \\xi_nf'(x^\\star)+\\frac{1}{2}\\xi_n^2 f''(x^\\star)\\end{align}\\]\nMostre que\n\n\\[\\begin{align}f(x_n)-f(x_{n-1})\\approx (\\xi_n-\\xi_{n-1})\\left[f'(x^\\star)+\\frac{1}{2}f''(x^\\star)(\\xi_n+\\xi_{n-1})\\right]\\end{align}\\]\n\nMostre que a fórmula de iteração do método da secante pode ser reescrita como\n\n\\[\\xi_{n+1}=\\xi_n-f(x_n)\\frac{\\xi_n-\\xi_{n-1}}{f(x_n)-f(n-1)}\\]\n\nA partir dos passos 2 e 3, prove que\n\n\\[\\xi_{n+1}=\\xi_n-\\frac{f(x_n)}{f'(x^\\star)+\\frac{1}{2}f''(x^\\star)(\\xi_n+\\xi_{n-1})}\\]\n\nUtilize a aproximação para \\(f(x_n)\\) para mostrar que\n\n\\[\\xi_{n+1}\\approx\\xi_n-\\frac{\\xi_nf'(x^\\star)+\\frac{1}{2}\\xi_n^2 f''(x^\\star)}{f'(x^\\star)+\\frac{1}{2}f''(x^\\star)(\\xi_n+\\xi_{n-1})}=\\xi_n-\\frac{\\xi_n+\\frac{1}{2}\\xi_n^2 \\frac{f''(x^\\star)}{f'(x^\\star)} }{1+\\frac{1}{2}\\frac{f''(x^\\star)}{f'(x^\\star)}(\\xi_n+\\xi_{n-1})}\\]\n\nSe \\(u\\) é próximo de zero, é verdade que \\[\\frac{1}{1+u}\\approx 1-u\\] Como \\((\\xi_n+\\xi_{n-1})\\) é próximo de zero faça \\[u=\\frac{1}{2}\\frac{f''(x^\\star)}{f'(x^\\star)}(\\xi_n+\\xi_{n-1})\\] e mostre que \\[\\xi_{n+1}\\approx\\xi_n-\\frac{\\xi_nf'(x^\\star)+\\frac{1}{2}\\xi_n^2 f''(x^\\star)}{f'(x^\\star)+\\frac{1}{2}f''(x^\\star)(\\xi_n+\\xi_{n-1})}=\\xi_n-\\left(\\xi_n+\\frac{1}{2}\\xi_n^2 \\frac{f''(x^\\star)}{f'(x^\\star)}\\right)\\left(1-\\frac{1}{2}\\frac{f''(x^\\star)}{f'(x^\\star)}(\\xi_n+\\xi_{n-1})\\right)\\]\nComo \\(\\varepsilon_n\\) é pequeno, o valor de \\(\\varepsilon_n^2\\approx 0\\). Utilize essa informação para mostrar que\n\n\\[\\xi_{n+1}\\approx \\frac{1}{2}\\frac{f''(x^\\star)}{f'(x^\\star)}\\xi_n\\xi_{n-1}\\] Isto implica que \\(|\\xi_{n+1}|&lt;|\\xi_n|\\)\n\nConclua que, se \\(x_0,x_1\\) estão próximos o suficiente da raiz, então o método da secante é convergente.\n\n\n\nExercício. Dizemos que \\(X\\) tem distribuição Lindley(\\(\\theta\\)) se sua função densidade é dada por \\[f(x|\\theta)=\\frac{\\theta^2}{1+\\theta}(1+x)e^{-\\theta x},\\] onde \\(x,\\theta&gt;0\\). Pode-se provar que\n\\[F(x|\\theta)=1-\\frac{1+\\theta(1+x)}{1+\\theta}e^{-\\theta x}\\] Para \\(\\theta=1\\) e considerando um erro de \\(10^{-4}\\), encontre a mediana desse modelo considerando os métodos\n\nBisseção\nSecante\nFalsa Posição\nMuller\nBrent\nNewton-Raphson\n\nVocê pode utilizar as funções ja implementadas no R. Para cada método, guarde o número de iterações até a convergência. Qual método convergiu mais rápido?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  }
]