[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estatística Computacional I",
    "section": "",
    "text": "Preface\nEste material foi criado para a disciplina Estatística Computacional I, do Curso de Bacharelado em Estatística, da Universidade Federal do Amazonas.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html",
    "href": "solucoes_caso_real.html",
    "title": "1  Solução de equações",
    "section": "",
    "text": "1.1 Método da bisseção\nSeja \\(f(x)\\) uma função real e contínua. O valor \\(x^\\star\\) que satisfaz\n\\[f(x^\\star)=0\\] é denominado raíz, ou zero da função.\nConsidere agora o problema de encontrar o valor \\(x\\) tal que\n\\[f(x)=a.\\] Isto é equivalente a encontrar a raiz da função \\(g(x)\\), dada por\n\\[g(x)=f(x)-a\\] Portanto, todo problema de solução de equações pode ser reescrito como um outro de encontrar raízes.\nDizemos que uma equação possui solução analítica quando conseguimos encontrar a raíz explicitamente. Por exemplo, a equação \\(f(x)=2x+4\\) e considere o problema de encontrar a solução para \\(f(x)=5\\). Observe que\n\\[f(x)=5\\Rightarrow 2x+1=5\\Rightarrow 2x-4=0,\\] o que implica que devemos encontrar a raiz de \\(g(x)=2x-4\\). Note que \\(x^\\star=2\\) é a raiz e, portanto, a solução desejada.\nExistem diversas equações que não podem ser resolvidas com simples manipulações algébricas, como por exemplo\n\\[x^3−x=1\\]\nQuando nos deparamos com esse tipo de equação, recorremos aos métodos numéricos. Estes são algoritmos que geram uma sequência de aproximações que convergem para a solução real com uma precisão desejada.\nA escolha do método depende do tipo de equação e da natureza das variáveis envolvidas.\nO método da bisseção é uma ferramenta versátil para encontrar raízes. Antes de apresentá-lo, é importante conhecer o Teorema do Valor Intermediário (ou Teorema de Bolzano).\nObserve que a condição do Teorema do Valor Intermediário pode ser reescrita para como \\[g(a)=f(a)-d \\leq 0 \\leq g(b)=f(b)-d,\\] (ou \\(g(b)\\leq 0\\leq g(a)\\)), o que implica que \\(c\\) é raíz de \\(g(x)\\). Temos o seguinte corolário.\nPortanto, se conhecemos um intervalo \\([a,b]\\) tal que \\(f(a)\\) e \\(f(b)\\) tem sinais opostos, então temos a certeza de que há pelo menos uma raíz de \\(f(x)\\) dentro do intervalo \\([a,b]\\). A Figure 1.1 ilustra o Teorema do Valor Intermediário, considerando o intervalo $[a_0,b_0]$$ e a raiz \\(x^\\star\\).\nFigure 1.1: Ilustração do Teorema do Valor Médio\nO método da bisseção é baseado em aplicações sucessivas do Teorema do Valor Intermediário. Suponha que conhecemos um intervalo \\([a_0,b_0]\\) tal que \\(f(a_0)\\) e \\(f(b_0)\\) possuem sinais opostos e que contém a única raiz \\(x^\\star\\). Considere como candidato à solução o valor\n\\[x_1=\\frac{a_0+b_0}{2},\\]\nou seja, o ponto médio do intervalo. Portanto, podemos escrever \\([a_0,b_0]=[a_0,x_1]\\cup[x_1,b_0]\\). Ao calcular \\(f(x_1)\\), três coisas podem acontecer:\nA figura abaixo mostra a Figure 1.1 novamente, agora com o ponto \\(x_1\\) em vermelho. Também em vermelho está representado o novo intervalo, com metade do comprimento do anterior, que contém a raiz e satisfaz as condições do Teorema do Valor Intermediário.\nFigure 1.2: Obtenção de um novo intervalo para o Teorema do Valor Intermediário\nComo ilustrado acima, se o procedimento não encontra a solução, ele ao menos elimina metade do espaço de busca. Podemos então definir o novo candidato a solução \\[x_2=\\frac{a_1+b_1}{2},\\] isto é, ponto médio do novo intevalo e verificar se a raiz está no intervalo \\([a_1,x_2]\\) ou \\([x_2,b_1]\\). Ao realizar esse procedimento \\(n\\) vezes, teremos a sequência\n\\[[a_0,b_0]\\supset [a_1,b_1]\\supset\\cdots\\supset [a_{n},b_{n}],\\] onde o intervalo \\([a_n,b_n]\\) contém a raiz \\(x^\\star\\). O comprimento deste intervalo é \\[\\frac{b_0-a_0}{2^n}\\] o que implica que \\[|x_{n}-x^\\star|&lt;\\frac{b_0-a_0}{2^n}=\\varepsilon_n,\\] onde \\(\\varepsilon_n\\) é o erro máximo da aproximação. É imediato que \\(\\lim_{n\\rightarrow \\infty}\\varepsilon_n=0\\), o que implica que o método converge para a solução. Abaixo, apresentamos o algoritmo do método da bisseção.\nO método da bisseção consiste em realizar as sucessivas divisões de intervalos até a iteração \\(n\\) que satifaz o erro máximo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-da-bisseção",
    "href": "solucoes_caso_real.html#método-da-bisseção",
    "title": "1  Solução de equações",
    "section": "",
    "text": "Teorema do Valor Intermediário. Seja \\(f\\) uma função real definida no intervalo \\([a,b]\\). Então, se \\(f(a)\\leq d \\leq f(b)\\) ou \\(f(b)\\leq d \\leq f(a)\\), existe \\(c\\in[a,b]\\) tal que \\(f(c)=d\\).\n\n\n\nCorolário. Seja \\(f\\) uma função real definida no intervalo \\([a,b]\\). Então, se \\(f(a)\\) e \\(f(b)\\) possuem sinais opostos, então existe \\(x^\\star\\in[a,b]\\) tal que \\(x^\\star\\) é raíz de \\(f(x)\\).\n\n\n\n\n\n\n\n\\(f(x_1)=0\\). Nesse caso, \\(x_1\\) é a solução do problema.\n\\(f(x_1)\\) e \\(f(a_0)\\) tem o mesmo sinal. Isso implica que \\(f(x_1)\\) e \\(f(b_0)\\) possuem sinais opostos e, portanto, a solução deve estar em \\([x_1,b_0]\\). Podemos então definir o novo intervalo de busca \\([a_1,b_1]\\) onde \\(a_1=x_1\\) e \\(b_1=b_0\\).\n\\(f(x_1)\\) e \\(f(b_0)\\) tem o mesmo sinal. Isso implica que \\(f(x_1)\\) e \\(f(a_0)\\) possuem sinais opostos e, portanto, a solução deve estar em \\([a_0,x_1]\\). Podemos então definir o novo intervalo de busca \\([a_1,b_1]\\) onde \\(a_1=a_0\\) e \\(b_1=x_1\\).\n\n\n\n\n\n\nAlgoritmo 1 (Método da Bisseção).\nPara encontrar uma raiz de \\(f(x)\\) no intervalo \\([a,b]\\) com uma tolerância de \\(\\varepsilon\\):\nPré-condição: \\(f(a)f(b)&lt;0.\\)\n\nDefina a tolerância \\(\\varepsilon&gt;0.\\) e faça \\(i=1\\)\nEnquanto \\(∣b−a|\\geq\\varepsilon\\) repita os seguintes passos:\n\nCalcule \\(x_i=(a+b)/2\\)\nSe \\(f(x_i)=0\\), pare e retorne \\(x^\\star=x_i\\).\nAtualização do intervalo\n\n\nSe \\(f(x_i)f(a)&lt;0\\), a raiz está o intervalo \\([a,x_i]\\). Faça \\(b=x_i\\)\nSenão, a raiz está no intervalo \\([x_i,b]\\). Faça \\(a=x_i\\)\nFaça \\(i=i+1\\)\n\n\n\n\n\nExample 1.1 Seja \\[f(x)=x^2−x-1\\] Vamos encontrar a raiz de \\(f\\) utilizando o método da bisseção. Observe que \\(f(1)=-1\\) e \\(f(2)=5\\) e, pelo Teorema do Valor Intermediário, há pelo menos uma raiz no intervalo [1,2]. O erro máximo na \\(n\\)-ésima iteração é\n\\[\\varepsilon_n=\\frac{1}{2^n}.\\] Escolhendo \\(n=15\\) temos \\(\\varepsilon_{15}=0.00003\\) (ou seja, uma precisão de quatro casas decimais). A tabela abaixo apresenta os intervalos e os pontos médios (soluções) para cada iteração.\n\n\n [1] 1.000000 1.500000 1.500000 1.500000 1.562500 1.593750 1.609375 1.617188\n [9] 1.617188 1.617188 1.617188 1.617676 1.617920 1.617920 1.617981 1.618011\n\n\n\n\n\nIteração\nSolução\nErro\n[a\nb]\n\n\n\n\n1\n1.500000\nInf\n1.500000\n2.000000\n\n\n2\n1.750000\n0.250000\n1.500000\n1.750000\n\n\n3\n1.625000\n0.125000\n1.500000\n1.625000\n\n\n4\n1.562500\n0.062500\n1.562500\n1.625000\n\n\n5\n1.593750\n0.031250\n1.593750\n1.625000\n\n\n6\n1.609375\n0.015625\n1.609375\n1.625000\n\n\n7\n1.617188\n0.007812\n1.617188\n1.625000\n\n\n8\n1.621094\n0.003906\n1.617188\n1.621094\n\n\n9\n1.619141\n0.001953\n1.617188\n1.619141\n\n\n10\n1.618164\n0.000977\n1.617188\n1.618164\n\n\n11\n1.617676\n0.000488\n1.617676\n1.618164\n\n\n12\n1.617920\n0.000244\n1.617920\n1.618164\n\n\n13\n1.618042\n0.000122\n1.617920\n1.618042\n\n\n14\n1.617981\n0.000061\n1.617981\n1.618042\n\n\n15\n1.618011\n0.000031\n1.618011\n1.618042",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-newton-raphson",
    "href": "solucoes_caso_real.html#método-newton-raphson",
    "title": "1  Solução de equações",
    "section": "1.2 Método Newton-Raphson",
    "text": "1.2 Método Newton-Raphson\nConsidere o problma de encontrar uma raiz de \\(f(x)\\). Seja \\(x_0\\) um ponto próximo da raiz \\(x^\\star\\). A equação da reta tangente ao ponto \\(x_0\\) é dada por\n\\[f(x)=f(x_0)+(x-x_0)f'(x_0).\\] A Figure 1.3 ilustra a reta tangente (em vermelho) ao ponto \\(x_0\\) para uma certa função. Observe que a raiz da reta tangente, marcada como \\(x_1\\) no gráfico, está mais próxima de \\(x^\\star\\) do que o ponto \\(x_0\\).\n\n\n\n\n\n\n\n\nFigure 1.3: Obtenção de um novo intervalo para o Teorema do Valor Intermediário\n\n\n\n\n\nA raiz \\(x_1\\) é calculada do seguinte modo: \\[0=f(x_0)+(x_1-x_0)f'(x_0)\\Rightarrow x_1=x_0-\\frac{f(x_0)}{f'(x_0)}.\\]\nPodemos então encontrar a reta tangente ao ponto \\(x_1\\). Por sua vez, a raiz desta reta, denominada por \\(x_2\\), estará mais próxima de \\(x^\\star\\) do que \\(x_1\\). Após realizar o mesmo procedimento \\(n\\) vezes teremos\n\\[x_n=x_{n-1}-\\frac{f(x_{n-1})}{f'(x_{n-1})}\\]\nIntuitivamente, temos que \\(x_n\\) deve estar mais próximo de \\(x^\\star\\) na medida que \\(n\\rightarrow\\infty\\). Abaixo, segue o algoritmo do método de Newton-Raphson.\n\nAlgoritmo 2. (Método de Newton-Raphson)\nPara encontrar a raiz \\(x^\\star\\) de \\(f(x)\\) com um erro \\(\\varepsilon\\).\nPré-condição: \\(f(x)\\) é diferenciável com \\(f'(x)\\neq 0\\) para qualquer \\(x\\) na vizinhança de \\(x^\\star\\). É necessário um valor \\(x_0\\) próximo de \\(x^\\star\\)\nDefina \\(x_{atual}=x_0\\) e erro=\\(+\\infty\\)\n\nEnquanto o erro for maior que \\(\\varepsilon\\) repita os seguintes passos:\n\nCalcule \\(x_m=x_{atual}-\\frac{f(x_{atual})}{f'(x_{atual})}\\)\nCalcule erro\\(=|x_m-x_{atual}|\\) e faça \\(x_{atual}=x_m\\).\n\nRetorne \\(x^\\star=x_m\\).\n\n\nO método\n\nTeorema de Kantorovich (simplificado) Seja \\(x_0\\) um ponto inicial para o método Newton-Raphson. A convergência para a raiz de \\(f(x)\\) é garantida se:\n\n\\(f'(x_0)\\neq 0\\)\nExiste uma constante \\(L\\) que limita a segunda derivada, ou seja \\(|f''(x_0)|\\leq L\\) em algum intervalo contento \\(x_0\\).\nA seguinte condição é satisfeita: \\[h_0=\\frac{|f(x_0)|}{f'(x_0)^2}L\\leq \\frac{1}{2}.\\]\n\n\n\nMétodos de quase-Newton.\nUma das desvantagens do método de Newton-Raphson é a necessidade de obter uma expressão analítica para a derivada de \\(f(x)\\). Qualquer método que utilize a mesma estrutura do Newton-Raphson mas troca a forma analítica de \\(f'(x)\\) por uma aproximação pertence à classe de métodos de quase-Newton.\n\n.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-da-secante",
    "href": "solucoes_caso_real.html#método-da-secante",
    "title": "1  Solução de equações",
    "section": "1.3 Método da secante",
    "text": "1.3 Método da secante\nSeja \\(x^\\star\\) a raiz de \\(f(x)\\) e sejam \\(x_0\\) e \\(x_1\\) dois valores próximos de \\(x^\\star\\). A reta secante aos pontos \\((x_0,f(x_0))\\) e \\((x_1,f(x_1))\\) é dada por \\[f(x)=f(x_1)+ (x-x_1)\\frac{f(x_1)-f(x_0)}{x_1-x_0}.\\]\nAssim como no métod Newton-Raphson, a raiz da reta secante acima tende a estar mais próxima de \\(x^\\star\\) do que \\(x_0\\) e \\(x_1\\). A Figure 1.4 ilustra essa afirmação, onde o ponto \\(x_2\\) é a raiz da reta secante (em vermelho).\n\n\n\n\n\n\n\n\nFigure 1.4: Obtenção de um novo pontos utilizando a reta secante formada a partir de dois pontos iniciais.\n\n\n\n\n\nO ponto \\(x_2\\) é obtido do seguinte modo:\n\\[0=f(x_1)+(x_2-x_1)\\frac{f(x_1)-f(x_)}{x_1-x_0}\\Rightarrow x_2=x_1-f(x_1)\\frac{x_1-x_0}{f(x_1)-f(x_0)}.\\] De modo análogo, podemos construir a reta secante aos pontos \\((x_1,f(x_1))\\) e \\((x_2,f(x_2))\\) e encontrar sua raiz \\(x_3\\). Ao realizar esse procedimento diversas vezes, teremos que o ponto \\(x_n\\) será dado por\n\\[ x_n=x_{n-1}-f(x_{n-1})\\frac{x_{n-1}-x_{n-2}}{f(x_{n-1})-f(x_{n-2})}.\\] Podemos continuar a iteração até obter \\(|x_n-x_{n-1}|&lt;\\varepsilon\\).\n\nAlgoritmo 3. (Método da secante)\nPara encontrar a raiz \\(x^\\star\\) de \\(f(x)\\) com um erro \\(\\varepsilon\\).\nPré-condição: São necessários valores \\(x_0\\) e \\(x_1\\) próximos de \\(x^\\star\\)\nDefina erro=\\(+\\infty\\) e \\(n=1\\)\n\nEnquanto o erro for maior que \\(\\varepsilon\\) repita os seguintes passos:\n\n\\(n=n+1\\)\nCalcule \\(x_n=x_{n-1}-f(x_{n-1})\\frac{x_{n-1}-x_{n-2}}{f(x_{n-1})-f(x_{n-2})}\\)\nCalcule erro\\(=|x_n-x_{n-1}|\\).\n\nRetorne \\(x^\\star=x_n\\).\n\n\n\nExemplo - (W de Lambert) Considere o problema de encontrar a raiz de \\(f(x)= e^{-x}-x\\). O resultado, conhecido como constante de \\(W\\) de Lambert, é um número irracional e é aproximadamente 0,567143.\nVamos escolher os pontos iniciais \\(x_0=0\\) e \\(x_1=1\\), onde \\[\\begin{align}f(0)&=e^{-0}-0=1\\\\f(1)&=e^{-1}-1\\approx-0,6321.\\end{align}\\] A fórmula de iteração é dada por\n\\[x_n=x_{n-1}-(e^{-x_{n-1}}-1)\\frac{x_{n-1}-x_{n-2}}{e^{-x_{n-1}}-x_{n-1}-e^{-x_{n-2}}+x_{n-2}}.\\]\nVamos fixar o erro em \\(10^{-7}\\) para obter uma precisão de 6 casas decimais.\n\n\n\n\n\nIterações\nx\nErro\n\n\n\n\n1\n0.612700\n0.387300\n\n\n2\n0.563838\n0.048861\n\n\n3\n0.567170\n0.003332\n\n\n4\n0.567143\n0.000027\n\n\n5\n0.567143\n0.000000\n\n\n\n\n\n\nFica como exercício provar que o método da secante pertence à classe dos métodos de quase-Newton. Assim como o método de Newton-Raphson, esse método tem boas propriedades para funções duas vezes continuamente diferenciáveis, como mostra o teorema abaixo.\n\nTeorema da convergência do método da secante. Seja \\(f\\) uma função real, contínua e duas vezes diferenciável no intervalo \\([a,b]\\), onde \\(x^\\star\\in(a,b)\\) é a única raiz nesse intervalo e \\(f'(x^\\star)\\neq 0\\)\nSe as soluções iniciais \\(x_0\\) e \\(x_1\\) forem escolhidas suficientemente próximas de \\(x^\\star\\) então a sequência \\(x_2,\\ldots,x_n\\) produzida pelo método da secante irá convergir para \\(x^\\star\\).\n\n\nAtenção. O teorema acima mostra que não basta escolher dois valores dentro do intervalo \\([a,b]\\) que contém \\(x^\\star\\). Ou seja, diferente do método da bisseção, essa escolha não é garantia de convergência. Isso ocorre porque, dependendo da função, a escolha dos pontos iniciais pode levar a geração de candidatos para fora do intervalo \\([a,b]\\) fazendo com que a convergência demore ou mesmo falhe.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-da-interpolação-quadrática-inversa-método-de-muller",
    "href": "solucoes_caso_real.html#método-da-interpolação-quadrática-inversa-método-de-muller",
    "title": "1  Solução de equações",
    "section": "1.4 Método da interpolação quadrática inversa (Método de Muller)",
    "text": "1.4 Método da interpolação quadrática inversa (Método de Muller)\nO método da interpolação quadrática inversa segue a mesma ideia do método da secante. Considere três pontos, \\(x_1,x_2,x_3\\) próximos de \\(x^\\star\\). Na \\(i\\)-ésima iteração, o próximo candidato à solução é a raiz da parábola que passa pelos pontos \\(x_{i-1},x_{i-2},x_{i-3}\\), dada por\n\\[x_{i} = x_{i-1} - \\frac{2f(x_{i-1})}{\\omega \\pm \\sqrt{\\omega^2 - 4f(x_{i-1})\\delta_{i-1}}}\\] onde \\[\\begin{align}\n\\delta_{i-1} &= \\frac{f(x_{i-1})-f(x_{i-2})}{x_{i-1}-x_{i-2}} + \\frac{f(x_{i-1})-f(x_{i-3})}{x_{i-1}-x_{i-3}} - \\frac{f(x_{i-2})-f(x_{i-3})}{x_{i-2}-x_{i-3}}\\\\\n\\omega &= \\frac{f(x_{i-1})-f(x_{i-2})}{x_{i-1}-x_{i-2}} + \\delta_{i-1}(x_{i-1}-x_{i-2})\n\\end{align}\\]\nEmbora esse método possa convergir rapidamente para a raiz, os pontos iniciais devem estar próximos da raiz. Além disso, o algoritmo pode falhar se em algum momento os valores de \\(f(x_i),f(x_{i-1})\\) ou \\(f(x_{i-2})\\) concidirem. Portanto, é recomendado que esse método seja utilizado em conjunto com outro, conforme discutido na seção Métodos Híbridos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#método-da-falsa-posição-regula-falsi",
    "href": "solucoes_caso_real.html#método-da-falsa-posição-regula-falsi",
    "title": "1  Solução de equações",
    "section": "1.5 Método da falsa posição (regula falsi)",
    "text": "1.5 Método da falsa posição (regula falsi)\nO método da falsa posição é uma combinação do método da secante com o método da bisseção. Assim como neste último, é necessário começar com um intervalo \\([x_a,x_b]\\) no qual \\(f(x_a)\\) e \\(f(x_b)\\) têm sinais opostos.\nPrimeiro, calcula-se a raiz da reta secante que passa pelos pontos \\((x_a,f(x_a))\\) e \\((x_b,f(x_b))\\), dada por\n\\[\nx_c=x_b-f(x_b)\\frac{x_b-x_a}{f(x_b)-f(x_a)}.\n\\] Em seguida, verifica-se qual dos intervalos \\([x_a,x_c]\\) ou \\([x_c,x_b]\\) contém a raiz. Então, repete-se a busca.\nAssim como ocorre no método da bisseção, o método da falsa posição cria uma sequência de intervalos encaixados que contém a raiz, convergindo portanto para a verdadeira solução. Espera-se que a escolha de \\(x_c\\) esteja mais próxima da raiz do que o ponto médio do intervalo.\n\nAlgoritmo 4 (Método da Falsa Posição).\nPara encontrar uma raiz de \\(f(x)\\) no intervalo \\([a,b]\\) com uma tolerância de \\(\\varepsilon\\):\nPré-condição: \\(f(a)f(b)&lt;0.\\)\n\nDefina a tolerância \\(\\varepsilon&gt;0.\\) e faça \\(i=1\\)\nEnquanto \\(∣b−a|\\geq\\varepsilon\\) repita os seguintes passos:\n\nCalcule \\(x_i=b - f(b)\\frac{b-a}{f(b)-f(a)}\\)\nSe \\(f(x_i)=0\\), pare e retorne \\(x^\\star=x_i\\).\nAtualização do intervalo\n\n\nSe \\(f(x_i)f(a)&lt;0\\), a raiz está o intervalo \\([a,x_i]\\). Faça \\(b=x_i\\)\nSenão, a raiz está no intervalo \\([x_i,b]\\). Faça \\(a=x_i\\)\nFaça \\(i=i+1\\)\n\n\n\n\nExemplo Seja \\[f(x)=x^2−x-1\\] Anteriormente, encontramos a raiz de \\(f\\), iniciando com o intervalo [1,2] e fixando erro de \\(0,00003\\), que foi obtido em 15 iterações. A tabela abaixo apresenta os intervalos e os pontos médios (soluções) para o método da falsa posição até a obtenção do mesmo erro fixado no método da bisseção.\n\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\n\n\n\nIteração\nSolução\nErro\n[a\nb]\n\n\n\n\n1\n1.800000\n1.000000\n1\n1.800000\n\n\n2\n1.712871\n0.087129\n1\n1.712871\n\n\n3\n1.669659\n0.043212\n1\n1.669659\n\n\n4\n1.646784\n0.022875\n1\n1.646784\n\n\n5\n1.634245\n0.012539\n1\n1.634245\n\n\n6\n1.627238\n0.007007\n1\n1.627238\n\n\n7\n1.623280\n0.003958\n1\n1.623280\n\n\n8\n1.621031\n0.002249\n1\n1.621031\n\n\n9\n1.619748\n0.001283\n1\n1.619748\n\n\n10\n1.619015\n0.000733\n1\n1.619015\n\n\n11\n1.618596\n0.000419\n1\n1.618596\n\n\n12\n1.618356\n0.000240\n1\n1.618356\n\n\n13\n1.618218\n0.000137\n1\n1.618218\n\n\n14\n1.618140\n0.000079\n1\n1.618140\n\n\n15\n1.618094\n0.000045\n1\n1.618094",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#métodos-híbridos",
    "href": "solucoes_caso_real.html#métodos-híbridos",
    "title": "1  Solução de equações",
    "section": "1.6 Métodos híbridos",
    "text": "1.6 Métodos híbridos\nOs métodos discutidos até o momento possuem vantagens e desvantagens, conforme sumarizado na tabela abaixo.\n\n\n\n\n\n\n\n\n\nMétodo\nOrdem de Convergência\nVantagem\nDesvantagem\n\n\n\n\nBisseção\nLinear\nGarante a convergência. Não exige o cálculo da derivada.\nA convergência é lenta.\n\n\nFalsa Posição\nLinear\nGeralmente converge mais rápido que a bisseção. Não exige o cálculo da derivada.\nPode ter convergência lenta se a função for muito côncava ou convexa.\n\n\nSecante\nSuperlinear\nConvergência rápida, sem precisar calcular a derivada.\nPode divergir se a aproximação inicial for ruim.\n\n\nInterpolação Quadrática Inversa\nSuperlinear\nGeralmente mais rápido que o método da secante. Pode encontrar raízes complexas.\nExige três pontos iniciais. Pode ser instável e falhar se os pontos forem colineares.\n\n\nNewton-Raphson\nQuadrática\nConvergência extremamente rápida (se as condições forem atendidas).\nExige o cálculo da derivada. Pode divergir se a aproximação inicial for ruim.\n\n\n\nOs métodos híbridos são algoritmos que decidem em cada iteração qual método utilizar. Os objetivos dessas decisões são acelerar e garantir a convergência. Vamos apresentar a ideia por trás de dois métodos: Dekker e Brent.\nO método de Dekker (1969) combina o método da bisseção com o da secante. sua ideia básica é utilizar o método da secante sempre que possível, criando verificações lógicas que evitem que o candidato à solução se afaste da raiz. Ele começa com um intervalo \\([a,b]\\) que contém a raiz \\(x^\\star\\) (ou seja, \\(f(a)\\) e \\(f(b)\\) tem sinais opostos). Na sua \\(i\\)-ésima iteração:\n\nGeração dos candidatos. Dois candidatos são construídos, utilizando o método da bisseção e o da secante:\n\n\\[\\begin{align}x_m&=\\frac{a_{i-1}+b_{i-1}}{2}\\\\x_s&=b_{i-1}-f(b_{i-1})\\frac{b_{i-1}-a_{i-1}}{f(b_{i-1})-f(a_{i-1})} \\end{align}\\] 2. Freio da secante. Se \\(x_s\\in[x_m,b_{i-1}]\\), então o ponto da secante não se afastou da solução e fazemos \\(b_{i}=x_s\\). Senão, escolhemos \\(b_i=x_m\\) por segurança. O objetivo desse passo é evitar utilizar um ponto \\(x_s\\) afastado da solução.\n\nDefinindo o novo intervalo. Se \\(f(a_{i-1})\\) e \\(f(b_i)\\) tem sinais oposto, então \\(a_i=a_{i-1}\\). Em caso contrário, \\(a_i=b_{i-1}\\)\nPermuta Se \\(|f(a_i)|&lt;|f(b_i)|\\), então é provável que \\(a_i\\) seja um palpite melhor que \\(b_i\\). Então os valores \\(a_i\\) e \\(b_i\\) são permutados. Isso garante que a solução será dada pel convergência da sequência \\(b_1,b_2,\\ldots\\)\n\nO método de Brent (1973) é uma modificação do método de Dekker, començando também com um intervalo \\([a,b]\\) que contém a raiz. A principal diferença é que Brent adiciona mais verificações para garantir que a interpolação não apenas se aproxime da raiz, mas o faça de forma eficiente. Ele utiliza a interpolação quadrática inversa (que é mais rápida que a secante) e recorre à bisseção quando a interpolação não faz um progresso significativo.\nEm geral, o método itera nos seguintes passos:\n\nGeração do candidato: O algoritmo tenta encontrar um novo ponto, \\(x_s\\), usando a interpolação quadrática inversa. Se essa interpolação não for possível, ele recorre ao método da secante.\nTestes de segurança: Antes de aceitar \\(x_s\\), ele é submetido a uma série de testes que o comparam com a garantia de progresso da bisseção. Se \\(x_s\\) não for significativamente melhor do que um passo de bisseção, o algoritmo descarta \\(x_s\\) e, em vez disso, usa o ponto médio \\(x_m\\) como candidato.\nAtualização do intervalo. O novo intervalo é atualizado com base no candidato aceito (\\(x_s\\) ou \\(x_m\\)), garantindo que a raiz continue delimitada e que o ponto com o menor valor de \\(f(x)\\) seja sempre a melhor estimativa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#funções-para-encontrar-raízes-no-r",
    "href": "solucoes_caso_real.html#funções-para-encontrar-raízes-no-r",
    "title": "1  Solução de equações",
    "section": "1.7 Funções para encontrar raízes no R",
    "text": "1.7 Funções para encontrar raízes no R\nO R oferece várias funções para encontrar raízes de equações não-lineares, tanto no pacote base (stats) quanto em pacotes externos.\n\n1.7.1 Pacote stats\nA função uniroot é a implementação do método de Brent. Seus principais argumentos são:\n\nf: a função da qual se deseja obter a raiz\ninterval: vetor contendo os pontos extremos do intervalo de busca\ntol: tolerância desejada (o valor padrão é 10^{-10})\nmaxiter: número máximo de tentativas (o valor padrão é 1000)\n\n\n\n1.7.2 Pacote pracma\nO pacote pracma traz diversas rotinas de cálculo numérico, incluindo os métodos clássicos de busca de raízes.\nOs métodos da bisseção, secante e falsa posição estão implementados nas funções bisect, secant e regulaFalsi, respectivamente. Seus argumentos são\n\nf: a função da qual se deseja obter a raiz\na: limite inferior do intervalo para a bisseção e falsa posição; um ponto perto da raiz para a secante\nb: limite superior do intervalo para a bisseção e falsa posição; um ponto perto da raiz para a secante\ntol: tolerância desejada (o valor padrão é \\(10^{-10}\\))\nmaxiter: número máximo de tentativas (o valor padrão é 100)\n\nO método da interpolação quadrática inversa (Muller) está implementado na função muller, cujos argumentos são\n\nf: a função da qual se deseja obter a raiz\np0,p1,p2: três pontos iniciais, próximos da raiz\ntol: tolerância desejada (o valor padrão é .0001)\nmaxiter: número máximo de tentativas (o valor padrão é \\(10^{-10}\\))\n\nO método de Brent está implementado na função brent e possui os argumentos\n\nf: a função da qual se deseja obter a raiz\na,b: intervalo que contém a raiz (as imagens nas extremidades do intervalo devem ter sinais opostos)\ntol: tolerância desejada (o valor padrão é \\(10^{-12}\\))\nmaxiter: número máximo de tentativas (o valor padrão é 500)\n\nA função newtonRaphson (ou de modo equivalente, newton) encontra a raiz da função utilizando o método Newton-Raphson. Os argumentos são\n\nfun: a função da qual se deseja obter a raiz\nx0: valor inicial, próximo da raiz\ndfun: a função da derivada de \\(f\\). Se dfun=NULL, o método da secante será utilizado\ntol: tolerância desejada (o valor padrão é \\(10^{-8}\\))\nmaxiter: número máximo de tentativas (o valor padrão é 500)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "solucoes_caso_real.html#exercícios",
    "href": "solucoes_caso_real.html#exercícios",
    "title": "1  Solução de equações",
    "section": "1.8 Exercícios",
    "text": "1.8 Exercícios\n\nExercício Seja \\(f:[a,b]\\rightarrow\\mathbb{R}\\) uma função contínua tal que \\(f(a)f(b)&lt;0\\). O método da bisseção gera uma sequência de intervalos encaixados \\([a_k,b_k]\\) onde a raiz \\(x^\\star\\in[a_k,b_k]\\) para todo \\(k\\in\\mathbb{N}\\).\nDiscuta a unicidade da raiz encontrada por esse método. Sob quais condições o Método da Bisseção garante que a raiz encontrada é única no intervalo inicial \\([a,b]?\\)\n\n\nExercício Implemente o método da bisseção para encontrar a raiz da função \\[f(x)=\\log(x)+x^2-4.\\]\nUtilize o intervalo inicial \\([1,2]\\). (Verifique previamente se \\(f(1)\\) e \\(f(2)\\) possuem sinais opostos). O critério de parada deve ser a obtenção de um erro absoluto máximo inferior a \\(10^{−6}\\). Ao final, imprima:\n\nA raiz aproximada encontrada.\nO número total de iterações realizadas.\nUma tabela com os valores \\(x_n\\), \\(f(x_n)\\) e \\(\\varepsilon_n\\).\n\n\n\nExercício. Seja \\(X\\) uma variável aleatória contínua, cuja função distribuição é dada por \\(F(x)\\). O quantil \\(100p\\%\\) é o valor \\(x_p\\) que satisfaz \\[F(x_p)=p.\\] Explique porque existe um único \\(x_p\\) que satifaz a equação acima e utilize esse fato para descrever como obter \\(x_p\\) a partir do método da bisseção.\n\n\nExercício. Seja \\(X\\) uma variável aleatória com a seguinte função distribuição \\[F(x)=1-\\frac{1}{5}(x^2+4x+5) e^{-x},\\] com \\(x&gt;0\\). Encontre a mediana de \\(X\\) utilizando o método da bisseção.\n\n\nExercício Considere o problema de resolver \\(x=\\sqrt{3}\\). Prove que o método de Newton-Raphson vai convergir para a verdadeira solução se o valor inicial \\(x_0\\) satisfaz\n\\[\\frac{|x_0^2-3|}{2x_0^2}\\leq \\frac{1}{2}.\\]\n\n\nExercício. Neste exercício você vai demonstrar o Teorema da Convergência do método da secante. Considere que as condições do teorema estão satisfeitas. Seja \\(x^\\star\\) a raiz do problema. É fato que, para qualquer ponto \\(x\\) próximo de \\(x^\\star\\), a função \\(f(x)\\) pode ser aproximada por\n\\[f(x)\\approx f(x^\\star) +(x-x^\\star)f'(x^\\star)+\\frac{1}{2}(x-x^\\star)f''(x^\\star)\\] onde a aproximação é obtida por expansão em séries de Taylor. Considere então que \\(x_n\\) e \\(x_{n-1}\\) estão próximas de \\(x^\\star\\). Seja \\(\\xi_n=x_n-x^\\star\\).\n\nProve que \\[\\begin{align}f(x_n)\\approx  \\xi_nf'(x^\\star)+\\frac{1}{2}\\xi_n^2 f''(x^\\star)\\end{align}\\]\nMostre que\n\n\\[\\begin{align}f(x_n)-f(x_{n-1})\\approx (\\xi_n-\\xi_{n-1})\\left[f'(x^\\star)+\\frac{1}{2}f''(x^\\star)(\\xi_n+\\xi_{n-1})\\right]\\end{align}\\]\n\nMostre que a fórmula de iteração do método da secante pode ser reescrita como\n\n\\[\\xi_{n+1}=\\xi_n-f(x_n)\\frac{\\xi_n-\\xi_{n-1}}{f(x_n)-f(n-1)}\\]\n\nA partir dos passos 2 e 3, prove que\n\n\\[\\xi_{n+1}=\\xi_n-\\frac{f(x_n)}{f'(x^\\star)+\\frac{1}{2}f''(x^\\star)(\\xi_n+\\xi_{n-1})}\\]\n\nUtilize a aproximação para \\(f(x_n)\\) para mostrar que\n\n\\[\\begin{align}\\xi_{n+1}&\\approx\\xi_n-\\frac{\\xi_nf'(x^\\star)+\\frac{1}{2}\\xi_n^2 f''(x^\\star)}{f'(x^\\star)+\\frac{1}{2}f''(x^\\star)(\\xi_n+\\xi_{n-1})}\\\\&=\\xi_n-\\frac{\\xi_n+\\frac{1}{2}\\xi_n^2 \\frac{f''(x^\\star)}{f'(x^\\star)} }{1+\\frac{1}{2}\\frac{f''(x^\\star)}{f'(x^\\star)}(\\xi_n+\\xi_{n-1})}\\end{align}\\]\n\nSe \\(u\\) é próximo de zero, é verdade que \\[\\frac{1}{1+u}\\approx 1-u\\] Como \\((\\xi_n+\\xi_{n-1})\\) é próximo de zero faça \\[u=\\frac{1}{2}\\frac{f''(x^\\star)}{f'(x^\\star)}(\\xi_n+\\xi_{n-1})\\] e mostre que \\[\\begin{align}\\xi_{n+1}&\\approx\\xi_n-\\frac{\\xi_nf'(x^\\star)+\\frac{1}{2}\\xi_n^2 f''(x^\\star)}{f'(x^\\star)+\\frac{1}{2}f''(x^\\star)(\\xi_n+\\xi_{n-1})}\\\\&=\\xi_n-\\left(\\xi_n+\\frac{1}{2}\\xi_n^2 \\frac{f''(x^\\star)}{f'(x^\\star)}\\right)\\left(1-\\frac{1}{2}\\frac{f''(x^\\star)}{f'(x^\\star)}(\\xi_n+\\xi_{n-1})\\right)\\end{align}\\]\nComo \\(\\varepsilon_n\\) é pequeno, o valor de \\(\\varepsilon_n^2\\approx 0\\). Utilize essa informação para mostrar que\n\n\\[\\xi_{n+1}\\approx \\frac{1}{2}\\frac{f''(x^\\star)}{f'(x^\\star)}\\xi_n\\xi_{n-1}\\] Isto implica que \\(|\\xi_{n+1}|&lt;|\\xi_n|\\)\n\nConclua que, se \\(x_0,x_1\\) estão próximos o suficiente da raiz, então o método da secante é convergente.\n\n\n\nExercício. Dizemos que \\(X\\) tem distribuição Lindley(\\(\\theta\\)) se sua função densidade é dada por \\[f(x|\\theta)=\\frac{\\theta^2}{1+\\theta}(1+x)e^{-\\theta x},\\] onde \\(x,\\theta&gt;0\\). Pode-se provar que\n\\[F(x|\\theta)=1-\\frac{1+\\theta(1+x)}{1+\\theta}e^{-\\theta x}\\] Para \\(\\theta=1\\) e considerando um erro de \\(10^{-4}\\), encontre a mediana desse modelo considerando os métodos\n\nBisseção\nSecante\nFalsa Posição\nMuller\nBrent\nNewton-Raphson\n\nVocê pode utilizar as funções ja implementadas no R. Para cada método, guarde o número de iterações até a convergência. Qual método convergiu mais rápido?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Solução de equações</span>"
    ]
  },
  {
    "objectID": "otimizacao.html",
    "href": "otimizacao.html",
    "title": "2  Métodos de otimização",
    "section": "",
    "text": "2.1 Introdução\nO problema de otimização é bastante comum na estatística, especialmente para obtenção de estimativas via método da máxima verossimilhança.\nNeste capítulo veremos os métodos clássicos de otimização numérica. Especificamente, serão estudados os métodos:\nSeja \\(f(x)\\) uma função contínua. O problema de otimização tem por objetivo determinar o valor de \\(x^\\star\\) qual que \\[f(x)\\geq f(x^\\star)\\] para todo \\(x\\) no domínio da função. No contexto de otimização, \\(f(x)\\) é denominada função objetivo, enquanto que o ponto \\(x^\\star\\) é denominado ponto de mínimo global.\nEm certas situações, estamos interessados apenas em determinar o valor \\(x^\\star\\in D\\) tal que \\[f(x)\\geq f(x^\\star)\\] para todo \\(x\\in D\\). Nesse caso, \\(x^\\star\\) é denominado ponto de mínimo local.\nDentro da otimização, o problema de encontrar um mínimo local é escrito como\n\\[\\begin{align}&\\min f(x)\\\\\n&\\hbox{s.t.}\\\\\n&x\\in D\n\\end{align}\\]\nEm geral, os métodos computacionais para otimização são implementados apenas para encontrar mínimos locais. Isso ocorre porque\n\\[\\max f(x)=\\min -f(x).\\] Observe que pontos ótimos, tanto mínimos quanto máximos, satisfazem \\(f'(x^\\star)=0\\). A Figure 2.1 apresenta a reta tangente a um ponto de máxmimo, no qual o valor da derivada (inclinação da reta) é zero.\nFigure 2.1: Em vermelho, a reta tangente ao ponto de máximo\nPortanto, o problema de encontrar um mínimo local em um intervalo \\(D\\) se reduz ao problema de encontrar \\(x^\\star\\) tal que\n\\[f'(x^\\star)=0,\\] Isto implica que podemos utilizar dos os todos os métodos vistos no capítulo anterior para encontrar mínimos locais.\nOs métodos vistos até o momento não são suficientes para resolver os problemas gerais de otimização, especialmente pela exigência da forma analítica da derivada da função.\nAlém disso, os problemas de otmização que lidamos na estatística e nas principais áreas da matemática aplicada envolvem funções do tipo \\(f:\\mathbb{R}^n\\rightarrow \\mathbb{R}\\), com \\(n&gt;1\\). Deste modo, métodos de confinamento, como o da bisseção, são difícies de generalizar. O exemplo abaixo ilustra uma aplicação comum na estatística.\nPara o caso \\(n\\)-dimensional, o teste da derivada se mantém, conforme o teorema a seguir.\nPara o próximo resultado, relembre que a matriz \\(B\\) é posivita definida se \\(\\boldsymbol{x}^TB\\boldsymbol{x}&gt;0\\) para todo \\(\\boldsymbol{x}\\neq \\textbf{0}\\) e positiva semidefinida se \\(\\boldsymbol{x}^TB\\boldsymbol{x}\\geq 0\\) para todo \\(\\boldsymbol{x}.\\)\nO seguinte teorema é fundamental para a construção dos principais algoritmos de otimização.\nNessas notas de aula, vamos considerar que \\(f\\) é uma função suave, o que significa que existe a segunda derivada e ela é contínua.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "otimizacao.html#introdução",
    "href": "otimizacao.html#introdução",
    "title": "2  Métodos de otimização",
    "section": "",
    "text": "Example 2.1 Considere o problema de encontrar o máximo da função \\[f(x)=x^2 e^{-x},\\] com \\(x&gt;0\\), cuja solução analítica é \\(x^\\star=2\\). A derivada de \\(f\\) é dada por\n\\[f'(x)=2xe^{-x}-x^2e^{-x}=(2x-x^2)e^{-x}\\] O gráfico da derivada da função, entre os pontos 1 e 3 é dado abaixo. Observe que as imagens nesses pontos têm sinais opostos.\n\ndfunc &lt;- function(x) (2*x - x^2) * exp(-x)\ncurve( dfunc(x) , 1, 3)\nabline(h=0, lty =2)\n\n\n\n\n\n\n\n\nUtilizando a função uniroot, obtemos o ponto desejado.\n\nuniroot( dfunc, c(1,3) )\n\n$root\n[1] 2\n\n$f.root\n[1] -7.437827e-08\n\n$iter\n[1] 6\n\n$init.it\n[1] NA\n\n$estim.prec\n[1] 6.709322e-05\n\n\n\n\nExercício. Considere a função\n\\[f(x) = \\sin(x) +\\cos(x\\sqrt{2}))+\\sin(\\pi x).\\] 1. Implemente a função e faça o seu gráfico para o intervalo \\((-10,10)\\).\n\nDetermine o mínimo global, sabendo que este está no intervalo \\((-10,10)\\)\n\n\n\nImportante A função logaritmo é monótona crescente. Portanto, é verdade que\n\\[f(x)\\geq f(x^\\star)\\Leftrightarrow \\log f(x) \\geq \\log f(x^\\star).\\] Deste modo, encontrar o mínimo de \\(\\log f\\) é equivalente a encontrar \\(x^\\star\\).\n\n\n\n\nExample 2.2 Os dados abaixo apresentam as máximas anuais do Rio Negro entre 2004 e 2014.\n\nmaximas &lt;- c(25.42, 28.76, 29.05, 27.16, 28.54, 28.96, 27.58, 29.30, 28.62, 28.21, 28.91, 28.27, 27.13, 28.10, 28.84, 28.18, 28.62, 29.77, 27.96, 28.62, 29.97, 29.33, 29.50)\n\nAssim como o Teorema Central do Limite dá uma distribuição aproximada para a média amostral, o Teorema de Fisher-Tippet diz que a distribuição dos máximos tende a uma distribuição denominada Valores Extremos. Nesse caso em particular, a distribuição será a Weibull, cuja densidade é dada por\n\\[f(x|\\alpha,\\beta) =\n\\begin{cases}\n\\frac{\\alpha}{\\beta}\\left(\\frac{x}{\\beta}\\right)^{\\alpha-1}e^{-(x/\\beta)^\\alpha} & \\text{para } x \\ge 0 \\\\\n0 & \\text{para } x &lt; 0\n\\end{cases}\\]\ncom \\(\\alpha,\\beta&gt;0\\). O valor de \\(\\alpha,\\beta\\) que maximiza a função \\[L(\\alpha,\\beta)=\\prod_{i=1}^n f(x_i|\\alpha,\\beta)\\] é denominado estimativa de máxima verossimilhança. Abaixo, mostramos a implementação de \\(-L(\\alpha,\\beta)\\) e algumas representações gráficas da superfície a ser otimizada\n\nL &lt;- function(a,b) -prod(dweibull(maximas, shape = a, scale = b, log = F))\na &lt;- seq(30,46, length.out = 50)\nb &lt;- seq(28,30, length.out = 50)\nz &lt;- outer(a, b, Vectorize(L))\n\npersp(a,b,z, theta = 45, phi = 45)\n\n\n\n\n\n\n\nimage(a,b,z, col=terrain.colors(20))\ncontour(a,b,z, add = T)\n\n\n\n\n\n\n\n\nÉ importante ressaltar que, por motivos de estabilidade numérica, \\(\\log L(\\alpha,\\beta)\\) deve ser implementada no lugar de \\(L(\\alpha,\\beta)\\)\n\n\n\nTheorem 2.1 (Condição necessária de primeira ordem) Se \\(\\boldsymbol{x}^\\star\\) é um ponto de mínimo, então \\[\\nabla f(\\boldsymbol{x}^\\star)=\\left(\\frac{\\partial f}{\\partial x_1},\\ldots,\\frac{\\partial f}{\\partial x_n}\\right)^T=\\textbf{0}_n.\\]\n\n\n\nTheorem 2.2 (Condição necessária de segunda ordem) Se \\(\\boldsymbol{x}^\\star\\) é um ponto de mínimo local de \\(f\\) e se existe \\(\\nabla f\\) contínua na vizinhança de \\(\\boldsymbol{x}^\\star\\), então \\(\\nabla f(\\boldsymbol{x}^\\star)=\\textbf{0}\\) e \\(\\mathcal{H}f(\\boldsymbol{x}^\\star)\\) é positiva semidefinida, onde \\(\\mathcal{H}f\\), denominada Hessiana, é a matriz das segundas derivadas parciais da função, definida como:\n\\[ \\mathcal{H} f(\\mathbf{x}) = \\left(\\begin{array}{ccc}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{array}\\right) \\]\n\n\n\nTheorem 2.3 (Teorema de Taylor) Suponha que \\(f:\\mathbb{R}^n\\rightarrow\\mathbb{R}\\) é contínua e diferenciável. Então, para qualquer \\(\\boldsymbol{x}\\) na vizinhança de \\(\\boldsymbol{x}_0\\), valem as aproximações:\n\nPrimeira ordem:\n\n\\[f(\\boldsymbol{x})\\approx f(\\boldsymbol{x}_0)+(\\boldsymbol{x}-\\boldsymbol{x}_0)^T\\nabla f(\\boldsymbol{x}_0)\\] 2. Segunda ordem:\n\\[f(\\boldsymbol{x})\\approx f(\\boldsymbol{x}_0)+(\\boldsymbol{x}-\\boldsymbol{x}_0)^T\\nabla f(\\boldsymbol{x}_0)+\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{x}_0)^T\\mathcal{H}(\\boldsymbol{x}_0)(\\boldsymbol{x}-\\boldsymbol{x}_0)\\]\n\n\nCorolário. Suponha que \\(f:\\mathbb{R}^n\\rightarrow\\mathbb{R}\\) é contínua e diferenciável. Então, para qualquer \\(\\boldsymbol{x}\\) na vizinhança de \\(\\boldsymbol{x}_0\\),\n\\[\\nabla f(\\boldsymbol{x})\\approx \\nabla f(\\boldsymbol{x}_0)+\\mathcal{H}f(\\boldsymbol{x}_0)(\\boldsymbol{x}-\\boldsymbol{x}_0).\\]\n\n\nTheorem 2.4 (Condição suficiente de segunda ordem) Suponha que \\(\\mathcal{H}f(\\boldsymbol{x})\\) é contínua na vizinhança de \\(\\boldsymbol{x}^\\star\\), \\(\\nabla f(\\boldsymbol{x}^\\star)=\\textbf{0}\\) e \\(\\mathcal{H}f(\\boldsymbol{x}^\\star)\\) é positiva definida. Então \\(\\boldsymbol{x}^\\star\\) é um minimizador local.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "otimizacao.html#método-newton-raphson",
    "href": "otimizacao.html#método-newton-raphson",
    "title": "2  Métodos de otimização",
    "section": "2.2 Método Newton-Raphson",
    "text": "2.2 Método Newton-Raphson\nEsse método consiste na aplicação direra do método dNewton-Raphson para encontrar raizes. Considere o problema de encontrar o zero da função \\(f'(x)\\). Então, a equação de iteração do método é\n\\[x_n=x_{n-1}-\\frac{f'(x_{n-1})}{f''(x_{n-1})}.\\] A vantagem deste método é que ele pode ser generalizado para \\(f:\\mathbb{R}^n\\rightarrow \\mathbb{R}\\). Para um ponto \\(\\boldsymbol{x}\\) próximo de \\(\\boldsymbol{x}_n\\), pelo corolário do Teorema de Taylor temos que\n\\[\\nabla f(\\boldsymbol{x})\\approx \\nabla f(\\boldsymbol{x}_{n}) +\\mathcal{H}f(\\boldsymbol{x}_{n})(\\boldsymbol{x}-\\boldsymbol{x}_{n})\\] Vamos encontrar a raiz a aproximação acima: \\[\\textbf{0}_n= \\nabla f(\\boldsymbol{x}_{n}) +\\mathcal{H}f(\\boldsymbol{x}_{n})(\\boldsymbol{x}-\\boldsymbol{x}_{n})\\Rightarrow \\boldsymbol{x}=\\boldsymbol{x}_n-[\\mathcal{H}f(\\boldsymbol{x}_n)]^{-1}\\nabla f(\\boldsymbol{x}_{n})\\] logo, a equação de iteração é dada por em torno de \\(x_0\\) 'e dada por\n\\[\\boldsymbol{x}_{n+1}=\\boldsymbol{x}_n-[\\mathcal{H}f(\\boldsymbol{x}_n)]^{-1}\\nabla f(\\boldsymbol{x}_{n})\\]\ne o critério de parada é \\(||\\boldsymbol{x}_n-\\boldsymbol{x}_{n-1}||\\leq \\varepsilon\\).\nVamos resscrever a iteração do método Newton-Raphson do seguinte modo:\n\\[\\boldsymbol{x}_{n}=\\boldsymbol{x}_{n-1}+\\underbrace{[\\mathcal{H}f(\\boldsymbol{x}_{n-1})]^{-1}}_{\\alpha_{n-1}}\\underbrace{(-\\nabla f(\\boldsymbol{x}_{n-1}))}_{p_{n-1}}=\\boldsymbol{x}_{n-1}+\\alpha_{n-1}p_{n-1}\\] Na equação acima, o termo \\(p_{n-1}\\) indica a direção de maior inclinação, enquato \\(\\alpha_{n-1}\\) indica o tamanho do salto (quanto mais plana for a função no ponto, maior é o valor de \\([\\mathcal{H}f]^{-1}\\) e maior será o salto). Essa estrutura de direção e salto é utilizada em outros métodos de otimização.\nComo a direção e o tamanho do salto são computados diretamente da função objetivo, temos que o metódo Newton-Rapshon é rápido quando o ponto inicial é escolhido próximo do ótimo e o custo de computar o gradiente e o inverso da Hessiana é baixo.\n\nAlgoritmo - Newton-Raphson\n\nInicialização. Comece com \\(\\boldsymbol{x}_0\\) próximo de um mínimo local. Fixe um erro \\(\\varepsilon\\) e faça ERRO\\(=+\\infty\\) e comece com \\(i=1\\).\nIteração. Enquanto ERRO\\(&gt;\\varepsilon\\).\n\nCalcule \\[\\boldsymbol{x}_i=\\boldsymbol{x}_{i-1}-[\\mathcal{H}f(\\boldsymbol{x}_{i-1})]^{-1}\\nabla f(\\boldsymbol{x}_{i-1})\\] e faça \\(i=i+1\\).\n\nb.Calcule ERRO\\(=|\\boldsymbol{x}_i-\\boldsymbol{x}_{i-1}|=\\sqrt{\\sum_{j=1}^n (x_{i,j}-x_{i-1,j})^2}\\)\nFinalizando. Retorne \\(x_i\\)\n\n\n\nExercício 3\nConsidere a função \\[f(x,y)=x^4+2y^4-4xy\\] Começando no ponto \\((2,1)\\), implemente o método Newton-Raphon para encontrar um mínimo local para esta função.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "otimizacao.html#método-da-descida-do-gradiente",
    "href": "otimizacao.html#método-da-descida-do-gradiente",
    "title": "2  Métodos de otimização",
    "section": "2.3 Método da Descida do Gradiente",
    "text": "2.3 Método da Descida do Gradiente\nUm dos principais problemas do método Newton-Raphson é a inversão da matriz Hessiana a cada iteração.\nIniciando em um ponto \\(\\boldsymbol{x}_0\\) próximo da raiz, a \\(n\\)-ésima iteração do método da Descida do Gradiente é dado por\n\\[\\boldsymbol{x}_{n}=\\boldsymbol{x}_{n-1}-\\alpha_n\\nabla f(\\boldsymbol{x}_{n-1})\\] onde \\(\\alpha_n\\) é denominada taxa de aprendizado, sendo geralmente uma constante. Observe que esse método substitui a inversa da matriz Hessiana por um escalar. Essa simplificação abre mão da precisão do método de Newton em favor da eficiência computacional.\n\nAlgoritmo - Descida do Gradiente\n\nInicialização. Comece com \\(\\boldsymbol{x}_0\\) próximo de um mínimo local. Fixe um erro \\(\\varepsilon\\), faça ERRO\\(=+\\infty\\) e comece com \\(i=1\\). Escolha um valor para \\(\\alpha\\).\nIteração. Enquanto ERRO\\(&gt;\\varepsilon\\).\n\nCalcule \\[\\boldsymbol{x}_i=\\boldsymbol{x}_{i-1}-\\alpha\\nabla f(\\boldsymbol{x}_{i-1})\\] e faça \\(i=i+1\\).\n\nb.Calcule ERRO\\(=|\\boldsymbol{x}_i-\\bolsymbol{x}_{i-1}|=\\sqrt{\\sum_{j=1}^n (x_{i,j}-x_{i-1,j})^2}\\)\nFinalizando. Retorne \\(x_i\\)\n\n\n\nExercício 4\nConsidere novamente a função \\[f(x,y)=x^4+2y^4-4xy\\] Começando no ponto \\((2,1)\\), implemente o método da Descida do Gradiente um mínimo local para esta função.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "otimizacao.html#o-método-bfgs-broyden-fletcher-goldfarb-and-shanno",
    "href": "otimizacao.html#o-método-bfgs-broyden-fletcher-goldfarb-and-shanno",
    "title": "2  Métodos de otimização",
    "section": "2.4 O Método BFGS (Broyden, Fletcher, Goldfarb, and Shanno)",
    "text": "2.4 O Método BFGS (Broyden, Fletcher, Goldfarb, and Shanno)\nSeja \\(f\\) uma função real e suave. Ao colocar o ponto de mínimo como a raiz de \\(f'(x)\\). Sabemos que a iteração do método de Newton Raphson é dado por \\[x_n=x_{n-1}+\\frac{f'(x_{n-1})}{f''(x_{n-1})}\\]\nenquanto que a iteração do método da secante é dado por\n\\[x_n=x_{n-1}+f'(x_{n-1})\\frac{x_{n-1}-x_{n-2}}{f'(x_{n-1})-f'(x_{n-2})}\\] onde \\[\\frac{f'(x_{n-1})-f'(x_{n-2})}{x_{n-1}-x_{n-2}}\\approx f''(x_{n-1})\\].\nPortanto, o método da secante, sendo um método de quase-Newton, depende apenas do gradiente para aproximar a segunda derivada.\nConsidere agora que \\(f:\\mathbb{R}^n\\rightarrow \\mathbb{R}\\), com \\(n&gt;1\\). A expansão do gradiente em série de Taylor de primeira ordem em torno de \\(\\boldsymbol{x}_n\\) é\n\\[\\nabla f(x)=\\nabla f(\\boldsymbol{x}_n)+\\mathcal{H}f(\\boldsymbol{x}_n)(\\boldsymbol{x}-\\boldsymbol{x}_n).\\] Deste modo, a matriz Hessiana em \\(\\boldsymbol{x}_n\\) deve satisfazer\n\\[\\nabla f(\\boldsymbol{x})-\\nabla f(\\boldsymbol{x}_n)=\\mathcal{H}f(\\boldsymbol{x}_n)(\\boldsymbol{x}-\\boldsymbol{x}_n).\\] O método BFGS procura criar uma sequência de matrizes positivas definidas \\(\\textbf{B}_1,\\ldots,\\textbf{B}_n\\) tais que\n\\[\\nabla f(\\boldsymbol{x}_{n+1})-\\nabla f(\\boldsymbol{x}_n)=\\textbf{B}_{n}(\\boldsymbol{x}-\\boldsymbol{x}_n).\\] de tal forma que \\(\\textbf{B}_n\\approx \\mathcal{H}(\\boldsymbol{x}_n)\\). A obtenção exata de \\(\\textbf{B}_n\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "otimizacao.html#método-de-nelder-e-mead",
    "href": "otimizacao.html#método-de-nelder-e-mead",
    "title": "2  Métodos de otimização",
    "section": "2.5 Método de Nelder e Mead",
    "text": "2.5 Método de Nelder e Mead\nUm \\(n\\)-simplex é a forma com o menor número de vértices possível em uma dimensão \\(n\\), que seriam \\(n+1\\) vértices.\n\n0-Simplex: um ponto.\n1-Simplex: um segmento de reta (ligando dois pontos).\n2-Simplex: É um triângulo (ligando três pontos não-colineares).\n3-Simplex: É um tetraedro (uma pirâmide com quatro faces triangulares).\n\nO métodos de Nelder Mead procura a direção do ótimo sem a necessidade do cálculo de derivadas, criando a busca através de uma sequência de simplex. Vamos explicar o algoritmo em detalhes para o caso geral, mas vamos ilustrar seu funcionamento para uma função \\(f:\\mathbb{R}^2\\rightarrow\\mathbb{R}\\). A ?fig-nelder apresenta o \\(n\\)-simplex inicial (um triângulo).\nPasso 1. Para o conjunto com \\(n+1\\) soluções, rotule as soluções como \\(x_0,\\ldots,x_{n-1}\\) de modo que \\(f(x_0)\\leq \\cdots\\leq f(x_n)\\). Determine o conjunto restante, dado por \\(x_0,\\ldots,x_{n-1}\\). Vamos traçar a reta que passa pela pior solução e a média dos pontos no conjunto restante \\(x_M\\) (reta em vermelho). Observe que essa reta indica a direção do salto, que é oposta à pior solução.\n\n\n\nInício de um iteração de Nelder Mead. Um simplex é contruído e uma reta é traçada entre a pior solução e o baricentro das demais para determinar a direção do salto.\n\n\nAgora que a direção do salto foi determinada, o método utiliza uma série de estratégias para determinar o tamanho do salto.\nPasso 2. Sua primeira tentativa é fazer uma reflexão, ou seja, se vale a pena procurar por uma solução utilizando o \\(n\\)-simplex refletido, calculando o ponto oposto ao pior na direção já determinada, dado por\n\\[x_r = x_M+(x_M-x_n)\\] Se \\(f(x_r)\\) é melhor que pelo menos uma das soluções no conjunto restante (ou seja, \\(f(x_r)&lt;f(x_{n-1})\\)), então, o método obteve sucesso.\n\nCaso \\(A\\): \\(f(x_r)&gt;f(x_0)\\), então a solução encontrada não é a melhor de todas, mas pode substituir a pior. Um novo simplex é construído considerando. os pontos do conjunto restante e \\(x_r\\). Volte para o Passo 1\nCaso \\(B\\): \\(f(x_r)&lt;f(x_0)\\) então uma solução melhor que as demais foi encontrada. Então o método realiza uma expansão, para determinar se vale a pena procurar por uma solução em um triângulo maior, além de \\(x_r\\), utilizando a direção já estabelecida. O ponto da expansão é dado por\n\n\\[x_e=x_M+2(x_M-x_n)\\] Se \\(f(x_e)&lt;f(x_r)\\), então a expansão apresentou uma solução melhor e um novo simplex é construído unindo o conjunto restante com o ponto \\(x_e\\). Caso contrário, o novo simplex é construído considerando o ponto \\(x_r\\). Volte para o Passo 1.\n\n\n\n\n\n\n\n\nFigure 2.2: Esquerda: reflexão. Direita: expansão\n\n\n\n\n\nPasso 3. Quando a reflexão falha em gerar um novo ponto, o método tenta uma contração.\n\n(Caso C). A contração externa ocorre quando a solução \\(x_r\\) é pior que todas as soluções no conjunto restante mas é melhor que \\(x_n\\). Isso significa que a direção da reflexão deveria estar certa mas a reflexão foi longe demais. Um ponto mais próximo de \\(x_M\\) é gerado\n\n\\[x_c=x_M+\\frac{1}{2}(x_r-x_M)\\] * (Caso D)A contração interna ocorre quando \\(x_r\\) é a pior de todas as soluções. Nesse caso o método assume que o ponto de mínimo está no interior do simplex, na direção do pior ponto. O ponto interno é dado por\n\\[x_c=x_M-\\frac{1}{2}(x_n - x_M).\\]\nSe \\(f(x_c)&lt; f(x_n)\\), então a contração teve êxito. Um novo simplex é construído considerando o ponto \\(x_c\\) e o conjunto restante. Volte para o Passo 1.\n\nPasso 4. O encolhimento é o último recurso. Ele ocorre quando todas as tentativas de melhorar o pior ponto falharam. O algoritmo reduz o tamanho do simplex encolhendo” todos os vértices (exceto \\(x_0\\)) em direção de \\(x_0\\). Todos os vértices são substituídos por\n\n\\[y_i=x_0+\\frac{1}{2}(x_i-x_0).\\] Um novo simplex será construído com os pontos \\(y_1,\\ldots,y_n\\). Retorne ao Passo 1.\nCritério de parada. Podemos parar o procedimento quando \\(|x_0-x_n|&lt;\\varepsilon\\)\nAbaixo, segue o algoritmo Nelder-Mead\n\nAlgoritmo - Método de Nelder Mead\n\nOrdenação dos Pontos: Ordene os pontos do simplex com base nos valores da função, do melhor para o pior: \\(x_0, x_1, \\ldots, x_n\\).\nCálculo do Centroide: Calcule o centroide (\\(x_M\\)) dos pontos, excluindo o pior (\\(x_n\\)). \\[x_M = \\frac{1}{n} \\sum_{i=0}^{n-1} x_i\\]\nReflexão: Calcule o ponto de reflexão (\\(x_r\\)) na direção oposta ao ponto \\(x_n\\). \\[x_r = x_M + (x_M - x_n)\\]\nAvaliação de \\(x_r\\):\n\nSucesso Extremo (\\(f(x_r) &lt; f(x_0)\\)): O ponto refletido é o melhor de todos. O método tenta uma expansão, calculando \\[x_e = x_M + 2(x_M - x_n)\\] Se \\(f(x_e) &lt; f(x_r)\\), substitua \\(x_n\\) por \\(x_e\\). Senão, substitua \\(x_n\\) por \\(x_r\\).\nSucesso Moderado (\\(f(x_{n-1}) &gt; f(x_r)\\))}: O ponto refletido é melhor que o segundo pior. Substitua \\(x_n\\) por \\(x_r\\).\nFracasso da Reflexão (\\(f(x_r) \\ge f(x_{n-1})\\)): O ponto refletido não é uma melhoria significativa. O método tenta uma contração.\n\n{Contração Externa} (\\(f(x_{n-1}) \\le f(x_r) &lt; f(x_n)\\)): Calcule o ponto de contração (\\(x_c\\)). \\[x_c = x_M + \\frac{1}{2}(x_r - x_M)\\] Se \\(f(x_c) &lt; f(x_r)\\), substitua \\(x_n\\) por \\(x_c\\). Senão, vá para o Encolhimento.\nContração Interna (\\(f(x_r) \\ge f(x_n)\\)): Calcule o ponto de contração (\\(x_c\\)). \\[x_c = x_M - \\frac{1}{2}(x_n - x_M)\\] Se \\(f(x_c) &lt; f(x_n)\\), substitua \\(x_n\\) por \\(x_c\\). Senão, vá para o Encolhimento.\n\n\nEncolhimento (Shrink): Se todas as outras tentativas falharem, encolha o simplex em direção ao melhor ponto (\\(x_0\\)). Para cada ponto \\(x_i\\) (\\(i=1, \\ldots, n\\)), substitua-o por \\(y_i\\). \\[y_i = x_0 + \\frac{1}{2}(x_i - x_0)\\]\n\n\n\nExercício 4\nConsidere novamente a função \\[f(x,y)=x^4+2y^4-4xy\\] Começando nos pontos \\((0,0)\\), \\((1,2)\\) e \\((2,1)\\), implemente o método de Nelder Mead para encontrar um mínimo local para esta função.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "otimizacao.html#a-função-optim",
    "href": "otimizacao.html#a-função-optim",
    "title": "2  Métodos de otimização",
    "section": "2.6 A função optim",
    "text": "2.6 A função optim\nA função optim agrega diversos otimizadores, sendo a principal função de otimização do R. Seus principais argumentos são:\n\npar: vetor com ponto inicial\nfn: a função a ser minimizada. O seu primeiro argumento deve ser um vetor com a mesma dimensão de par\nmethod: são os métodos implementados. Em especial destacamos as opções Nelder-Mead (padrão) e BFGS.\n\n\nExercício 5\nConsidere novamente a função \\[f(x,y)=x^4+2y^4-4xy\\] e o ponto inicial \\((0,0)\\), utilize a função optim para encontrar o mínimo global.\n\n\nExercício 6\nUtilizand a função optim, encontre as estimativas de máxima verossimilhança para o problema dado no Exemplo 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "otimizacao.html#exercícios",
    "href": "otimizacao.html#exercícios",
    "title": "2  Métodos de otimização",
    "section": "2.7 Exercícios",
    "text": "2.7 Exercícios\n\nExercício.\nProve que\n\nTodo minimizador global é um minizador local.\nNem todo minimizador local é um minimizador global\n\n\n\nExercício.\nProve que o problema \\(\\min f(x)\\) é equivalente a \\(\\max -f(x)\\).\n\n\nExercício\nUma condição \\(C\\) é dita ser suficiente para o resultado \\(A\\) quando a ocorrência de \\(C\\) implica em \\(A\\). Por outro lado, dizemos que uma condição \\(C\\) é necessária para \\(A\\) quando o resultado \\(A\\) implica na ocorrência de \\(C\\).\nVimos no Teorema da Condição Suficiente de Segunda Ordem que a condição:\n \\(\\mathcal{H}f(\\boldsymbol{x})\\) é contínua na vizinhança de \\(\\boldsymbol{x}^\\star\\), \\(\\nabla f(\\boldsymbol{x}^\\star)=\\textbf{0}\\) e \\(\\mathcal{H}f(\\boldsymbol{x}^\\star)\\) é positiva definida  é suficiente para que \\(\\boldsymbol{x}^\\star\\). Nesse exercício, vamos mostrar que ela não é necessária, ou seja, é possível que exista um minimizador local que não satifaz a condição.\nSeja \\(f(x)=x^4\\).\n\nProve que \\(x^\\star=0\\) é um minimizador global.\nProve que a condição do Teorema da Condição Suficiente de Segunda Ordem não está satisfeita.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de otimização</span>"
    ]
  },
  {
    "objectID": "integral.html",
    "href": "integral.html",
    "title": "3  Métodos de quadratura",
    "section": "",
    "text": "3.1 Regras de Newton-Cotes\nEm estatística, é usual estar interessado em encontrar expressões do tipo\n\\[E(g(\\boldsymbol{X}))=\\int_{\\mathbb{R}^n}g(\\boldsymbol{x})f(\\boldsymbol{x})d\\boldsymbol{x}.\n\\]\nTal problema nem sempre é trivial. Por exemplo, considerando \\(g(x)=I(x&lt;a)\\), e \\(f(x)\\) a função densidade da distribuição normal padrão, temos que\n\\[F(a)=\\int_{-\\infty}^a f(x)d=\\int_{\\mathbb{R}}I(x&lt;a)f(x)dx=E(g(X))\\] e, como sabemos, \\(F(x)\\) não tem expressão analítica.\nNeste momento do curso, discutiremos alguns métodos de integração numérica denominados métodos de quadratura. Tais métodos estão intimamente relacionados com a própria definição de integral. Considere que estamos interessados na área em azul da figura abaixo.\nVamos denominar o conjunto de pontos dentro da \\((x,y)\\) área em azul por \\(\\mathcal{S}\\). O valor da área é dado por\n\\[A=\\int_{(x,y)\\in \\mathcal{C}}1dxdy\\]\nObserve que a figura foi dividida em quadrados 128 quadrados de área 1.\nSeja \\(n_{I}\\) o número de quadrados de área um que estão completamente dentro de \\(\\mathcal{C}\\) e seja \\(n_S\\) o número de quadrados que fazem interseção com \\(\\mathcal{C}\\). Então\n\\[8=n_I\\leq A \\leq n_S=36\\] e podemos afirmar que \\(A\\) está entre 8 e 36. A figura abaixo identifica os quadrados 8 quadrados dentro de \\(\\mathcal{C}\\) (em azul claro) e os 36 fora (borda vermelha).\nPodemos diminuir aumentar o número de subdivisões. A figura abaixo mostra a mesma região dividida em quadrados de área 1/32. Nela temos \\(n_I=561\\) e \\(n_S=604\\), o que resulta em\n\\[17,5=561\\frac{1}{32}=n_I\\frac{1}{32}=\\leq A\\leq n_S\\frac{1}{32}=\\frac{604}{32}=20,75\\]\ne, com essa nova subdivisão, podemos afirmar que \\(A\\in[17,5\\;\\;,\\;\\;20,74]\\), com foco em funções contínuas na reta.\nPodemos continuar subdividindo a imagem até obter um intervalo \\(A\\in[a,b]\\), onde \\(|b-a|&lt;\\varepsilon\\), para qualquer tolerância \\(\\varepsilon&gt;0\\) fixada.\nEm termos gerais, suponha que estamos interessados na integral \\[\\int_a^b f(x)dx.\\]\nO princípio básico destes métodos de quadratura advém da definição da integral de Riemann: \\[\\int_a^b f(x)dx = \\lim_{n\\rightarrow\\infty}\\sum_{i=1}^{n}(x_i - x_{i-1})f(\\tilde{x}_i),\n\\] onde \\(a=x_0&lt;x_1&lt;\\cdots&lt;x_{n}=b\\) e \\(\\tilde{x}_i\\in [x_{i-1},x_{i}]\\). A integral de Riemann pode ser interpretada como a soma das áreas de infinitos retângulos. Uma maneira intuitiva de aproximar a integral de \\(f\\) é considerar \\(n\\) finito e grande o suficiente para que\n\\[\\int_a^b f(x)dx \\approx \\sum_{i=1}^{n}(x_i - x_{i-1})f(\\tilde{x}_i),\n\\]\nObserve que para qualquer intervalo \\(D\\) sempre é possível utilizar o método da substituição para fazer\n\\[\\int_D g(t)dt=\\int_0^1 f(x)dx\\] de modo que, para a maioria das integrais definidas, basta discutir o problema de integração no intervalo \\((0,1)\\). A tabela abaixo mostra algumas integrais e sua respectiva transformação para o intervalo \\((0,1)\\)\n\\[\n\\]_a^b f(x)dx=(b-a)_0^1 f(a+(b-a)t)dt$$\nPor simplicidade, vamos considerar apenas o caso \\[\\int_0^1 f(x)dx.\\]\nConsidere a integral \\[I=\\int_0^1 f(x)dx\\] Os métodos de Newton-Cotes, são métodos de quadratura que dividem o intervalo \\([0,1]\\) em \\(n\\) subintervalos de comprimento \\(1/n\\), definidos pelos pontos \\(0=x_0,\\ldots,x_n=1\\). Vamos apresentar três variantes do método, denominadas regras:\nA regra do ponto médio, considera como conjunto de nós os pontos médios dos subintervalos, ou seja \\[\\tilde{x}_i=\\frac{1}{2}\\left[\\frac{i}{n}+\\frac{i-1}{n}\\right]=\\frac{2i-1}{2n}.\\] Deste modo, a integral\n\\[\\int_{x_{i-1}}^{x_{i}} f(x)dx\\] é aproximada pela área do retângulo de base \\(1/n\\) e altura \\(f(\\tilde{x}_i)\\), ou seja \\[\\int_{x_{i-1}}^{x_{i}} f(x)dx\\approx \\frac{f(\\tilde{x}_i)}{n}.\\]\nA fórmula da integral da regra do ponto médio se torna\n\\[\\hat{I}_n=\\sum_{i=1}^{n}w_if(\\tilde{x}_i)=\\frac{1}{n}\\sum_{i=1}^{n} f\\left(\\frac{2i-1}{2n}\\right)\\] onde \\(w_i=1/n\\) para \\(i=1,\\ldots,n\\).\nA regra do trapézio aproxima a integral no intervalo pela área de um trapézio. A Figure 3.2 ilustra a regra do trapézio em conjunto com a regra do ponto médio para o primeiro exemplo desta seção.\nfx = function(x) x*(1-x)\noo = par( mfrow = c(1,2))\ncurve(fx(x), 0, 1, col ='gray',lwd = 2, ylab = expression( f(x)))\nsegments(0,0,0,fx(1/8),lwd = 2)\nsegments(1/4,0,1/4,fx(1/8), lwd = 2)\nsegments(0,fx(1/8),1/4,fx(1/8), lwd = 2)\n\nsegments(1/4,0,1/4,fx(3/8),lwd = 2)\nsegments(2/4,0,2/4,fx(3/8), lwd = 2)\nsegments(1/4,fx(3/8),2/4,fx(3/8), lwd = 2)\n\nsegments(2/4,0,2/4,fx(5/8),lwd = 2)\nsegments(3/4,0,3/4,fx(5/8), lwd = 2)\nsegments(2/4,fx(5/8),3/4,fx(5/8), lwd = 2)\n\nsegments(3/4,0,3/4,fx(7/8),lwd = 2)\nsegments(4/4,0,4/4,fx(7/8), lwd = 2)\nsegments(3/4,fx(7/8),4/4,fx(7/8), lwd = 2)\ntitle(sub = '(a)')\n\ncurve(fx(x), 0, 1, col ='gray',lwd = 2, ylab = expression( f(x)))\nsegments(0,0,0,fx(0),lwd = 2)\nsegments(1/4,0,1/4,fx(1/4), lwd = 2)\nsegments(0,fx(0),1/4,fx(1/4), lwd = 2)\n\nsegments(1/4,0,1/4,fx(1/4),lwd = 2)\nsegments(2/4,0,2/4,fx(2/4), lwd = 2)\nsegments(1/4,fx(1/4),2/4,fx(2/4), lwd = 2)\n\nsegments(2/4,0,2/4,fx(2/4),lwd = 2)\nsegments(3/4,0,3/4,fx(3/4), lwd = 2)\nsegments(2/4,fx(2/4),3/4,fx(3/4), lwd = 2)\n\nsegments(3/4,0,3/4,fx(3/4),lwd = 2)\nsegments(4/4,0,4/4,fx(4/4), lwd = 2)\nsegments(3/4,fx(3/4),4/4,fx(1), lwd = 2)\ntitle(sub = '(b)')\n\npar(oo)\n\n\n\n\n\n\n\nFigure 3.2: Ambos os gráficos apresentam a função dada no Exemplo 1. (a) Método Newton-Cotes com regra retangular. (b) Método Newton-Cotes com regra trapezoidal.\nA regra do trapézio interpola os pontos \\((x_{i-1},f(x_{i-1}))\\) e \\((x_{i},f(x_{i}))\\) através de uma linha reta para delimitar o trapézio correspondente. Para o intervalo \\((x_{i-1},x_{i})\\) a integral \\[\\int_{x_{i-1}}^{x_{i}}f(x)dx\\] é aproximada pela área do trapézio correspondente, dada por\n\\[\\frac{1}{2}(x_{i}-x_{i-1})(f(x_i)+f(x_{i-1}))=\\frac{1}{2n}[f(x_i)+f(x_{i-1})]\\] Considerando \\(0=x_1,\\ldots,x_n=1\\) igualmente espaçados, teremos que a integral de Newton-Cotes com regra do trapézio é dada por\n\\[\\begin{align}\\hat{I}_n=\\frac{1}{2n}\\sum_{i=1}^{n}[f(x_{i})+f(x_{i-1})]\\\\&=\\end{align}\\] Pode-se provar que \\[\\begin{align}\\hat{I}_n=\\sum_{i=0}^{n}w_if(x_i),\\end{align}\\]\nou seja, o conjunto de nós na regra do trapézio são os pontos \\(x_0,\\ldots,x_n=1\\) e \\[w_i=\\left\\{\\begin{array}{ll}\\frac{1}{2n},&\\hbox{ se }i=0\\\\\n\\frac{1}{n},&i=1,\\ldots,n-1\\\\\\frac{1}{2n},&\\hbox{ se }i=n\\end{array}\\right.\\]\nA regra de Simpson aproxima \\(f(x)\\) no intervalo \\((x_{i-1},x_i)\\) através por um polinômio de grau dois que passa pelos pontos \\((x_{i-1},f(x_{i-1})), (\\tilde{x}_i,f(\\tilde{x}_i))\\) e \\((x_i,f(x_i))\\), onde \\(\\tilde{x}_i\\) é o ponto médio do intervalo. Tal polinômio é dado por \\[P(x)=2n^2f(x_{i-1})(x-\\tilde{x}_i)(x-x_i)-4n^2f(\\tilde{x}_i)(x-x_{i-1})(x-x_i)+2n^2f(x_i)(x-x_{i-1})(x-\\tilde{x}_i)\\] Considerando que\n\\[\\begin{align}\\int_{x_{i-1}}^{x_i}f(x)dx&\\approx \\int_{x_{i-1}}^{x_i} P(x)dx=\\frac{1}{3n}\\left[f(x_{i-1})+4f(\\tilde{x}_i)+f(x_i)\\right]\\end{align}\\] A intregral obtida pela regra de Simpson é dada por \\[\\hat{I}_n=\\frac{1}{3n}\\sum_{i=1}^n [f(x_{i-1})+4f(\\tilde{x}_i)+f(x_i)].\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de quadratura</span>"
    ]
  },
  {
    "objectID": "integral.html#regras-de-newton-cotes",
    "href": "integral.html#regras-de-newton-cotes",
    "title": "3  Métodos de quadratura",
    "section": "",
    "text": "A regra do ponto médio\nA regra do trapézio\nA regra de Simpson\n\n\n\n\n\n\nExample 3.1 Para exemplificar, considere a integral \\[\\begin{equation}\\label{eq::integral_beta}\n\\int_0^1 x(1-x)dx,\n\\end{equation}\\] cujo resultado, igual a \\(1/6\\approx 0,166\\), pode ser obtido analiticamente.\nAo escolher o conjunto com \\(n=4\\), temos \\[\\begin{align}\\hat{I}_4&=\\frac{1}{4}\\sum_{i=1}^4 f\\left(\\frac{2i-1}{8}\\right)=\\frac{1}{4}\\sum_{i=1}^4 \\left(\\frac{2i-1}{8}\\right)\\left(1-\\frac{2i-1}{8}\\right)\\\\&=\n\\frac{1}{4}\\sum_{i=1}^4 \\left(\\frac{2i-1}{8}\\right)\\left(\\frac{9-2i}{8}\\right)=\\frac{7+15+15+7}{256}\\\\&=\\frac{44}{256}=0,1660377\\end{align}\\] A figura abaixo mostra a aproximação desta integral pelos 4 retângulos.\n\nfx = function(x) x*(1-x)\ncurve(fx(x), 0, 1, col ='gray',lwd = 2, ylab = expression( f(x)))\nsegments(0,0,0,fx(1/8),lwd = 2)\nsegments(1/4,0,1/4,fx(1/8), lwd = 2)\nsegments(0,fx(1/8),1/4,fx(1/8), lwd = 2)\n\nsegments(1/4,0,1/4,fx(3/8),lwd = 2)\nsegments(2/4,0,2/4,fx(3/8), lwd = 2)\nsegments(1/4,fx(3/8),2/4,fx(3/8), lwd = 2)\n\nsegments(2/4,0,2/4,fx(5/8),lwd = 2)\nsegments(3/4,0,3/4,fx(5/8), lwd = 2)\nsegments(2/4,fx(5/8),3/4,fx(5/8), lwd = 2)\n\nsegments(3/4,0,3/4,fx(7/8),lwd = 2)\nsegments(4/4,0,4/4,fx(7/8), lwd = 2)\nsegments(3/4,fx(7/8),4/4,fx(7/8), lwd = 2)\n\n\n\n\n\n\n\nFigure 3.1: A função dada no exemplo Exemplo 1 e os retângulos utilizados para computar a integral via método de Newton-Cotes com a regra retangular.\n\n\n\n\n\n\n\nExercício. Seja \\(n\\) o número de intervalos para a regra do ponto médio. No R, \\(0=x_0,\\ldots,x_n=1\\) pode ser obtido pela função\nxi = seq(0,1, length = n+1)\ne os pontos médios podem ser obtidos utilizando a função\n.5* (xi[-1] + xi[-n])\nUtilizando essas funções, utilize a regra do ponto médio para resolver a integral \\[\\int_0^1 x^{3/2}(1-x)^{7/2}dx.\\]\nA integral acima é um caso particular da função Beta(\\(a,b\\)), definida por\n\\[B(a,b)=\\int_0^1 x^{a-1}(1-x)^{b-1}dx,\\] onde \\(a,b&gt;0\\). No R, esta função está implementada na função beta(a,b). Compare o valor que você obteve com o valor da função beta(3/2+1, 7/2+1).\n\n\nExercício. Considere a seguinte integral.\n\\[\\int_{-1,96}^{1,96} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}dx.\\]\n\nTransforme a integral para o intervalo (0,1).\nAproxime o valor da integral utilizando a regra do ponto médio. Certifique-se de obter uma tolerância de 0,0001 (quatro casas decimais)\n\n\n\n\n\n\n\n\n\n\n\nExemplo. Utilize a regra de Simpson para aproximar integral \\[\\int_0^1 x^{5/2}e^{-x}dx\\] com uma tolerância igual a 0,0001.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de quadratura</span>"
    ]
  },
  {
    "objectID": "integral.html#quadratura-gaussiana",
    "href": "integral.html#quadratura-gaussiana",
    "title": "3  Métodos de quadratura",
    "section": "3.2 Quadratura Gaussiana",
    "text": "3.2 Quadratura Gaussiana\nSeja \\(P_n(x)\\) um polinômio de grau \\(n\\). Dizemos que os polinômios \\(P_0(x),P_1(x),\\ldots,\\) são ortonormais para o intervalo \\([a,b]\\) e para a função de peso \\(w(x)\\geq 0\\) se \\[\\begin{align}\\int_a^b w(x)P_i(x)P_j(x)dx&=0\\\\\\int_a^b w(x)P_i(x)^2dx&=1\\end{align}\\] para todo \\(i\\neq j\\). Esse polinômios podem ser escritos como \\[P_n(x)=k_n\\prod_{i=1}^n(x-t_i),\\] onde \\(k_n&gt;0\\) e \\(a&lt;t_1&lt;\\cdots&lt;t_n&lt;b\\) são suas raízes reais.\nO seguinte teorema relaciona a integral de uma função \\(f\\) contínua em \\([a,b]\\) e os polinômios ortonormais.\n\nTheorem 3.1 Teorema Seja \\(f(x)\\) uma função contínua em \\([a,b]\\). Para qualquer polinômio ortonormal \\(P_n(x)\\) sejam \\(t_1,\\ldots,t_n\\) suas raízes. Então,\n\\[\\int_a^b f(x)w(x)dx=\\sum_{j=1}^n w_j f(t_j)+\\frac{f^{(2n)}(\\xi)}{k_n^2 (2n)!},\\] onde \\(\\xi\\in[a,b]\\), \\(f^{(2n)}\\) é a derivada de ordem \\(2n\\) da função e \\[w_j=-\\frac{k_{n+1}}{k_n}\\frac{1}{P_{n+1}(t_j)P_n'(t_j)}.\\]\n\nO teorema acima mostra que a integral \\(\\int_a^b f(x)w(x)dx\\) é bem aproximada por \\[\\sum_{j=1}^n w_j f(t_j),\\] uma vez que o termo restante decresce para zero rapidamente. Além disso, \\(f^{(2n)}(x)=0\\) se \\(f(x)\\) for um polinômio de grau \\(2n+1\\), o que implica que o resultado é exato. Podemos então definir a quadratura gaussiana.\n\nDefinition 3.2 (Quadratura gaussiana) Seja \\(f(x)\\) uma função contínua em \\([a,b]\\) e sejam \\(P_0(x),P_1(x),\\ldots,\\) polinômios ortogonais no mesmo intervalo considerando a função peso \\(w(x)\\leq\\). Então, a integral \\[\\int_a^b f(x)w(x)dx\\] então a quadratura gaussiana com \\(n\\) nós é dada por \\[\\hat{I}_n=\\sum_{j=1}^n w_j f(t_j).\\]\n\nEm geral, tal quadratura costuma vir acompanhada com o nome do polinômio ortogonal utilizado. A tabela abaixo apresenta as quadraturas usuais.\n\\[\\begin{array}{lll}\\hline\n\\hbox{Quadratura} & \\hbox{Intervalo} & \\hbox{Função peso} \\\\ \\hline\n\\hbox{Gauss-Legendre} & [-1,1] & w(x)=1 \\\\\n\\hbox{Gauss-Chebyshev} & [-1,1] & w(x)=(1-x^2)^{1/2} \\\\\n\\hbox{Gauss-Jacobi} & [-1,1] & w(x)=(1-x)^\\alpha (1+x)^\\beta, \\alpha,\\beta&gt;1 \\\\\n\\hbox{Gauss-Laguerre} & [0,\\infty) & w(x)=e^{-x}x^\\alpha, \\alpha&gt;-1 \\\\\n\\hbox{Gauss-Hermite} & (-\\infty,\\infty) & w(x)=e^{-x^2} \\\\\n\\end{array}\\]\n\nExercício. Para a quadratura de Gauss-Legendre com 5 nós, temos\n\\[\\begin{array}{cc}\\hline \\hbox{Nó}(t_j)& \\hbox{Peso}(w_j) \\\\ \\hline\n-\\frac{1}{3}\\sqrt{5+2\\sqrt{\\frac{10}{7}}}\n& \\frac{322-13\\sqrt{70}}{900} \\\\\n-\\frac{1}{3}\\sqrt{5-2\\sqrt{\\frac{10}{7}}}\n& \\frac{322+13\\sqrt{70}}{900} \\\\\n0 & \\frac{128}{225} \\\\\n+\\frac{1}{3}\\sqrt{5-2\\sqrt{\\frac{10}{7}}}\n& \\frac{322+13\\sqrt{70}}{900} \\\\\n+\\frac{1}{3}\\sqrt{5+2\\sqrt{\\frac{10}{7}}}\n& \\frac{322-13\\sqrt{70}}{900} \\\\ \\hline\n\\end{array}\\]\nResolva a integral\n\\[\\int_{-1,96}^{1,96} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}dx\\] utilizando essa quadratura.\n\n\nRegra de Gauss-Kronrod\nConsidere a quadratura gaussiana \\(\\hat{I}_n\\). Sabemos que os nós desta quadratura são \\(t_1,\\ldots,t_n\\), relacionadas com as raízes do polinômio ortogonal \\(P_n(x)\\).\nAregra de Gauss-Kronrod consiste em criar o conjunto de nós \\(y_1,\\ldots,y_{2n+1}\\) contendo os nós \\(t_1,\\ldots,t_n\\) da regra de Gauss e um novo conjunto com \\(n+1\\) nós obtidos através do polinômio \\(Q_{n+1}(x)\\) ortogonal ao polinômio de Gauss \\(P_n(x)\\) e a todos os polinômios de grau inferior ou igual a \\(n-1\\). Deste modo, a regra de Gauss-Kronrod é dada por\n\\[\\hat{K}_{2n+1}=\\sum_{i=1}^{2n+1} \\tilde{w}_i f(y_i)\\] Observe que, na prática, tanto os pesos quanto os nós das regras de Gauss ou de Gauss-Kronrod já estão pré-calculados. A real vantagem do método de Gauss-Kronrod está no número de cálculos para estimar o erro, conforme vemos abaixo:\n\nRegra de Gauss. Para comparar o erro da integral utilizando a difereça \\(|\\hat{I}_{2n+1}-\\hat{I}_n|\\), a função \\(f(.)\\) deve ser avaliada \\(3n+1\\) vezes, porque existem \\(3n+1\\) nós distintos.\nRegra de Gauss-Kronrod. Para comparar o erro da integral, pode-se utilizar a difereça \\(|\\hat{K}_{2n+1}-\\hat{I}_n|\\). Nesse caso, a função \\(f(.)\\) deve ser avaliada apenas \\(2n+1\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de quadratura</span>"
    ]
  },
  {
    "objectID": "integral.html#quadratura-adaptativa-e-a-função-integrate",
    "href": "integral.html#quadratura-adaptativa-e-a-função-integrate",
    "title": "3  Métodos de quadratura",
    "section": "3.3 Quadratura adaptativa e a função integrate",
    "text": "3.3 Quadratura adaptativa e a função integrate\nA quadratura adaptativa é um método de integração numérica que ajusta dinamicamente os nós para maximizar a precisão onde a função é mais difícil de ser integrada.\nA ideia central é um processo de subdivisão recursiva. Utilizando a regra de Gauss-Kronrod como exemplo, temos os seguintes passos:\n\nAproximação Inicial: A integral é calculada em todo o intervalo \\([a,b]\\) usando a regra de Gauss-Kronrod. O resultado é a aproximação \\(\\hat{K}_{2n+1}\\) com uma estimatvida do erro dada por \\(\\hbox{erro}=|\\hat{I}_n-\\hat{K}_{2n+1}|\\)\nVerificação do Erro: O erro é comparado com uma tolerância \\(\\varepsilon\\).\n\nSe o erro &lt;\\(\\varepsilon\\), a aproximação é considerada suficientemente precisa. A integração para esse intervalo é concluída e o resultado é retornado.\nSe o erro \\(\\geq \\varepsilon\\), a aproximação não é precisa o suficiente. O método precisa de mais avaliações para melhorar o resultado.\n\nSubdivisão do Intervalo: O intervalo original \\([a,b]\\) é dividido em dois subintervalos, \\([a,m]\\) e \\([m,b]\\), onde \\(m\\) é o ponto médio. A ideia é que, ao subdividir o intervalo, a função em cada pedaço se torne mais suave, permitindo que as regras de quadratura funcionem melhor.\nProcesso Recursivo: O método se aplica novamente a cada um dos novos subintervalos, com uma nova tolerância para cada um (por exemplo, metade da tolerância original para cada subintervalo).\nCombinação dos Resultados: A aproximação final para a integral original é a soma das aproximações de cada subintervalo que foram aceitas.\n\nA função integrate, do pacote stats do R utiliza uma quadratura adaptativa com a regra de Gauss-Kronrod para aproximar a integral de uma função real. Seus argumentos principais são:\n\nf: função cujo primeiro argumento é um vetor numérico e que retorne um vetor numérico de mesmo comprimento.\nlower,upper: os limites de integração. É possível fazer lower = -Inf e upper = Inf\n\n\nExercício.\nCalcule analiticamente as integrais:\n\n\\(\\int_0^2 x^2dx\\)\n\n2 \\(\\int_0^\\pi \\sin(x)dx\\)\n\n\\(\\int_0^{10} e^{-x}dx\\)\n\nEm seguida, calcule as mesmas integrais utilizando a função integrate.\n\n\nExercício\nConsidere a função densidade\n\\[f(x)=\\frac{2}{5\\sqrt{\\pi}}e^{-x^2}+ \\frac{2}{15}x^3 e^{-x},\\] para \\(x&gt;0\\). Combine as funções integrate e uniroot para encontrar a mediana deste modelo.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de quadratura</span>"
    ]
  },
  {
    "objectID": "integral.html#exercícios",
    "href": "integral.html#exercícios",
    "title": "3  Métodos de quadratura",
    "section": "3.4 Exercícios",
    "text": "3.4 Exercícios\n\nExercício Considerando a regra do trapézio prove que \\[\\hat{I}_n=\\sum_{i=0}^n w_i f(x_i),\\] onde \\(0=x_0,\\ldots,x_n=1\\) é o conjunto de nós e \\[w_i=\\left\\{\\begin{array}{ll}\\frac{1}{2n},&\\hbox{ se }i=0\\\\\n\\frac{1}{n},&i=1,\\ldots,n-1\\\\\\frac{1}{2n},&\\hbox{ se }i=n\\end{array}\\right.\\]\n\n\nExercício Considerando a regra do trapézio prove que \\[\\hat{I}_n=\\sum_{i=0}^n w_i f(x_i),\\] onde \\(0=x_0,\\ldots,x_n=1\\) é o conjunto de nós e \\[w_i=\\left\\{\\begin{array}{ll}\\frac{1}{2n},&\\hbox{ se }i=0\\\\\n\\frac{1}{n},&i=1,\\ldots,n-1\\\\\\frac{1}{2n},&\\hbox{ se }i=n\\end{array}\\right.\\]\n\n\nExercício Resolva analiticamente a integral\n\\[\\int_0^1 (a+bx)dx\\]\nAgora, considerando \\(n=1\\), resolva essa integral utilizando a regra do ponto médio. Qual é a sua conclusão?\n\n\nExercício Considere a regra de Simpson para aproximar a integral de uma função \\(f(x)\\) no intervalo \\([0,1]\\) utilizando \\(n\\) subintervalos. A fórmula da aproximação é dada por:\n\\[\\hat{I}_n=\\frac{1}{3n}\\sum_{i=1}^n[f(x_{i-1})+4f(\\tilde{x}_i)+f(x_i)]\\] Mostre que essa mesma aproximação pode ser escrita como \\[\\hat{I}_n=\\sum_{j=0}^{2n}w_jf(y_j)\\] onde \\(y_j=j/2n\\) para \\(j=0,\\ldots 2n\\) e\n\\[w_j=\\left\\{\\begin{array}{ll} \\frac{1}{3n},&j=0\\\\\\frac{4}{3n},&j \\hbox{ é ímpar}\\\\\\frac{2}{3n},&j\\hbox{ é par diferente de } 2n\\\\ \\frac{1}{3n},&j=2n\\end{array}\\right.\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos de quadratura</span>"
    ]
  },
  {
    "objectID": "gerador_congruencial.html",
    "href": "gerador_congruencial.html",
    "title": "4  Introdução aos geradores de números pseudo-aleatórios",
    "section": "",
    "text": "4.1 Geradores de números pseudo-aleatórios\nA noção de gerar números aleatórios é antiga, sendo que sua função primordial era o sorteio, com o intuito religioso ou por lazer.\nNo começo do século XX, geradores de números aleatórios simples, baseados em sorteios, foram utilizados para a seleção de amostras e desenhos de experimentos. Contudo, para problemas mais complexos, envolvendo o sorteio de uma coleção muito grande de números, a utilização de dados, moedas ou urnas não eram uma opção viável. Uma solução inicial foi a produção de tabelas contendo números aleatórios. Tippet (1927) produziu uma tabela com 41.600 números aleatórios selecionados do censo de 1925. Fisher e Yates (1943) produziram uma tabela de números aleatórios selecionados de uma tabela de logaritmos. Kermack e Kendrick (1937) criaram uma tabela de números aleatórios baseada em um diretório de números telefônicos.\nPor volta dos anos 50, a simulação era necessária para o uso no Método de Monte Carlo (um meio de calcular integrais). Agora, milhares de números aleatórios eram necessários. Em 1955, a RAND Corporation produziu suas tabelas de números aleatórios, com milhões de dígitos, produzidos por um processo físico (uma fonte de pulso de frequência aleatória).\nAtualmente, os números aleatórios (ou melhor, pseudo-aleatório, como veremos a seguir) são gerados por computadores com objetivos diversos como\nComputadores digitais não são capazes de gerar números aleatórios. Em vez disso, eles se utilizam de uma sequência de números \\(x_{i-1},\\ldots,x_{i-k}\\) para gerar o número \\(x_i\\), isto é,\n\\[x_i=f(x_{i-1},\\ldots,x_{i-k}).\\] Acima, a função \\(f(.)\\) é denominada gerador e \\(k\\) é a ordem do gerador. Os valores iniciais da sequência são denominados semente. Pela natureza determinística do gerador, todas as sequências geradas com a mesma semente são iguais. Um bom gerador faz com que não seja fácil determinar qual será o novo valor baseado nos anteriores.\nComo um computador só pode representar um número finito de números, é certo que a sequência se repetirá em algum momento. O comprimento da sequência do início até a repetição é denominado período.\nAo longo destas notas de aula, o termo “aleatório” será utilizado no lugar “pseudo-aleatório”. Os termos gerador de simulador de números aleatórios são sinônimos.\nNosso objetivo inicial é construir um simulador que gere sequências que se parecem com uma amostra de variáveis aleatórias independentes de uma Uniforme(0,1). Na prática, os simuladores de números pseudo-aleatórios geram uma sequência de números inteiros \\(x_1,x_2,\\ldots\\) não repetidos onde cada inteiro é não nulo e menor que um inteiro \\(m\\). A aproximação para Uniforme(0,1) é feita escalando a sequência gerada para o intervalo (0,1), ou seja, fazendo \\(x_1/m,\\ldots,x_p/m\\)\nExistem duas técnicas básicas para gerar números pseudo-aleatórios:\nVamos explorar as principais ideias por trás destes geradores. Para tanto, é fundamental alguns resultados de artimética modular.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução aos geradores de números pseudo-aleatórios</span>"
    ]
  },
  {
    "objectID": "gerador_congruencial.html#geradores-de-números-pseudo-aleatórios",
    "href": "gerador_congruencial.html#geradores-de-números-pseudo-aleatórios",
    "title": "4  Introdução aos geradores de números pseudo-aleatórios",
    "section": "",
    "text": "Métodos congruenciais\nMétodos de registro de mudança de feedback",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução aos geradores de números pseudo-aleatórios</span>"
    ]
  },
  {
    "objectID": "gerador_congruencial.html#tópicos-selecionados-de-aritmética-modular",
    "href": "gerador_congruencial.html#tópicos-selecionados-de-aritmética-modular",
    "title": "4  Introdução aos geradores de números pseudo-aleatórios",
    "section": "4.2 Tópicos selecionados de aritmética modular",
    "text": "4.2 Tópicos selecionados de aritmética modular\nÉ importante ressaltar que os dois métodos citados acima são baseados na aritmética modular - um sistema aritmético para inteiros onde os números retrocedem ao atingir um valor denominado módulo. Um exemplo familiar desta aritmética é o relógio de ponteiro, no qual a contagem volta ao início após o ponteiro chegar em 12h (neste caso, o módulo é 12). A relação básica desta aritmética é a congruência módulo, definida abaixo.\n\nDefinition 4.2  \nSeja \\(m\\) um inteiro positivo. Dois números inteiros \\(a\\) e \\(b\\) são congruentes módulo \\(m\\) se sua diferença é um inteiro divisível por \\(m\\). Utilizamos a notação \\(a\\equiv b \\pmod m\\) (lê-se \\(a\\) é congruente a \\(b\\) módulo \\(m\\).\n\nA congruência módulo pode ser entendida facilmente com a analogia com o relógio de ponteiros. Por exemplo, a Figure 4.1 apresenta um relógio apontando para o 3. Se for de madrugada, a interpretação será de que são 3h. Em caso contrário serão 15h. Portanto, 15h é equivalente a 3h nesse relógio, ou ainda, 15 é congruente a 3 módulo 12. Para mostrar esse fato, basta notar que\n\\[\\frac{15-3}{12}=\\frac{12}{12}=1,\\] e, como o resultado da divisão acima é um inteiro, temos a referida congruência.\n\n\nWarning: pacote 'ggplot2' foi compilado no R versão 4.5.1\n\n\n\n\n\n\n\n\nFigure 4.1: Um relógio de ponteiros. O objetivo é apresentar a ideia de que 3h e 15h são equivalentes no relógio de 12h (ou 15 é congruente a 3 módulo 12)\n\n\n\n\n\n\nExercício 1. Encontre quatro números que são congruentes a 2 módulo 5.\n\nA congruência módulo é simétrica, reflexiva e transitiva, como mostra a seguinte proposição\n\nPropriedades 1.\n\n\\(a\\equiv b \\pmod m \\Rightarrow b\\equiv a\\pmod m\\) (simétrica)\n\\(a \\equiv a \\pmod m\\) \\(\\forall \\;a\\) (reflexiva)\n\\(a\\equiv b\\pmod m\\) e \\(b\\equiv c\\pmod m\\Rightarrow\\) \\(a\\equiv c\\pmod m\\) (transitiva)\n\n\nAlém das propriedades acima, podemos apresentar as seguintes operações.\n\nPropriedades 2.\n\n\\((a+b) \\pmod m = a\\pmod m + b\\pmod m\\)\n\\((ab) \\pmod m = (a\\pmod m)(b\\pmod m)\\)\n\n\nA redução modular \\(b\\pmod m\\) é definida como o resto da divisão euclidiana de \\(b\\) por \\(m\\), sendo o seu resultado denominado resíduo. Relembre que, para a divisão de \\(b\\) por \\(m\\), temos que \\[b=qm+r\\] onde \\(q\\) é o quociente inteiro e \\(r\\) o resto, que satisfaz \\(0\\leq r &lt; m\\). Exemplos:\n\n\\(10\\pmod 5\\): como\n\n\\[\\frac{b}{m}=\\frac{10}{5}=2\\Rightarrow b=2m+0,\\] logo, o resíduo é 0.\n\n\\(15\\pmod {10}\\): como\n\n\\[\\frac{b}{m}=\\frac{15}{10}=\\frac{10}{10}+\\frac{5}{10}=1+\\frac{5}{10}\\Rightarrow b=m+5,\\] o resíduo é 5.\n\n\\(5\\pmod {10}\\): como\n\n\\[\\frac{b}{m}=\\frac{5}{10}=0+\\frac{5}{10}\\Rightarrow b=0.m+5\\] o resíduo é 5.\n\nImportante. A redução modular \\(b\\pmod m\\) é dada por \\[b-\\lfloor b/m\\rfloor m, \\] onde \\(\\lfloor.\\rfloor\\) é o maior interio menor ou igual ao argumento. No R pode-se calcular \\(\\lfloor b/m\\rfloor\\) usando a função b%/%m. Assim, o resíduo pode ser calculado por b- b%/%m *m.\n\n\nExercício. Crie um algoritmo para encontrar os resíduos de \\(b\\mod 31\\) com \\(b=31,32,\\ldots,100\\)\n\nObserve que os resíduos são inteiros dentro do conjunto \\(\\{0,1,\\ldots,m-1\\}\\). Desde que \\(b\\) não seja múltiplo de \\(m\\), é correto afirmar que \\(0&lt;r&lt;m\\), onde \\(r\\) é o resíduo. Deste modo, teremos\n\\[0&lt;\\frac{r}{m}&lt;1.\\] A capacidade de gerar todos os números inteiros entre 0 e \\(m-1\\) sem repetição será explorada mais adiante para a contrução de um gerador. O próximo teorema será fundamental mais adiante.\n\nInverso multiplicativo modular. Existe um inteiro \\(b\\) tal que \\(ab\\equiv 1\\pmod m\\) se e somente se o máximo divisor comum entre \\(a\\) e \\(m\\) é 1 (ou seja, \\(a\\) e \\(m\\) são primos entre si).\n\n\n4.3 Geradores lineares congruenciais simples\nLehmer (1951) propôs o gerador linear congruencial simples (GLCS), dado por\n\\[x_i\\equiv (ax_{i-1}+b)\\pmod m\\] onde \\(a\\) e \\(b\\) são inteiros denominados multiplicador e incremento. Escolhemos uma semente inicial \\(x_0\\), com \\(0\\leq x_0 &lt;m\\) (note que não faz sentido ter uma semente maior que \\(m\\), pois haverá outra menor que será congruente com esta). Como \\(x_i\\) é um resíduo, sabemos que \\(0\\leq x_i\\leq m-1\\), o que implica em \\(m\\) possibilidades de números. Portanto, esse gerador possui, no máximo, período \\(m\\).\nNo exemplo abaixo, o GLCS com semente inicial \\(x_0=7\\), \\(a=5\\) e \\(b=3\\) gerou a seguinte sequência de números.\n\na = 5\nm = 8\nb = 3\nx = 7\n\nfor(i in 2:(m+3)){\n  y &lt;- (a*x[i-1] + b)\n  x[i] &lt;- y - m * y%/% m \n}\nx\n\n [1] 7 6 1 0 3 2 5 4 7 6 1\n\n\nObserve que a semente inicial se repetiu na iteração 9, o que implica que esse gerador conseguiu simular todos os 8 números distintos possíveis.\nO GLCS tem dentre os seus resultados, a possibilidade de gerar um resíduo igual a 0. Embora isto não seja um problema para o gerador em si, essa característica é indesejável, pois nosso objetivo é obter \\[\\frac{r_i}{m}\\in(0,1)\\] logo, o gerador não pode simular o valor \\(r_i=0\\).\nAo fazer \\(b=0\\), temos o gerador congruencial multiplicativo (GCM). Desde que \\(0&lt;x_0&lt;m\\), o GCM não vai gerar o número 0 se \\(m\\) for primo, uma vez que \\(ax_{i-1}\\) nunca será múltiplo de \\(m\\). Deste modo, o GCM pode gerar os números no conjunto \\(\\{1,\\ldots,m-1\\}\\) tendo, portanto, período máximo igual a \\(m-1\\).\n\n Exemplo 1 Considere o GCM com \\(m=31\\) (primo) e \\(a=7\\), com \\(x_0=19\\). Abaixo mostramos os 30 valores gerados:\n\n# criando um vetor semente igual a 19\nsemente &lt;- 19\nx &lt;- semente\n\n# criando o multiplicador e o módulo\na &lt;- 7\nm &lt;- 31\n\n# gerando os números pseudo-aleatórios\nfor(i in 2:m){\n  y &lt;- a * x[i-1]\n  x[i] &lt;- y - y%/%m * m\n}\n\n# resultado\n(amostra &lt;- x[-1])\n\n [1]  9  1  7 18  2 14  5  4 28 10  8 25 20 16 19  9  1  7 18  2 14  5  4 28 10\n[26]  8 25 20 16 19\n\n# período\nwhich(amostra == semente)\n\n[1] 15 30\n\n\nEmbora o período máximo seja 30, esse gerador tem período 15.\n\nO exemplo acima mostra que ter \\(m\\) primo não é suficiente para obter o período máximo. Vamos avaliar melhor essa questão. Note que \\(x_k \\equiv ax_{k-1}\\pmod m\\) implica que existe \\(c\\) inteiro tal que \\((x_k-ax_{k-1})/m = c\\). Disto, temos que:\n\\[\\begin{align}c&= \\frac{x_k - ax_{k-1}}{m}=\\frac{x_k \\pm a^2 x_{k-2}- ax_{k-1}}{m}=\\frac{x_{k}-a^2 x_{k-2} -a(x_{k-1}-a x_{k-2})}{m}\\\\ &=\\frac{x_k-a^2 x_{k-2}}{m}+a\\frac{x_{k-1}-ax_{k-2}}{m}\\end{align}\\]\ne, como \\(x_{k-1}\\) e \\(ax_{k-2}\\) são congruentes módulo \\(m\\), existe um inteiro \\(c'\\) tal que\n\\[\\begin{align}c&= \\frac{x_k-a^2 x_{k-2}}{m}+ak_2\\Rightarrow \\frac{x_k-a^2 x_{k-2}}{m}=c-ac'\\end{align}\\] e, como \\(c-ac'\\) é inteiro, temos que \\[x_k\\equiv a^2 x_{k-2} \\pmod m.\\] Podemos fazer uma indução e mostrar que $x_k a^k x_0m $. Em geral, os geradores de números pseudo-aleatórios corretamente implementados devem começar a se repetir na semenente inicial. Deste modo, um gerador com período \\(k\\) deve ter \\(x_k=x_0\\), o que implica em\n\\[x_0\\equiv a^kx_0\\pmod m \\Rightarrow a^kx_0=x_0\\pmod m.\\] Disto, podemos enunciar a seguinte proposição.\n\nPara um GCM, se \\(x_0\\) e \\(m\\) são primos entre si, então o menor valor de \\(k\\) que satisfaz \\[a^k\\equiv 1\\pmod m\\] é o periodo do GCM.\n\nA demonstração da proposição se dá pela existência de \\(x_0^{-1}\\), garantida pelo Teorema do Inverso Modular. \\[a^k x_0 x_0^{-1 }\\equiv x_0 x_0^{-1}\\pmod m\\Rightarrow a^k\\equiv 1\\pmod m.\\]\n\n Exemplo 1 Considere novamente o GCM com \\(m=31\\) (primo) e \\(a=7\\), com \\(x_0=19\\). Como \\(m\\) e \\(a\\) são números primos, seu máximo divisor comum será 1. Portanto, o período deste gerador satisfaz\n\\[{7}^k\\equiv 1\\pmod {31}.\\]\nA função abaixo testa os inteiros \\(1,2\\ldots\\) até encontrar o período. Conforme o esperado,o período é igual a 15.\n\nparar &lt;- FALSE\nk &lt;- 1\nwhile( parar == FALSE){\n  teste = ( (7^k - 1)/31 ) \n  if( teste == ceiling(teste) ){\n    parar &lt;- TRUE\n  }else{\n    k &lt;- k+1\n  }\n}\n\nk\n\n[1] 15\n\n\n\nVimos até o momento que, se \\(m\\) é um número primo e o máximo divisor comum entre \\(m\\) e \\(x_0\\) é um, então, o período \\(k\\) do GCM é o menor inteiro positivo que satisfaz\n\\[a^k\\equiv 1\\pmod m.\\] Portanto, é importante entender em quais condições \\(k=m-1\\). Considere a seguinte definição.\n\nDefinition 4.1 Um inteiro \\(a\\) é uma raiz primitiva módulo \\(m\\) se todas as potências \\(a^1,a^2,\\ldots,a^{m-1}\\) módulo \\(m\\) geram o conjunto \\(1,\\ldots,m-1\\).\n\nNão há uma fórmula específica para encontrar raízes primitivas. Contudo, existem algoritmos para esse tipo de busca. No R a função primroot do pacote numbers encontra essas raízes por força bruta (ou seja, os inteiros entre 1 e \\(m-1\\) são testados um de cada vez). A vantagem das raízes primitivas é dada no seguinte teorema.\n\nTheorem 4.1 Considere um GCM onde \\(m\\) é um número primo e o máximo divisor comum entre \\(x_0\\) e \\(m\\) é 1. Se \\(a\\) é uma raiz primitiva módulo \\(m\\), então o período do GCM é \\(m-1\\).\n\n\nExemplo 2 Considere o Exemplo 1 novamente, onde \\(m=31\\) (primo) e \\(x_0=19\\) ( mdc(31,19)=1). Vamos encontrar as raízes primitivas módulo 31:\n\nnumbers::primroot(31, all = T)\n\n[1]  3 11 12 13 17 21 22 24\n\n\nNote que no Exemplo 1 utilizamos \\(a=7\\), que não é raíz primitiva de 31. Por isso o período encontrado foi menor que 30. Como há 8 raízes primitivas neste problema, qualquer uma delas pode, em princípio, ser utilizada como valor para \\(a\\). Abaixo ilustramos o gerador com \\(a=3\\).\n\n# criando um vetor semente igual a 19\nsemente &lt;- 19\nx &lt;- semente\n\n# criando o multiplicador e o módulo\na &lt;- 3\nm &lt;- 31\n\n# gerando os números pseudo-aleatórios\nfor(i in 2:(m) ){\n  y &lt;- a * x[i-1]\n  x[i] &lt;- y - y%/%m * m\n}\n\n# resultado\n(amostra &lt;- x[-1])\n\n [1] 26 16 17 20 29 25 13  8 24 10 30 28 22  4 12  5 15 14 11  2  6 18 23  7 21\n[26]  1  3  9 27 19\n\n# período\nwhich(amostra == semente)\n\n[1] 30\n\n\n\n\nExercício. Considerando o número primo 1021, crie um gerador congruencial multiplicativo para gerar uma amostra de tamanho 1000 da Uniforme(0,1). Em seguida:\n\nFaça o gráfico da distribuição empírica desta amostra (utilize o comando plot(ecdf(x)), onde x é a amostra gerada).\nFaça o gráfico da função de distribuição da Uniforme(0,1) em cima do gráfico anterior (utilize o comando curve(punif(x), add = T)).\nTeste \\(H_0: X\\sim\\hbox{Uniforme}(0,1)\\) com o comando ks.test(x,'punif')\nQual é a sua conclusão sobre o seu gerador?\n\n\nO exercício anterior mostra algumas estatísticas que são favoráveis ao gerador. Contudo, um bom gerador deve ter sucesso em uma grande bateria de testes para ser considerado adequado, mas este não é o escopo destas notas. O exercício abaixo ilustra um erro comum em um GMC.\n\nExemplo 3.(O padrão reticular) Considere a seguinte amostra, de tamanho 30, do GCM criado no Exemplo 2.\n\n(u &lt;- amostra / m)\n\n [1] 0.83870968 0.51612903 0.54838710 0.64516129 0.93548387 0.80645161\n [7] 0.41935484 0.25806452 0.77419355 0.32258065 0.96774194 0.90322581\n[13] 0.70967742 0.12903226 0.38709677 0.16129032 0.48387097 0.45161290\n[19] 0.35483871 0.06451613 0.19354839 0.58064516 0.74193548 0.22580645\n[25] 0.67741935 0.03225806 0.09677419 0.29032258 0.87096774 0.61290323\n\n\nNotemos que\n\nplot(ecdf(u))\ncurve(punif(x), add = T, col =2)\n\n\n\n\n\n\n\n\no que é consistente com uma Uniforme(0,1). O teste de Kolmogorov-Smirnov chega a mesma conclusão:\n\nks.test(u, 'punif')\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  u\nD = 0.032258, p-value = 1\nalternative hypothesis: two-sided\n\n\nVamos então mostrar o gráfico de dispersão de \\((u_i,u_{i-1})\\)\n\nplot( u[-1],u[-30], xlab = expression(u[i-1]) ,ylab=expression(u[i]) )\n\n\n\n\n\n\n\n\nEste tipo de padrão, denominado reticular, não deve aparecer em uma amostra aleatória de uma Uniforme(0,1).\n\n\n\n4.4 Geradores de Registradores de Deslocamento com Realimentação (GRDR)\nUm bit (contração de binary digit) a unidade básica da computação, podendo estar em dois estados: 0 ou 1. Ao agrupar bits, podemos representar diversas informações. Por exemplo, com 8 bits, teremos \\(2^8=256\\) combinações distintas, o que é suficiente para representar todos os caracteres de um teclado (letras, números, símbolos), um pixel de uma imagem em escala de cinza, ou um número de 0 a 255.\nA transformação de um número binário em um número na base 10 pela soma do resultado da multiplicação do bit pelo seu valor posicional. Por exemplo, um número com 4 bits possui os valores posicionais: \\(2^3, 2^2,2^1,2^0\\). Deste modo, o número 1100 em binário representa o número\n\\[(1\\times 2^3)+(1\\times 2^2)+(0\\times 2^1)+(0\\times 2^0)=8+4+0+0=12.\\]\nTausworthe (1965) introduziu um gerador baseado em uma sequências de zeros e uns gerados por recorrência da forma\n\\[b_i\\equiv (a_pb_{i-p}+a_{p-1}b_{i-p+1}+\\cdots+a_1b_{i-1})\\pmod 2\\] onde \\(b_i\\) são bits. Como \\(b_i\\in\\{0,1\\}\\) e 2 é primo, desde que a semente inicial não seja composta somente de zeros, é possível mostrar que o período deste gerador é dado por \\(2^{p}-1.\\) Por eficiência, a maioria dos coeficientes \\(a_j\\) do gerador acima são nulos. Consdire o caso particular\n\\[b_i\\equiv (b_{i-p}+b_{i-p+q})\\pmod 2,\\] com \\(0&lt;q&lt;p\\). Observe que:\n\nSe \\(b_{i-p}=0\\) e \\(b_{i-p+q}=0\\), então para que a divisão\n\n\\[\\frac{b_i-b_{i-p}-b_{i-p+q}}{2}=\\frac{b_i}{2}\\] seja inteira, é necessário que \\(b_i=0\\).\n\nSe \\(b_{i-p}=1\\) e \\(b_{i-p+q}=0\\), então para que a divisão\n\n\\[\\frac{b_i-b_{i-p}-b_{i-p+q}}{2}=\\frac{b_i-1}{2}\\] seja inteira, é necessário que \\(b_i=1\\). O mesmo vale para \\(b_{i-p}=0\\) e \\(b_{i-p+q}=1\\).\n\nSe \\(b_{i-p}=1\\) e \\(b_{i-p+q}=1\\), então para que a divisão\n\n\\[\\frac{b_i-b_{i-p}-b_{i-p+q}}{2}=\\frac{b_i-2}{2}\\] seja inteira, é necessário que \\(b_i=0\\).\nDeste modo, a operação \\(b_i\\equiv (b_{i-p}+b_{i-p+q})\\) é equivalente à operação lógica \\(\\oplus\\) (lê-se “ou exclusivo”), no qual \\(b_i=1\\) se \\(b_{i-p}\\neq b_{i-p+q}\\) e \\(b_i=0\\) em caso contrário. Deste modo, o gerador pode ser escrito como\n\\[b_i=b_{i-p}\\oplus b_{i-p+q}.\\]\nAo utilizar o gerador acima \\(l\\) vezes, (\\(l\\leq p\\)), podemos transformar os \\(l\\) bits gerados em um número na base 10. Se \\(l\\) é um primo relativo de \\(2^{p}-1\\), então o período dos grupos de tamanho \\(l\\) também será \\(2^p-1\\). O exemplo abaixo ilustra esse fato.\n\nExample 4.1 Considere o gerador \\[b_i=b_{i-5}\\oplus b_{i-3},\\] ou seja, \\(p=5\\) e \\(q=2\\). Portanto, o período do gerador é \\(2^{5}-1=31\\). Considere \\(l=5\\) (ou seja, um número de 5 bits). Isso implica que podemos gerar os números entre 1 e 31 antes começar a repetição. Considere a semente 10100. Abaixo geramos todas as 31-tuplas possíveis.\n\nb = c(1,0,1,0,0)\np=5 ;q=2\nfor(i in 6:150 ) b[i] = xor(b[i-p], b[i-p+q])\n\nbs = matrix(b,ncol=5,byrow=T)\nbs\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]    1    0    1    0    0\n [2,]    0    0    1    0    0\n [3,]    1    0    1    1    0\n [4,]    0    1    1    1    1\n [5,]    1    0    0    0    1\n [6,]    1    0    1    1    1\n [7,]    0    1    0    1    0\n [8,]    0    0    0    1    0\n [9,]    0    1    0    1    1\n[10,]    0    0    1    1    1\n[11,]    1    1    0    0    0\n[12,]    1    1    0    1    1\n[13,]    1    0    1    0    1\n[14,]    0    0    0    0    1\n[15,]    0    0    1    0    1\n[16,]    1    0    0    1    1\n[17,]    1    1    1    0    0\n[18,]    0    1    1    0    1\n[19,]    1    1    0    1    0\n[20,]    1    0    0    0    0\n[21,]    1    0    0    1    0\n[22,]    1    1    0    0    1\n[23,]    1    1    1    1    0\n[24,]    0    0    1    1    0\n[25,]    1    1    1    0    1\n[26,]    0    1    0    0    0\n[27,]    0    1    0    0    1\n[28,]    0    1    1    0    0\n[29,]    1    1    1    1    1\n[30,]    0    0    0    1    1\n\n\nVamos transformar esses números para a base 10:\n\nx = apply(bs, 1, function(x)  strtoi(paste0(x, collapse = \"\"), base = 2 ))\nx\n\n [1] 20  4 22 15 17 23 10  2 11  7 24 27 21  1  5 19 28 13 26 16 18 25 30  6 29\n[26]  8  9 12 31  3\n\n\nObserve que esse gerador também simula padrões indesejados, como o apresentado a seguir.\n\nplot(x[1:24],x[7:30])\n\n\n\n\n\n\n\n\n\nEm princípio, a vantagem deste gerador é computacional, uma vez que a operação \\(\\oplus\\) pode ser realizada diretamente em circuitos digitais. Além disso, toda a operação pode ser realizada com um registrador de deslocamento com alimentação, que estão entre as operações mais rápidas de um chip. Para entender esse conceito, considere um estado inicial de 5 bits:\n\\[[1,0,1,0,0]\\] e o gerador \\(b_i=b_{i-5}\\oplus b_{i-3}\\). A regra de realimentação será: o novo bit é o resultado do \\(\\oplus\\) entre o bit que sai (o primeiro da esquerda) e o \\(3^o\\) bit do registrador.\n\nPasso 1: Deslocamento para a esquerda: O bit 1 (primeiro da esquerda sai e o resto do registrador é deslocado, deixando um espaço vazio na direita. Esse é o estado após deslocamento: \\([0, 1, 0, 0, \\hbox{vazio}]\\)\nPasso 2: Operação de Realimentação (\\(\\oplus\\)): Pegamos o bit que saiu (1) e o \\(3^o\\) bit do estado original (1) e aplicamos o \\(\\oplus\\). Teremos \\(1\\oplus 1=0\\)\nPasso 3: Inserção do novo bit. O resultado da operação é inserido no espaço vazio à direita (nesse caso um 0). O novo estado do registrador: \\([0, 1, 0, 0, 0]\\)\n\n\n4.4.1 Twisting e o gerador Mersenne Twister\nPara evitar os padrões que surgem nos GRDR, Matsumoto and Kurita (1992, 1994) propuseram uma modificação, multiplicando \\(b_{i-p+q}\\) por uma matriz \\(A\\), de modo que\n\\[b_i=b_{i-p}\\oplus A b_{i-p+q}.\\] Tal gerador foi denominado twisted GSFR generator (algo como GRDR com torção).O objetivo de \\(A\\) é evitar que ocorram os padrões já discutidos, dando mais uniformidade para as amostras geradas. A matriz \\(A\\) é escolhida de modo que a operação \\(Ab_{i-p+q}\\) uma sequência de deslocamentos e \\(\\oplus\\), quebrando padrões possíveis correlações.\nEsse gerador é a base para o Mersenne Twister, desenvolvido em 1997 por Makoto Matsumoto e Takuji Nishimura. Seu nome é devido ao módulo utilizado, \\(2^{19937}-1\\), um conhecido número primo de Mersenne. Esse é o gerador utilizado pelo R por padrão, embora existam outros métodos de geração de números pseudo-aleatórios implementados (para mais detalhes, digite ?Random no console).\n\nExercício. Utilize a função runif(5000) para gerar 5000 pontos da distribuição Uniforme(0,1). Para essa amostra:\n\nCalcule a média e a variância amostral e compare com os valores esperados para a Uniforme(0,1).\nCompare a função de distribuição empírica com a função de distribuição Uniforme(0,1)\nFaça o teste Kolmogorov-Smirnov para testar se a amostra é proveniente da Uniforme(0,1).\nFaça o gráfico com os pares \\((X_1,X_2),(X_2,\\ldots,X_3),\\ldots,(X_{n-1}, X_n)\\).\n\n\n\n\n\n4.5 Exercícios\n\nExercício 3.\n\nImplemente um gerador congruencial multiplicativo com \\(m=2^{13}-1\\). Utilize esse gerador para simular uma amostra de tamanho 500 de uma Uniforme(0,1).\nCalcule a média e a variância da amostra e compare com os valores esperados para a distribuição Uniforme(0,1).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução aos geradores de números pseudo-aleatórios</span>"
    ]
  },
  {
    "objectID": "gerador_congruencial.html#geradores-lineares-congruenciais-simples",
    "href": "gerador_congruencial.html#geradores-lineares-congruenciais-simples",
    "title": "4  Introdução aos geradores de números pseudo-aleatórios",
    "section": "4.3 Geradores lineares congruenciais simples",
    "text": "4.3 Geradores lineares congruenciais simples\nLehmer (1951) propôs o gerador linear congruencial simples (GLCS), dado por\n\\[x_i\\equiv (ax_{i-1}+b)\\pmod m\\] onde \\(a\\) e \\(b\\) são inteiros denominados multiplicador e incremento. Escolhemos uma semente inicial \\(x_0\\), com \\(0\\leq x_0 &lt;m\\) (note que não faz sentido ter uma semente maior que \\(m\\), pois haverá outra menor que será congruente com esta). Como \\(x_i\\) é um resíduo, sabemos que \\(0\\leq x_i\\leq m-1\\), o que implica em \\(m\\) possibilidades de números. Portanto, esse gerador possui, no máximo, período \\(m\\).\nNo exemplo abaixo, o GLCS com semente inicial \\(x_0=7\\), \\(a=5\\) e \\(b=3\\) gerou a seguinte sequência de números.\n\na = 5\nm = 8\nb = 3\nx = 7\n\nfor(i in 2:(m+3)){\n  y &lt;- (a*x[i-1] + b)\n  x[i] &lt;- y - m * y%/% m \n}\nx\n\n [1] 7 6 1 0 3 2 5 4 7 6 1\n\n\nObserve que a semente inicial se repetiu na iteração 9, o que implica que esse gerador conseguiu simular todos os 8 números distintos possíveis.\nO GLCS tem dentre os seus resultados, a possibilidade de gerar um resíduo igual a 0. Embora isto não seja um problema para o gerador em si, essa característica é indesejável, pois nosso objetivo é obter \\[\\frac{r_i}{m}\\in(0,1)\\] logo, o gerador não pode simular o valor \\(r_i=0\\).\nAo fazer \\(b=0\\), temos o gerador congruencial multiplicativo (GCM). Desde que \\(0&lt;x_0&lt;m\\), o GCM não vai gerar o número 0 se \\(m\\) for primo, uma vez que \\(ax_{i-1}\\) nunca será múltiplo de \\(m\\). Deste modo, o GCM pode gerar os números no conjunto \\(\\{1,\\ldots,m-1\\}\\) tendo, portanto, período máximo igual a \\(m-1\\).\n\n Exemplo 1 Considere o GCM com \\(m=31\\) (primo) e \\(a=7\\), com \\(x_0=19\\). Abaixo mostramos os 30 valores gerados:\n\n# criando um vetor semente igual a 19\nsemente &lt;- 19\nx &lt;- semente\n\n# criando o multiplicador e o módulo\na &lt;- 7\nm &lt;- 31\n\n# gerando os números pseudo-aleatórios\nfor(i in 2:m){\n  y &lt;- a * x[i-1]\n  x[i] &lt;- y - y%/%m * m\n}\n\n# resultado\n(amostra &lt;- x[-1])\n\n [1]  9  1  7 18  2 14  5  4 28 10  8 25 20 16 19  9  1  7 18  2 14  5  4 28 10\n[26]  8 25 20 16 19\n\n# período\nwhich(amostra == semente)\n\n[1] 15 30\n\n\nEmbora o período máximo seja 30, esse gerador tem período 15.\n\nO exemplo acima mostra que ter \\(m\\) primo não é suficiente para obter o período máximo. Vamos avaliar melhor essa questão. Note que \\(x_k \\equiv ax_{k-1}\\pmod m\\) implica que existe \\(c\\) inteiro tal que \\((x_k-ax_{k-1})/m = c\\). Disto, temos que:\n\\[\\begin{align}c&= \\frac{x_k - ax_{k-1}}{m}=\\frac{x_k \\pm a^2 x_{k-2}- ax_{k-1}}{m}=\\frac{x_{k}-a^2 x_{k-2} -a(x_{k-1}-a x_{k-2})}{m}\\\\ &=\\frac{x_k-a^2 x_{k-2}}{m}+a\\frac{x_{k-1}-ax_{k-2}}{m}\\end{align}\\]\ne, como \\(x_{k-1}\\) e \\(ax_{k-2}\\) são congruentes módulo \\(m\\), existe um inteiro \\(c'\\) tal que\n\\[\\begin{align}c&= \\frac{x_k-a^2 x_{k-2}}{m}+ak_2\\Rightarrow \\frac{x_k-a^2 x_{k-2}}{m}=c-ac'\\end{align}\\] e, como \\(c-ac'\\) é inteiro, temos que \\[x_k\\equiv a^2 x_{k-2} \\pmod m.\\] Podemos fazer uma indução e mostrar que $x_k a^k x_0m $. Em geral, os geradores de números pseudo-aleatórios corretamente implementados devem começar a se repetir na semenente inicial. Deste modo, um gerador com período \\(k\\) deve ter \\(x_k=x_0\\), o que implica em\n\\[x_0\\equiv a^kx_0\\pmod m \\Rightarrow a^kx_0=x_0\\pmod m.\\] Disto, podemos enunciar a seguinte proposição.\n\nPara um GCM, se \\(x_0\\) e \\(m\\) são primos entre si, então o menor valor de \\(k\\) que satisfaz \\[a^k\\equiv 1\\pmod m\\] é o periodo do GCM.\n\nA demonstração da proposição se dá pela existência de \\(x_0^{-1}\\), garantida pelo Teorema do Inverso Modular. \\[a^k x_0 x_0^{-1 }\\equiv x_0 x_0^{-1}\\pmod m\\Rightarrow a^k\\equiv 1\\pmod m.\\]\n\n Exemplo 1 Considere novamente o GCM com \\(m=31\\) (primo) e \\(a=7\\), com \\(x_0=19\\). Como \\(m\\) e \\(a\\) são números primos, seu máximo divisor comum será 1. Portanto, o período deste gerador satisfaz\n\\[{7}^k\\equiv 1\\pmod {31}.\\]\nA função abaixo testa os inteiros \\(1,2\\ldots\\) até encontrar o período. Conforme o esperado,o período é igual a 15.\n\nparar &lt;- FALSE\nk &lt;- 1\nwhile( parar == FALSE){\n  teste = ( (7^k - 1)/31 ) \n  if( teste == ceiling(teste) ){\n    parar &lt;- TRUE\n  }else{\n    k &lt;- k+1\n  }\n}\n\nk\n\n[1] 15\n\n\n\nVimos até o momento que, se \\(m\\) é um número primo e o máximo divisor comum entre \\(m\\) e \\(x_0\\) é um, então, o período \\(k\\) do GCM é o menor inteiro positivo que satisfaz\n\\[a^k\\equiv 1\\pmod m.\\] Portanto, é importante entender em quais condições \\(k=m-1\\). Considere a seguinte definição.\n\nDefinition 4.1 Um inteiro \\(a\\) é uma raiz primitiva módulo \\(m\\) se todas as potências \\(a^1,a^2,\\ldots,a^{m-1}\\) módulo \\(m\\) geram o conjunto \\(1,\\ldots,m-1\\).\n\nNão há uma fórmula específica para encontrar raízes primitivas. Contudo, existem algoritmos para esse tipo de busca. No R a função primroot do pacote numbers encontra essas raízes por força bruta (ou seja, os inteiros entre 1 e \\(m-1\\) são testados um de cada vez). A vantagem das raízes primitivas é dada no seguinte teorema.\n\nTheorem 4.1 Considere um GCM onde \\(m\\) é um número primo e o máximo divisor comum entre \\(x_0\\) e \\(m\\) é 1. Se \\(a\\) é uma raiz primitiva módulo \\(m\\), então o período do GCM é \\(m-1\\).\n\n\nExemplo 2 Considere o Exemplo 1 novamente, onde \\(m=31\\) (primo) e \\(x_0=19\\) ( mdc(31,19)=1). Vamos encontrar as raízes primitivas módulo 31:\n\nnumbers::primroot(31, all = T)\n\n[1]  3 11 12 13 17 21 22 24\n\n\nNote que no Exemplo 1 utilizamos \\(a=7\\), que não é raíz primitiva de 31. Por isso o período encontrado foi menor que 30. Como há 8 raízes primitivas neste problema, qualquer uma delas pode, em princípio, ser utilizada como valor para \\(a\\). Abaixo ilustramos o gerador com \\(a=3\\).\n\n# criando um vetor semente igual a 19\nsemente &lt;- 19\nx &lt;- semente\n\n# criando o multiplicador e o módulo\na &lt;- 3\nm &lt;- 31\n\n# gerando os números pseudo-aleatórios\nfor(i in 2:(m) ){\n  y &lt;- a * x[i-1]\n  x[i] &lt;- y - y%/%m * m\n}\n\n# resultado\n(amostra &lt;- x[-1])\n\n [1] 26 16 17 20 29 25 13  8 24 10 30 28 22  4 12  5 15 14 11  2  6 18 23  7 21\n[26]  1  3  9 27 19\n\n# período\nwhich(amostra == semente)\n\n[1] 30\n\n\n\n\nExercício. Considerando o número primo 1021, crie um gerador congruencial multiplicativo para gerar uma amostra de tamanho 1000 da Uniforme(0,1). Em seguida:\n\nFaça o gráfico da distribuição empírica desta amostra (utilize o comando plot(ecdf(x)), onde x é a amostra gerada).\nFaça o gráfico da função de distribuição da Uniforme(0,1) em cima do gráfico anterior (utilize o comando curve(punif(x), add = T)).\nTeste \\(H_0: X\\sim\\hbox{Uniforme}(0,1)\\) com o comando ks.test(x,'punif')\nQual é a sua conclusão sobre o seu gerador?\n\n\nO exercício anterior mostra algumas estatísticas que são favoráveis ao gerador. Contudo, um bom gerador deve ter sucesso em uma grande bateria de testes para ser considerado adequado, mas este não é o escopo destas notas. O exercício abaixo ilustra um erro comum em um GMC.\n\nExemplo 3.(O padrão reticular) Considere a seguinte amostra, de tamanho 30, do GCM criado no Exemplo 2.\n\n(u &lt;- amostra / m)\n\n [1] 0.83870968 0.51612903 0.54838710 0.64516129 0.93548387 0.80645161\n [7] 0.41935484 0.25806452 0.77419355 0.32258065 0.96774194 0.90322581\n[13] 0.70967742 0.12903226 0.38709677 0.16129032 0.48387097 0.45161290\n[19] 0.35483871 0.06451613 0.19354839 0.58064516 0.74193548 0.22580645\n[25] 0.67741935 0.03225806 0.09677419 0.29032258 0.87096774 0.61290323\n\n\nNotemos que\n\nplot(ecdf(u))\ncurve(punif(x), add = T, col =2)\n\n\n\n\n\n\n\n\no que é consistente com uma Uniforme(0,1). O teste de Kolmogorov-Smirnov chega a mesma conclusão:\n\nks.test(u, 'punif')\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  u\nD = 0.032258, p-value = 1\nalternative hypothesis: two-sided\n\n\nVamos então mostrar o gráfico de dispersão de \\((u_i,u_{i-1})\\)\n\nplot( u[-1],u[-30], xlab = expression(u[i-1]) ,ylab=expression(u[i]) )\n\n\n\n\n\n\n\n\nEste tipo de padrão, denominado reticular, não deve aparecer em uma amostra aleatória de uma Uniforme(0,1).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução aos geradores de números pseudo-aleatórios</span>"
    ]
  },
  {
    "objectID": "gerador_congruencial.html#geradores-de-registradores-de-deslocamento-com-realimentação-grdr",
    "href": "gerador_congruencial.html#geradores-de-registradores-de-deslocamento-com-realimentação-grdr",
    "title": "4  Introdução aos geradores de números pseudo-aleatórios",
    "section": "4.4 Geradores de Registradores de Deslocamento com Realimentação (GRDR)",
    "text": "4.4 Geradores de Registradores de Deslocamento com Realimentação (GRDR)\nUm bit (contração de binary digit) a unidade básica da computação, podendo estar em dois estados: 0 ou 1. Ao agrupar bits, podemos representar diversas informações. Por exemplo, com 8 bits, teremos \\(2^8=256\\) combinações distintas, o que é suficiente para representar todos os caracteres de um teclado (letras, números, símbolos), um pixel de uma imagem em escala de cinza, ou um número de 0 a 255.\nA transformação de um número binário em um número na base 10 pela soma do resultado da multiplicação do bit pelo seu valor posicional. Por exemplo, um número com 4 bits possui os valores posicionais: \\(2^3, 2^2,2^1,2^0\\). Deste modo, o número 1100 em binário representa o número\n\\[(1\\times 2^3)+(1\\times 2^2)+(0\\times 2^1)+(0\\times 2^0)=8+4+0+0=12.\\]\nTausworthe (1965) introduziu um gerador baseado em uma sequências de zeros e uns gerados por recorrência da forma\n\\[b_i\\equiv (a_pb_{i-p}+a_{p-1}b_{i-p+1}+\\cdots+a_1b_{i-1})\\pmod 2\\] onde \\(b_i\\) são bits. Como \\(b_i\\in\\{0,1\\}\\) e 2 é primo, desde que a semente inicial não seja composta somente de zeros, é possível mostrar que o período deste gerador é dado por \\(2^{p}-1.\\) Por eficiência, a maioria dos coeficientes \\(a_j\\) do gerador acima são nulos. Consdire o caso particular\n\\[b_i\\equiv (b_{i-p}+b_{i-p+q})\\pmod 2,\\] com \\(0&lt;q&lt;p\\). Observe que:\n\nSe \\(b_{i-p}=0\\) e \\(b_{i-p+q}=0\\), então para que a divisão\n\n\\[\\frac{b_i-b_{i-p}-b_{i-p+q}}{2}=\\frac{b_i}{2}\\] seja inteira, é necessário que \\(b_i=0\\).\n\nSe \\(b_{i-p}=1\\) e \\(b_{i-p+q}=0\\), então para que a divisão\n\n\\[\\frac{b_i-b_{i-p}-b_{i-p+q}}{2}=\\frac{b_i-1}{2}\\] seja inteira, é necessário que \\(b_i=1\\). O mesmo vale para \\(b_{i-p}=0\\) e \\(b_{i-p+q}=1\\).\n\nSe \\(b_{i-p}=1\\) e \\(b_{i-p+q}=1\\), então para que a divisão\n\n\\[\\frac{b_i-b_{i-p}-b_{i-p+q}}{2}=\\frac{b_i-2}{2}\\] seja inteira, é necessário que \\(b_i=0\\).\nDeste modo, a operação \\(b_i\\equiv (b_{i-p}+b_{i-p+q})\\) é equivalente à operação lógica \\(\\oplus\\) (lê-se “ou exclusivo”), no qual \\(b_i=1\\) se \\(b_{i-p}\\neq b_{i-p+q}\\) e \\(b_i=0\\) em caso contrário. Deste modo, o gerador pode ser escrito como\n\\[b_i=b_{i-p}\\oplus b_{i-p+q}.\\]\nAo utilizar o gerador acima \\(l\\) vezes, (\\(l\\leq p\\)), podemos transformar os \\(l\\) bits gerados em um número na base 10. Se \\(l\\) é um primo relativo de \\(2^{p}-1\\), então o período dos grupos de tamanho \\(l\\) também será \\(2^p-1\\). O exemplo abaixo ilustra esse fato.\n\nExample 4.1 Considere o gerador \\[b_i=b_{i-5}\\oplus b_{i-3},\\] ou seja, \\(p=5\\) e \\(q=2\\). Portanto, o período do gerador é \\(2^{5}-1=31\\). Considere \\(l=5\\) (ou seja, um número de 5 bits). Isso implica que podemos gerar os números entre 1 e 31 antes começar a repetição. Considere a semente 10100. Abaixo geramos todas as 31-tuplas possíveis.\n\nb = c(1,0,1,0,0)\np=5 ;q=2\nfor(i in 6:150 ) b[i] = xor(b[i-p], b[i-p+q])\n\nbs = matrix(b,ncol=5,byrow=T)\nbs\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]    1    0    1    0    0\n [2,]    0    0    1    0    0\n [3,]    1    0    1    1    0\n [4,]    0    1    1    1    1\n [5,]    1    0    0    0    1\n [6,]    1    0    1    1    1\n [7,]    0    1    0    1    0\n [8,]    0    0    0    1    0\n [9,]    0    1    0    1    1\n[10,]    0    0    1    1    1\n[11,]    1    1    0    0    0\n[12,]    1    1    0    1    1\n[13,]    1    0    1    0    1\n[14,]    0    0    0    0    1\n[15,]    0    0    1    0    1\n[16,]    1    0    0    1    1\n[17,]    1    1    1    0    0\n[18,]    0    1    1    0    1\n[19,]    1    1    0    1    0\n[20,]    1    0    0    0    0\n[21,]    1    0    0    1    0\n[22,]    1    1    0    0    1\n[23,]    1    1    1    1    0\n[24,]    0    0    1    1    0\n[25,]    1    1    1    0    1\n[26,]    0    1    0    0    0\n[27,]    0    1    0    0    1\n[28,]    0    1    1    0    0\n[29,]    1    1    1    1    1\n[30,]    0    0    0    1    1\n\n\nVamos transformar esses números para a base 10:\n\nx = apply(bs, 1, function(x)  strtoi(paste0(x, collapse = \"\"), base = 2 ))\nx\n\n [1] 20  4 22 15 17 23 10  2 11  7 24 27 21  1  5 19 28 13 26 16 18 25 30  6 29\n[26]  8  9 12 31  3\n\n\nObserve que esse gerador também simula padrões indesejados, como o apresentado a seguir.\n\nplot(x[1:24],x[7:30])\n\n\n\n\n\n\n\n\n\nEm princípio, a vantagem deste gerador é computacional, uma vez que a operação \\(\\oplus\\) pode ser realizada diretamente em circuitos digitais. Além disso, toda a operação pode ser realizada com um registrador de deslocamento com alimentação, que estão entre as operações mais rápidas de um chip. Para entender esse conceito, considere um estado inicial de 5 bits:\n\\[[1,0,1,0,0]\\] e o gerador \\(b_i=b_{i-5}\\oplus b_{i-3}\\). A regra de realimentação será: o novo bit é o resultado do \\(\\oplus\\) entre o bit que sai (o primeiro da esquerda) e o \\(3^o\\) bit do registrador.\n\nPasso 1: Deslocamento para a esquerda: O bit 1 (primeiro da esquerda sai e o resto do registrador é deslocado, deixando um espaço vazio na direita. Esse é o estado após deslocamento: \\([0, 1, 0, 0, \\hbox{vazio}]\\)\nPasso 2: Operação de Realimentação (\\(\\oplus\\)): Pegamos o bit que saiu (1) e o \\(3^o\\) bit do estado original (1) e aplicamos o \\(\\oplus\\). Teremos \\(1\\oplus 1=0\\)\nPasso 3: Inserção do novo bit. O resultado da operação é inserido no espaço vazio à direita (nesse caso um 0). O novo estado do registrador: \\([0, 1, 0, 0, 0]\\)\n\n\n4.4.1 Twisting e o gerador Mersenne Twister\nPara evitar os padrões que surgem nos GRDR, Matsumoto and Kurita (1992, 1994) propuseram uma modificação, multiplicando \\(b_{i-p+q}\\) por uma matriz \\(A\\), de modo que\n\\[b_i=b_{i-p}\\oplus A b_{i-p+q}.\\] Tal gerador foi denominado twisted GSFR generator (algo como GRDR com torção).O objetivo de \\(A\\) é evitar que ocorram os padrões já discutidos, dando mais uniformidade para as amostras geradas. A matriz \\(A\\) é escolhida de modo que a operação \\(Ab_{i-p+q}\\) uma sequência de deslocamentos e \\(\\oplus\\), quebrando padrões possíveis correlações.\nEsse gerador é a base para o Mersenne Twister, desenvolvido em 1997 por Makoto Matsumoto e Takuji Nishimura. Seu nome é devido ao módulo utilizado, \\(2^{19937}-1\\), um conhecido número primo de Mersenne. Esse é o gerador utilizado pelo R por padrão, embora existam outros métodos de geração de números pseudo-aleatórios implementados (para mais detalhes, digite ?Random no console).\n\nExercício. Utilize a função runif(5000) para gerar 5000 pontos da distribuição Uniforme(0,1). Para essa amostra:\n\nCalcule a média e a variância amostral e compare com os valores esperados para a Uniforme(0,1).\nCompare a função de distribuição empírica com a função de distribuição Uniforme(0,1)\nFaça o teste Kolmogorov-Smirnov para testar se a amostra é proveniente da Uniforme(0,1).\nFaça o gráfico com os pares \\((X_1,X_2),(X_2,\\ldots,X_3),\\ldots,(X_{n-1}, X_n)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução aos geradores de números pseudo-aleatórios</span>"
    ]
  },
  {
    "objectID": "gerador_congruencial.html#exercícios",
    "href": "gerador_congruencial.html#exercícios",
    "title": "4  Introdução aos geradores de números pseudo-aleatórios",
    "section": "4.5 Exercícios",
    "text": "4.5 Exercícios\n\nExercício 3.\n\nImplemente um gerador congruencial multiplicativo com \\(m=2^{13}-1\\). Utilize esse gerador para simular uma amostra de tamanho 500 de uma Uniforme(0,1).\nCalcule a média e a variância da amostra e compare com os valores esperados para a distribuição Uniforme(0,1).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução aos geradores de números pseudo-aleatórios</span>"
    ]
  },
  {
    "objectID": "gerador_congruencial.html#referências",
    "href": "gerador_congruencial.html#referências",
    "title": "4  Introdução aos geradores de números pseudo-aleatórios",
    "section": "4.6 Referências",
    "text": "4.6 Referências\nTábuas de números aleatórios\n\n\nTippet (1927). “Random number tables” tracts of computer, No.-15, Cambridge University press.\nFisher, R. A., and F. Yates. “Statistical tables for biological, agricultural and medical research.” Statistical tables for biological, agricultural and medical research. 2nd rev. ed (1943).\n\n\nGerador congruencial simples\n\nLehmer, D. H. (1951), Mathematical methods in large-scale computing units, Proceedings of the Second Symposium on Large Scale Digital Computing Machinery, Harvard University Press, Cambridge, Massachusetts. 141–146.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução aos geradores de números pseudo-aleatórios</span>"
    ]
  },
  {
    "objectID": "inversao.html",
    "href": "inversao.html",
    "title": "5  Método da inversão",
    "section": "",
    "text": "5.1 Leitura",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Método da inversão</span>"
    ]
  },
  {
    "objectID": "inversao.html#leitura",
    "href": "inversao.html#leitura",
    "title": "5  Método da inversão",
    "section": "",
    "text": "Leitura\n\nRandom number generation and Monte Carlo Methods: seção 4.1\nIntroducing Monte Carlo Methods with R: capítulo 2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Método da inversão</span>"
    ]
  },
  {
    "objectID": "inversao.html#funções-de-uniformes",
    "href": "inversao.html#funções-de-uniformes",
    "title": "5  Método da inversão",
    "section": "5.2 1. Funções de uniformes",
    "text": "5.2 1. Funções de uniformes\nAnteriormente, vimos que é possível gerar números que se assemelham a uma amostra de variáveis aleatórias independentes com distribuição Uniforme(0,1). Em particular, utilizamos a função runif() do R para este fim.\nNote que isto implica que podemos simular qualquer distribuição que seja função de variáveis aleatórias uniformes. O exemplo abaixo mostra como gerar amostras da distribuição Triangular(0,1,2)\n\nExemplo. Seja \\(X\\sim\\hbox{Triangular}(0,1,2)\\), cuja função densidade é dada por \\[f(x)=1-|x-1|,\\] com \\(x\\in(0,2)\\). Sejam \\(U_1\\) e \\(U_2\\) variáveis aleatórias independentes com distribuição Uniforme(0,1). Pode-se mostrar que \\(X=U_1+U_2\\hbox{Triangular}(0,1,2)\\). Portanto, podemos simular da distribuição de \\(X\\) com o seguinte algoritmo:\n\nGere \\(u_1,u_2\\sim\\hbox{Uniforme}(0,1)\\) independentes.\nFaça \\(x=u_1+u_2\\)\n\nAbaixo, simulamos uma amostra tamanho 5000 de \\(X\\sim\\hbox{Triangular}(0,1,2)\\):\n\nn = 50000\nu1 = runif(n)\nu2 = runif(n)\nx = u1 + u2\n\nhist(x,freq = FALSE, main = '')\n\n\n\n\n\n\n\n\n\n\nExercício. A função densidade da distribuição de Bates é dada por \\[f(x)=\\frac{n}{2(n-1)!}\\sum_{k=0}^{n}(-1)^k{n\\choose k }(nx-k)^{n-1}\\hbox{sgn}(nx-k),\\] onde \\(n\\geq 1\\) é inteiro e \\(\\hbox{sgn}(a)\\) é o sinal de \\(a\\) (+1 se \\(a\\) é positivo e \\(-1\\) se \\(a\\) é negativo). A distribuição de Bates tem aplicação em formação de feixe e síntese de diagrama de irradiação no campo da engenharia elétrica.\nContudo, pode-se mostrar que\n\\[X=\\frac{1}{n}\\sum_{i=1}^n U_i,\\] onde \\(U_1,\\ldots,U_n\\) são variáveis independentes com distribuição Uniforme(0,1). Deste modo, para \\(n\\) suficientemente grande, o Teorema Central do Limite garante que\n\\[X\\approx N\\left(\\frac{1}{2}, \\frac{1}{12n} \\right).\\] Deste modo, a distribuição Bates também pode ser utilizada como aproximação para a distribuição normal. Vamos explorar esse útlimo aspecto com mais detalhes.\nComo gerar uniformes e fazer operções básicas como somas e divisões tem custo computacional baixo, é usual simular da distribuição Bates, com o objetivo de obter amostras de uma normal, em problemas que não exigem muita precisão como simulação de normais em de jogos eletrônicos. Fazendo\n\\[Y=\\sqrt{12n}\\left(X-\\frac{1}{2}\\right),\\] teremos que \\(Y\\approx N(0,1)\\). Para manter o custo computacional ainda baixo, é usual escolher \\(n=12\\), o que evita realizar o cálculo da raiz quadrada. O seguinte algoritmo simula uma variável com distribuição (aproximada) normal padrão:\n\nSimule uma amostra aleatória \\(u_1,\\ldots,u_{12}\\) com distribuição Uniforme(0,1)\nCalcule \\(x=\\sum_{i=1}^{12} x_i/12\\)\nRetorne o valor de \\(y= 12(x-1/2)\\)\n\nConsiderando o algoritmo acima:\n\ngere uma amostra de tamanho 5000 da distribuição (aproximada) normal padrão.\ncompare o gráfico função de distribuição empírica (ecdf) da amostra simulada com a função de distribuição da normal padrão (pnorm).\n\n\nUm exemplo mais elaborado, para gerar amostras da distribuição normal padrão a partir de uniformes, é dado abaixo.\n\nAlgoritmo Box-Muller - Geração de normais padrão \n\nGere \\(u_1,u_2\\sim\\hbox{Uniforme(0,1)}\\) independentes\nFaça \\(x_1= \\sqrt{-2 \\log(u_1)}\\cos(2\\pi u_2)\\) e \\(x_2 =\\sqrt{-2\\log(u_1)}\\sin(2\\pi u_2)\\)\n\n\n\nExercício Simule uma amostra de tamanho 5000 da distribuição Normal(0,1) utilizado o algoritmo de Box-Muller.\n\nO algoritmo de Box-Muller foi um dos primeiros geradores de normais e sua aplicação mostra a fragilidade dos primeiros geradores congruenciais, como ilustra o exemplo abaixo.\n\nExample 5.1 O gráfico de dispersão de duas amostras indepedentes de normais padrão deve ter um comportamento de pontos ao acaso dentro de uma circunferência, como mostrado abaixo:\n\nplot(rnorm(5000),rnorm(5000), asp  =1)\n\n\n\n\n\n\n\n\nConsidere então Gerador Congruencial Linear conhecido como RANDOM:\n\nlcg_random &lt;- function(n, seed = 123, a = 106, c = 1283, m = 6075) {\n  # n: número de pontos a serem gerados\n  # seed: semente inicial\n  # a, c, m: parâmetros do LCG: X_n+1 = (a * X_n + c) mod m\n  \n  resultados &lt;- numeric(n)\n  X &lt;- seed\n  \n  for (i in 1:n) {\n    X &lt;- (a * X + c) %% m\n    resultados[i] &lt;- X / m # Normaliza para o intervalo [0, 1]\n  }\n  \n  return(resultados)\n}\n\nAo gerar duas amostras independentes de normais padrão pelo algoritmo de Box-Muller, utilizando para as uniformes do gerador RANDOM, obtemos o seguinte gráfico de dispersão, que não possui o comportamento esperado:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Método da inversão</span>"
    ]
  },
  {
    "objectID": "inversao.html#teorema-da-probabilidade-integral",
    "href": "inversao.html#teorema-da-probabilidade-integral",
    "title": "5  Método da inversão",
    "section": "5.3 2. Teorema da Probabilidade Integral",
    "text": "5.3 2. Teorema da Probabilidade Integral\nO teorema abaixo é a chave para simular outras distribuições.\n\nTeorema da Probabilidade Integral. \nSe \\(U=F_X(X)\\), então \\(U\\sim\\)Uniforme(0,1).\n\n\nProva Defina \\[F_X^{-1}(u)=\\min\\{x:u\\leq F_X(x)\\}.\\] Então, \\[F_U(u)=P(U\\leq u)=P(F_X(X)\\leq u)=P(X\\leq F^{-1}_X(u))=F_X(F^{-1}_X(u))=u.\\]\n\nA partir do Teorema da Probabilidade Integral, podemos estabelecer a relação \\(X=F_X^{-1}(U)\\). Desde modo, podemos gerar amostras da distribuição de \\(X\\) do seguinte modo:\n\n Algoritmo do Método da Inversão\n\nGere \\(u\\sim \\hbox{Unirforme}(0,1)\\)\nFaça \\(x=F_X^{-1}(u)\\)\n\n\n\nExemplo 2.1. Geração de exponenciais  Se \\(X\\sim\\hbox{Exponencial}(\\lambda)\\), então \\[F_X(x)=1-e^{-\\lambda x}\\] e \\[F^{-1}_X(u)=-\\frac{1}{\\lambda}\\log(1-u).\\]\nAssim, podemos gerar uma amostra de \\(n\\) variáveis aleatórias independentes com distibuição Exponencial com o seguinte algoritmo:\n\nGere \\(u_1,\\ldots,u_n\\) independentes com distribuição Uniforme(0,1)\nRetorne \\(x_i=-\\frac{1}{\\lambda}\\log(1-u_i)\\)\n\nÉ importante notar que \\(1-u_i\\) pode ser trocado por \\(u_i\\) sem prejuízos, uma vez que \\(1-U_i\\sim\\hbox{Uniforme}(0,1)\\).\n\n\nExercício.\n\nGere uma amostra de tamanho 5000 do modelo Exponencial(3) utilizando o método da inversão.\nFaça o gráfico da função de distribuição empírica (ecdf) e compare com a gráfico da função de distribuição da Exponencial(3).\n\n\n\nExercício. A distribuição Weibull é muito útil para análise de sobrevida e para valores extremos. Sua função distribuição é dada por\n\\[F(x)=1-e^{-(x/\\lambda)^\\kappa},\\] onde \\(x,\\lambda,\\kappa&gt;0\\). Construa um gerador de números aleatórios para essa distribuição utilizando o Método da Inversão.\n\n\nExemplo 2.2. Geração de Bernoullis.  Se \\(X\\sim\\hbox{Bernoulli}(\\theta)\\) então,\n\\[F_X(x)=\\left\\{\\begin{array}{ll}0&,x&lt;0\\\\ 1-\\theta&,0\\leq x &lt; 1\\\\\n1&,x\\geq 1\\end{array}\\right.\\] então,\n\\[F_X^{-1}(u)=\\left\\{\\begin{array}{ll}0&,0&lt;u\\leq 1-\\theta\\\\ 1&,1-\\theta&lt;u&lt;1\\end{array}\\right.\\] Portanto, podemos gerar uma amostra de \\(n\\) variáveis independentes com distribuição Bernoulli(\\(\\theta\\)) utilizando o seguinte algoritmo:\n\nGere \\(u_1,\\ldots,u_n\\) independentes com distribuição Uniforme\\((0,1)\\)\nSe \\(u_i&lt;1-\\theta\\), faça \\(x_i=0\\). Se não, faça \\(x_i=1\\).\n\n\nAcima, mostramos como gerar uma amostra de tamanho \\(n\\) da distribuição Bernoulli(\\(\\theta\\)). A soma destes valores é uma amostra de tamanho 1 de uma distribuição Binomial(\\(n,\\theta\\)) e é imediata a implementação para uma amostra de tamanho qualquer. Na verdade, simular binomiais através da soma de Bernoullis não é uma estratégia ótima (veja Kachitvichyanukul & Schmeiser (1988)), mas é um exemplo de como podemos utilizar a distribuição uniforme para gerar uma infinidade de outras distribuições.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Método da inversão</span>"
    ]
  },
  {
    "objectID": "inversao.html#pesquisas-em-tabelas",
    "href": "inversao.html#pesquisas-em-tabelas",
    "title": "5  Método da inversão",
    "section": "5.4 3. Pesquisas em tabelas",
    "text": "5.4 3. Pesquisas em tabelas\nSeja \\(X\\) uma variável aleatória discreta. Seja \\(p_j=P(X=j)\\). O Método da Inversão consiste:\n\nSimular \\(U\\sim\\hbox{Uniforme}\\)\nEncontrar \\(j\\) que satisfaz o critério de seleção\n\n\\[\\sum_{i=1}^{j-1}p_j&lt; U \\leq \\sum_{i=1}^jp_j.\\]\nSe o suporte de \\(X\\) é finito, então o método da inversão é denominado Table Lookup (pesquisa em tabela). Nesse caso, uma tabela é construída com duas colunas, sendo que uma possui os possíveis valores de \\(X\\) e a outra o critério que permite dizer que o ponto \\(j\\) foi simulado a partir de \\(U\\).\nOs principais algoritmos de pesquisa em tabela são\n\nBusca Sequencial (Build-up search).: a tabela é construída de modo iterativo em tempo real até obter o critério de seleção.\nBusca Binária (Chop-down search).: utiliza uma tabela já pronta e uma busca semelhante ao método da bisseção até obter o critério de seleção.\n\nSem perda de generalidade, considere que \\(j\\in\\{1,\\ldots,k\\}\\). O método de Busca Sequencial começa verificando se \\(U\\leq p_1\\). Em caso afirmativo, o valor 1 é gerado. Senão, é fato que \\(U&gt;p_1\\) e verifica-se se \\(U\\leq p_1+p_2\\). Caso \\(U\\leq p_1+p_2\\), então é fato que \\(p_1&lt;U\\leq p_1+p_2\\) e o valor 2 é gerado. Caso contrário, continuamos acumulando as probabilidades até encontrar o valor \\(j\\) que satisfaz o critério de seleção.\n\nAlgoritmo Busca Sequencial. \nGere \\(u\\sim\\hbox{Uniforme}(0,1)\\). Faça \\(j=1\\) e \\(s=p_1\\)\n\nSe \\(u\\leq s\\), retorne \\(x=j\\). Senão, faça \\(j=j+1\\), \\(s=s+p_{j}\\) e repita o passo 1.\n\n\n\nExample 5.2 Considere a função de distribuição\n\\[P(X=x)=\\frac{x}{10},\\] onde \\(x\\in\\{1,2,3,4\\}\\). A tabela de busca é\n\\[\\begin{array}{c|c|c}\\hline x & F(x) & \\hbox{Intervalo de busca}\\\\ \\hline 1  & \\frac{1}{10} & (0,\\frac{1}{10}] \\\\  2& \\frac{3}{10} & (\\frac{1}{10}, \\frac{3}{10}]\\\\ 3 & \\frac{6}{10} & (\\frac{3}{10},\\frac{6}{10}] \\\\ 4 & 1 & (\\frac{6}{10},1]  \\\\ \\hline  \\end{array}\\]\nObserve que o método de Busca Sequencial não precisa construir a tabela inteira para verificar se \\(u\\) está no intervalo de busca. Por exemplo, se \\(u= 2/10\\), o método começa com \\(j=1\\), \\(s=p_1=1/10\\) e faz as seguintes iterações:\n\nIteração 1. Como \\(u\\nleq s=1/10\\), não geramos um número. Temos que \\(j=j+1=2\\) e \\(s=s+p_2=3/10\\)\nIteração 2. Como \\(u=2/10&lt;3/10\\), o valor \\(j=2\\) é gerado.\n\n\n\nExercício. Considere um jogo eletrônico, no qual o jogador recebe um prêmio após derrotar um monstro. Existem 5 prêmios possíveis e a probabilidade do jogador receber cada um dos itens é dada abaixo.\n\\[\\begin{array}{c|l|c}\\hline\n\\hbox{Índice}& \\hbox{Item} & \\hbox{Probabilidade}\\\\ \\hline\n1&\\hbox{500 Moedas de Ouro} &0,50\\\\\n2&\\hbox{Gema Rara}  &0,25\\\\\n3&\\hbox{Poção de Cura}  &0,15\\\\\n4&\\hbox{Espada Mágica}  &0,08 \\\\\n5&\\hbox{Pergaminho Lendário}    &0,02\\\\ \\hline\n\\end{array}\\]\nConstrua um algoritmo para simular os itens acima, considerando a busca sequencial.\n\nComo a busca sequencial não precisa construir a tabela toda, ela pode ser utilizada para variáveis discretas com suporte infinito.\n\nExercício Dizemos que \\(X\\sim\\hbox{Borel}(\\alpha)\\) se sua função de probabilidade é dada por \\[P(X=x)=\\frac{e^{-\\alpha x}(\\alpha x)^{x-1}}{x!},\\] com \\(\\alpha\\in(0,1)\\) e \\(x=1,2,\\ldots,\\). Contrua um gerador para simular uma amostra de variáveis aleatórias independentes de tamanho \\(n\\) para um \\(\\alpha\\) fixado utilizando a busca sequencial.\n\nConsidere uma distribuição discreta com suporte \\(\\{1,2,\\ldots,k\\}\\). Considere que uma tabela com a função de distribuição está construída. O método da busca binária começa selecionando o ponto \\(j\\) no meio da tabela. Ao comparar \\(u\\) com \\(F(j)\\), temos duas situações:\n\nSe \\(u\\leq F(j)\\), então todos os valores maiores que \\(j\\) não podem ser gerados. O ponto que vai ser selecionado deve estar no conjunto \\(\\{1,\\ldots,j\\}\\)\nSe \\(u&gt; F(j)\\), então os valores \\(x\\leq j\\). Então, o ponto que vai ser selecionado deve estar no conjunto \\(\\{j+1,\\ldots,k\\}\\)\n\nDeste modo, uma nova busca é criada, considerando apenas os valores que podem ser simulados.\n\nAlgoritmo Busca Binária\nVariáveis do Algoritmo:\n\nlow: O índice mais baixo do nosso intervalo de busca atual.\nhigh: O índice mais alto do nosso intervalo de busca atual.\nmid: O índice do meio do intervalo de busca.\n\nPassos do Algoritmo (usando índices de 1 a k):\nInicialização:\n\nGere \\(U\\sim\\)Uniforme(0,1). Defina os limites da busca: low = 1, high = k.\nLoop de Busca: Continue o processo enquanto o intervalo de busca tiver mais de um elemento (low &lt; high):\n\n\nCalcular o meio: Encontre o índice do meio do intervalo atual (usando o piso para garantir um inteiro):\n\n\\[\\hbox{mid}=\\left\\lfloor \\frac{\\hbox{low}+\\hbox{high}}{2}\\right\\rfloor\\]\n\nA Comparação Precisa: Compare \\(U\\) com o valor de \\(F(\\hbox{mid})\\)\n\n\nSe \\(U&gt;F(\\hbox{mid})\\): Isso significa que \\(U\\) está na metade superior do espaço de probabilidade. Portanto, descartamos a metade inferior da busca, incluindo o meio.\n\n\\[\\hbox{low}=\\hbox{mid}+1\\] - Se \\(U\\leq F(\\hbox{mod})\\): Isso significa que \\(U\\) está na metade inferior do espaço de probabilidade. Portanto, descartamos a metade superior da busca, mas mantemos mid como um candidato potencial.\n\\[\\hbox{high}=\\hbox{mid}\\]\nTerminação: O loop termina quando low e high se encontram (low = high). Retorne \\(j=\\)low.\n\nÉ importante notar que uma das vantagens do R é a vetorização. Nesse sentido, a busca binária pode ser rapidamente implementada uma vez que \\(U\\) pode ser comparada simultaneamente com todos os valores de \\(F(x)\\). Essa verificação está implementada na função findInterval.\n\nExample 5.3 Considere novamente a função de distribuição\n\\[P(X=x)=\\frac{x}{10},\\] onde \\(x\\in\\{1,2,3,4\\}\\). A tabela de busca é\n\\[\\begin{array}{c|c|c}\\hline x & F(x) & \\hbox{Intervalo de busca}\\\\ \\hline 1  & \\frac{1}{10} & (0,\\frac{1}{10}] \\\\  2& \\frac{3}{10} & (\\frac{1}{10}, \\frac{3}{10}]\\\\ 3 & \\frac{6}{10} & (\\frac{3}{10},\\frac{6}{10}] \\\\ 4 & 1 & (\\frac{6}{10},1]  \\\\ \\hline  \\end{array}\\] Vamos guardar os pontos que delimitam o intervalo de busca no vetor abaixo:\n\nintervalos &lt;- c(0, .1, .3, .6, 1)\n\nVamos simular uma amostra de tamanho 5000 desta tabela utilizando a função findInterval.:\n\nn &lt;- 5000\nu &lt;- runif(n)\nx &lt;- findInterval(u, intervalos)\n\nPor último, vamos encontrar a tabela de frequências relativas e comparar com as probabilidades do modelo:\n\nfreq &lt;- prop.table( table(x) )\nplot(freq)\nlines( 1:4, (1:4)/10, col =2)\nlegend('topleft',c('P(X=x)','Frequência relativa'), col = c(2,1), lty=1, bty='n')\n\n\n\n\n\n\n\n\n\n\nExercício. Os dados a seguir mostram o número de meninos entre os 12 primeiros filhos de 6115 famílias com 13 filhos, retirados de registros hospitalares na Saxônia do século XIX.\n\\[\\begin{array}{l|cccccccccccc}\\hline\n\\hbox{Meninos} & 0& 1&  2&  3&  4&  5&  6&  7&  8&  9&  10& 11& 12\\\\\n\\hbox{Frequência}&  3&  24& 104&    286&    670&    1033&   1343&   1112&   829&    478&    181&    45& 7\\\\ \\hline\\end{array}\\]\nConsiderando que as frequências relativas são bos aproximações para a função de probabilidade, crie um simulador para essa distribuição.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Método da inversão</span>"
    ]
  },
  {
    "objectID": "inversao.html#inversão-numérica-de-f_x.-e-o-método-da-interpolação-em-tabelas",
    "href": "inversao.html#inversão-numérica-de-f_x.-e-o-método-da-interpolação-em-tabelas",
    "title": "5  Método da inversão",
    "section": "5.5 4. Inversão numérica de \\(F_x(.)\\) e o Método da Interpolação em Tabelas",
    "text": "5.5 4. Inversão numérica de \\(F_x(.)\\) e o Método da Interpolação em Tabelas\nSeja \\(X\\) uma variável aleatória contínua. Existem casos nos quais temos a expressão analítica para \\(F_X(.)\\) mas não para \\(F_X^{-1}(.)\\). Ainda assim, podemos utilizar o Método da Inversão da seguinte forma:\n\nGere \\(u\\sim\\hbox{Uniforme}(0,1)\\)\nEncontre numericamente o valor de \\(x\\) que resolve \\(u=F_X(x)\\)\n\nConforme discutido no Capítulo 1, a solução está sujeita a uma tolerância \\(\\varepsilon&gt;0\\) pré-especificada. Isso implica que qualquer valor \\(x\\) que satisfaça\n\\[|u-F_X(x)|&lt;\\varepsilon\\]\né uma solução.\n\nExercício\nConsidere a densidade \\[f_X(x)=\\frac{x\\sin(x)}{\\sin(1)-\\cos(1)},\\] com \\(x\\in(0,1)\\). Note que \\[F_X(x)=\\frac{\\sin(x)-x\\cos(x)}{\\sin(1)-\\cos(1)}\\] mas não há expressão para \\(F_X^{-1}(.)\\). Crie um gerador para essa distribuição utilizando o método da inversão e utilize-o para obter uma amostra de tamanho 50000. Monitore o tempo de execução utilizando a função Sys.time.\n\nO exercício acima mostra que o método da inversão, quando requer algoritmos de busca de raízes, pode ser computacionalmente intensivo. Uma alternativa mais eficiente combina a pesquisa em tabela (geralmente implementada com busca binária) e a interpolação, método conhecido como Interpolação em Tabela.\nSeja \\(X\\) uma variável aleatória contínua com valores no domínio \\(D\\). Assuma que, para um conjunto de pontos \\(\\{y_1,\\ldots,y_m\\}\\in D\\), a seguinte tabela está disponível\n\\[\\begin{array}{c|c|c}\\hline \\hbox{Índice} & \\hbox{x} & F(x)\\\\ \\hline\n1 & y_1 & F(y_1) \\\\\n2 & y_2 & F(y_2) \\\\\n\\vdots & \\vdots & \\vdots \\\\\nm & y_m & F(y_m)\\\\\\hline \\end{array}\\]\nO Método da Interpolação de Tabelas consiste em usar um algoritmo de busca para encontrar o índice \\(j\\in\\{1,\\ldots,m\\}\\) tal que \\(F(y_{j-1})&lt;u\\leq F(y_j)\\). Desde que \\(m\\) seja grande o suficiente, o intervalo \\([y_{j-1},y_j]\\) será pequeno e podemos utilizar uma interpolação para uma boa aproximação de \\(x\\) a partir de \\(u\\). As funções interpolantes mais simples são as lineares, embora polinômios de graus 2 ou 3 possam dar melhores ajustes. Para a interpolação linear, o valor simulado é a coordenada \\(x\\) sobre a reta que conecta \\((y_{j-1},F(y_{j-1}))\\) e \\((y_{j},F(y_{j}))\\), dado por\n\\[x=y_{j-1}+(y_j-y_{j-1})\\frac{u-F(y_{j-1})}{F(y_j)-F(y_{j-1})}.\\] Abaixo, ilustramos como \\(x\\) pode ser simulado por interpolação linear. Alinha sólida em preto mostra \\(F(x)\\) enquanto que a pontilhada em preto apresenta a interpolação linear. O valor real a ser simulado é \\(x^o\\) enquanto \\(x\\) é o valor simulado pela interpolação.\n\n\n\n\n\n\n\n\n\nA função rnorm utiliza uma tabela com precisão de 16 dígitos para simular amostras da distribuição normal (ver Wichura (1988)).\nO algoritmo base para o Método da Interpolação em Tabela é dado abaixo.\n\nMétodo da Interpolação em Tabela (interpolação linear)\nSejam \\(y_1,\\ldots,y_M\\) os valores tabelados e sejam \\(v_i=F_X(y_i)\\), para \\(i=1,\\ldots,M\\)\n\nGere \\(u\\sim\\hbox{Uniforme}(0,1)\\)\nEncontre \\(k\\) tal que \\(v_{k-1}&lt;u\\leq v_k\\)\nA partir da reta que passa em \\((y_{k-1},v_{k-1}),(y_k,v_k)\\), faça\n\n\\[x=y_k+(y_k-y_{k-1})\\frac{u-v_{k-1}}{v_k-v_{k-1}}\\]\n\n\nExemplo. Considere a função densidade\n\\[f(x)= 60x^3(1-x)^2,\\] com \\(x\\in(0,1)\\), cuja função distribuição é dada por \\[F(x)=15x^4-24x^5 + 10x^6.\\] Abaixo, vamos construir uma tabela para a progressão 0,01 até 0,99 com razão 0,01.\n\nFx &lt;- function(x) 15*x^4 -24*x^5 + 10*x^6\n\ny &lt;- c(0,seq(.01, .99, .01))\nÍndice = 1:length(y)\nv &lt;- Fx(y)\nhead( data.frame(Índice, y, v) ,6)\n\n  Índice    y            v\n1      1 0.00 0.000000e+00\n2      2 0.01 1.476100e-07\n3      3 0.02 2.323840e-06\n4      4 0.03 1.157409e-05\n5      5 0.04 3.598336e-05\n6      6 0.05 8.640625e-05\n\n\nAbaixo, simulamos \\(5000\\) valores dessa distribuição utilizando o método da interpolação de tabelas:\n\nn = 5000\nu &lt;- runif(5000)\n\n# pesquisando o indice na tabela\nj &lt;- findInterval(u, v)\n\n# interpolação\nx &lt;- y[j] + (y[j] - y[j-1])*( u- v[j-1])/( v[j] - v[j-1])\n\n# gráfico\nfx &lt;- function(x) 60*x^3*(1-x)^2\ncurve( fx(x), 0,1)\nlines(density(x), col =2)\nlegend(\"topleft\", legend = c(\"Amostra Simulada\", \"f(x) Teórica\"),\n   col = c(\"red\", \"black\"), lty = 1, lwd = c(1, 2), bty = 'n')\n\n\n\n\n\n\n\n\n\n\nExercício. A função densidade da distribuição coseno elevado é dada por\n\\[f(x)=\\frac{1}{2s}\\left[1+\\cos\\left(\\frac{x-\\mu}{s}\\pi\\right)\\right],\\] onde \\(x\\in(-s+\\mu,s+\\mu)\\). Pode-se mostrar que \\[Z=\\frac{X-\\mu}{s}\\] também tem distribuiçãoo coseno elevado, mas com \\(\\mu=0\\) e \\(s=1\\) (ou seja, \\(Z\\) é a coseno elevado padrão). Deste modo, precisamos apenas construir um simulador para \\(Z\\), uma vez que \\(X\\) pode ser simulado através do seguinte algoritmo:\n\nSimule \\(z\\) da distribuição coseno elevado padrão\nFaça \\(x=\\mu+sz\\).\n\nConsiderando que a função distribuição de \\(Z\\) é dada por\n\\[F(z)=\\frac{1}{2}\\left[1+z+\\frac{\\sin(\\pi z)}{\\pi}\\right],\\] com \\(z\\in(-1,1)\\), construa um simulador para a distribuição coseno elevado padrão utilizando o método da interpolação em tabelas. Gere uma amostra de tamanho 50.000 e verifique se a amostra gerada se comporta como o esperado (utilize estatísticas como histograma, função de distribuição empírica ou o eestimador kernel).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Método da inversão</span>"
    ]
  },
  {
    "objectID": "inversao.html#gerando-vetores-aleatórios",
    "href": "inversao.html#gerando-vetores-aleatórios",
    "title": "5  Método da inversão",
    "section": "5.6 5. Gerando vetores aleatórios",
    "text": "5.6 5. Gerando vetores aleatórios\nNão é possível aplicar o Método da Inversão diretamente em vetores aleatórios. Contudo, o método é válido para distribuições condicionais univariadas, pois\n\\[F_{X|y}(x)=u\\Leftrightarrow x=F_{X|y}^{-1}(u).\\]\nDeste modo, geramos uma amostra do vetor \\({\\bf X}\\) do seguinte modo:\n\nGere \\(u_1,\\ldots,u_d\\sim \\hbox{Uniformes}(0,1)\\) independentes\nFaça \\(x_1=F^{-1}_{X_1}(u_1)\\)\nFaça \\(x_2=F^{-1}_{X_2|x_1}(u_2)\\)\n\n\nFaça \\(x_d=F^{-1}_{X_d|x_1,\\ldots,x_{d-1}}(u_d)\\)\n\n\nO algoritmo acima funciona porque ele imita proceduralmente a regra da cadeia da probabilidade. Cada variável no vetor é gerada de sua respectiva distribuição condicional, dados os valores de todas as variáveis que a precederam. Ao encadear esses valores, o vetor resultante é, por construção, uma amostra da distribuição conjunta desejada.\nExemplo 2.6  Seja \\((X,Y)\\) um vetor aleatório uniformemente distribuído no triângulo \\(\\Delta\\), definido pelos vértices \\(\\{(0,0),(1,0),(1,1)\\}\\), cuja densidade é\n\\[f_{X,Y}(x,y)=2I((x,y)\\in\\Delta).\\] Teremos que\n\\[f_Y(y)=\\int_y^{1}2dx=2(1-y)\\] com \\(y\\in(0,1).\\) Além disso, \\[F_Y(y)=\\int_0^y 2(1-t)dt=2y-y^2\\] e \\(y\\in(0,1)\\). Portanto, podemos gerar \\(Y\\) através do Método da Inversão\n\nSimule \\(u\\sim\\hbox{Uniforme}(0,1)\\)\nFaça \\(y=1-\\sqrt{1-u}\\)\n\nAgora, note que\n\\[f(x|y)=\\frac{f_{X,Y}(x,y)}{f_Y(y)}=\\frac{2}{2(1-y)}=\\frac{1}{1-y},\\] com \\(x\\in(y,1)\\), ou seja \\(X|Y=y\\sim\\hbox{Uniforme}(y,1)\\). Então, podemos gerar \\((x,y)\\) com o seguinte algoritmo:\n\nGere \\(u\\sim\\hbox{Uniforme}(0,1)\\)\nFaça \\(y = 1-\\sqrt{1-u}\\)\nGere \\(x\\sim\\hbox{Uniforme}(y,1)\\)\n\nAbaixo, geramos uma amostra de tamanho 5000 dessa distribuição e mostramos um diagrama de dispersão.\n\nu &lt;- runif(5000)\ny &lt;- 1 - sqrt(1-u)\nx &lt;- runif(5000, y, 1)\n\nplot(x,y)\n\n\n\n\n\n\n\n\n\n\nExercício. Seja \\[P(X=x)=\\frac{|x|+1}{4}\\] e \\[f(y|x)=\\frac{1}{2s}\\left[1+\\cos\\left(\\frac{y-x}{s}\\pi\\right)\\right]\\] com \\(y\\in(-s+x,s+x)\\). Para \\(s=1\\), simule uma amostra de tamanho 5.000 do vetor \\((X,Y)\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Método da inversão</span>"
    ]
  },
  {
    "objectID": "inversao.html#referências",
    "href": "inversao.html#referências",
    "title": "5  Método da inversão",
    "section": "5.7 Referências",
    "text": "5.7 Referências\n\nGeração eficiente de binomiais\n\nKachitvichyanukul, V. and Schmeiser, B. W. (1988) Binomial random variate generation. Communications of the ACM, 31, 216–222.\n\nGerador de números gaussianos usando o método da inversão por interpolação\n\nWichura, M. J. (1988) Algorithm AS 241: The percentage points of the normal distribution. Applied Statistics, 37, 477–484.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Método da inversão</span>"
    ]
  },
  {
    "objectID": "uniforme_multivariada.html",
    "href": "uniforme_multivariada.html",
    "title": "6  Método da uniforme multivariada com marginais não uniformes",
    "section": "",
    "text": "6.1 Funções indicadoras\nNesta aula vamos discutir como simular \\(X\\) a partir de uma distribuição uniforme multivariada. Esse método é a base para do método da aceitação/rejeição que será discutido posteriormente.\nSeja \\(S\\subset \\mathbb{R}^d\\). A função \\(I_S:\\mathbb{R}^d\\rightarrow\\{0,1\\}\\), dada por\n\\[I_S(x)=\\left\\{\\begin{array}{ll}1,&x\\in S\\\\ 0,& x\\in S^c\\end{array}\\right.\\] é denominada função indicadora de \\(S\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Método da uniforme multivariada com marginais não uniformes</span>"
    ]
  },
  {
    "objectID": "uniforme_multivariada.html#funções-indicadoras",
    "href": "uniforme_multivariada.html#funções-indicadoras",
    "title": "6  Método da uniforme multivariada com marginais não uniformes",
    "section": "",
    "text": "Propriedades\n\n\\(I_A(x)I_B(x)=I_{A\\cap B}(x)\\)\n\\(\\int_A f(x)dx=\\int_{\\mathbb{R}^d}I_A(x)f(x)dx\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Método da uniforme multivariada com marginais não uniformes</span>"
    ]
  },
  {
    "objectID": "uniforme_multivariada.html#distribuição-uniforme-multivariada",
    "href": "uniforme_multivariada.html#distribuição-uniforme-multivariada",
    "title": "6  Método da uniforme multivariada com marginais não uniformes",
    "section": "6.2 Distribuição uniforme multivariada",
    "text": "6.2 Distribuição uniforme multivariada\nSeja \\(S\\subset \\mathbb{R}^d\\) satisfazendo\n\\[0&lt;\\int_{\\mathbb{R}^d} I_S(x)dx&lt;\\infty.\\] A integral acima é denominada volume de \\(S\\) e é denotada por Vol(\\(S\\)).\nDizemos que o vetor \\(\\boldsymbol{X}\\) tem distribuição uniforme multivariada no conjunto \\(S\\) se sua densidade é dada por\n\\[f(\\boldsymbol{x})=Vol(S)^{-1}I_S(\\boldsymbol{x}).\\]\nNeste caso, dizemos que \\(\\boldsymbol{X}\\sim\\)Uniforme(\\(S\\)).\n\nLema Seja \\(V\\subset S\\). Se \\(\\boldsymbol{X}\\sim\\)Uniforme(\\(S\\)), então \\(\\boldsymbol{X}|\\boldsymbol{X}\\in V\\sim\\)Uniforme(\\(V\\)).\n\nConsidere que é fácil simular pontos uniformemente distribuídos em \\(S\\). Então, podemos simular pontos de uma região mais complicada \\(V\\subset S\\) com o seguinte algoritmo:\n\nAlgoritmo.\n\nSimule \\(u\\sim\\)Uniforme(\\(S\\))\nSe \\(u\\in V\\), faça \\(x=u\\). Senão, volte ao passo 1.\n\n\n\nExample 6.1 Vamos ilustrar o algoritmo acima simulando pontos uniformemente distribuídos dentro da circunferência de centro (1/2,1/2) e raio 1/2, isto é\n\\[f(\\boldsymbol{x})=\\frac{4}{\\pi}I_V(\\boldsymbol{x}),\\] com \\[V=\\{x\\in \\mathbb{R}^2:(x_1−1/2)^2+(x_2−1/2)^2&lt;1/4\\}.\\]\nNotemos que \\(V\\subset S\\), onde\n\\[S=\\{\\boldsymbol{x}\\in\\mathbb{R}^2:x_i∈(0,1),i=1,2\\}\\] Notemos ainda que, se \\(U_1,U_2\\sim\\)Uniforme(0,1) são independentes, então a densidade do vetor \\(\\boldsymbol{U}=(U_1,U_2)\\) é dada por\n\\[f_\\boldsymbol{U}(\\boldsymbol{u})=f_{U_1}(u_1)f_{U_2}(u_2)=I_{(0,1)}(u_1)I_{(0,1)}(u_2)=I_S(\\boldsymbol{u})\\]\nEntão, podemos simular \\(\\boldsymbol{X}\\)∼Uniforme(\\(V\\)) com o seguinte algoritmo:\n\nr_uniformeV &lt;- function(){\n  \n  x &lt;- c(NA,NA)\n  \n  while( is.na( x[1]) ){\n    # gerando um ponto de S\n    u &lt;- runif(2)\n    \n    # verificando o ponto está em V\n    if( sum( (u -.5)^2) &lt; 1/4){\n      x &lt;- u\n    }\n  }\n  return(x)\n}  \n\nVamos gerar 1000 pontos uniformes em \\(V\\):\n\nx &lt;- sapply( 1:1000, function(x) r_uniformeV() )\noo &lt;- par( pty = \"s\" )\n  plot(x[1,],x[2,] , xlab= \"x1\", ylab=\"x2\")\n\n\n\n\n\n\n\npar(oo)  \n\n\nNote que o algoritmo dado simula um candidato no passo 1 e decide se aceita ou não esse candidato no passo 2. A probabilidade do candidato ser aceito é\n\\[P(\\boldsymbol{X}\\in   V)=\\frac{1}{Vol(S)}\\int_V I_S(x)d\\boldsymbol{x}=\\frac{Vol(V)}{Vol(S)}\\]\nDeste modo, se \\(T\\) é o número de tentativas até a geração de \\(X\\), teremos que \\(T\\sim \\hbox{Geométrica}(P(\\boldsymbol{X}\\in V))\\) e o número médio de tentativas é dado por \\(1/P(X\\in V)\\).\nNo Exemplo 1, \\(P(\\boldsymbol{X}\\in V)=\\pi/4\\approx 0,78\\). O simulador precisa de 1,27 tentativas em média para gerar um vetor da Uniforme(\\(V\\)).\n\nExercício. Considere o conjunto\n\\[V=\\{(x,y)\\in[-3,3]^2:|y|&lt;e^{-|x|}\\} \\]\nSimule uma amostra de tamanho 5000 de pontos uniformes em \\(V\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Método da uniforme multivariada com marginais não uniformes</span>"
    ]
  },
  {
    "objectID": "uniforme_multivariada.html#método-da-uniforme-multivariada-com-marginais-não-uniformes",
    "href": "uniforme_multivariada.html#método-da-uniforme-multivariada-com-marginais-não-uniformes",
    "title": "6  Método da uniforme multivariada com marginais não uniformes",
    "section": "6.3 Método da uniforme multivariada com marginais não uniformes",
    "text": "6.3 Método da uniforme multivariada com marginais não uniformes\nSuponha que desejamos simular o vetor \\(\\boldsymbol{X}\\subset \\mathbb{R}^d\\) com densidade \\(f(\\boldsymbol{x})\\). Considere o conjunto\n\\[V=\\{(\\boldsymbol{x},u):x\\in\\mathbb{R}^d,0&lt;u&lt;f(\\boldsymbol{x})\\}.\\] Note que\n\\[Vol(V)=\\int_{\\mathbb{R}^d}\\int_{0}^{f(\\boldsymbol{x})}1dudx=\\int_{\\mathbb{R}^d}f(\\boldsymbol{x})d\\boldsymbol{x}=1\\]\nSeja \\((\\boldsymbol{X},U)\\) um vetor aleatório uniformemente distribuído em \\(V\\), com densidade dada por\n\\[f_{\\boldsymbol{X},U}(\\boldsymbol{x},u)=Vol(V)^{-1}I_V(\\boldsymbol{x},u)=I_V(\\boldsymbol{x},u).\\] Notemos que\n\\[\\int_{0}^{f(\\boldsymbol{x})}f_{\\boldsymbol{X},U}(x,u)du=\\int_0^{f(\\boldsymbol{x})}du=f(\\boldsymbol{x}).\\] então, a distribuição marginal de \\(X\\) tem densidade \\(f(.)\\). Então, se é fácil simular \\((\\boldsymbol{X},U)\\)∼Uniforme(\\(S\\)), com \\(V\\subset S\\), podemos simular \\(\\boldsymbol{X}\\) com o seguinte algoritmo:\n\nMétodo da uniforme multivariada com marginais não uniformes\n\nGere \\((y,u)\\sim\\)Uniforme(\\(S\\))\nSe \\((y,u)\\in V\\), faça \\(x=y\\). Senão, retorne ao passo 1.\n\n\nEm geral, escolhemos uma região \\(S\\) simples, como um hipercubo, pois o passo 1 do algoritmo acima é resolvido a partir da simulação de uniformes univariadas independentes.\n\nExample 6.2 Considere o problema de simular \\(X\\), com densidade dada por\n\\[f(x)=60x^3(1−x)^2,\\]\ncom \\(x\\in(0,1)\\). Seja\n\\[V=\\{(x,u):0&lt;x&lt;1,0&lt;u&lt;60x^3(1−x)^2\\}.\\]\nVamos definir \\[S=\\{(x,u):0&lt;x&lt;1,0&lt;u&lt;M\\},\\]\nonde \\(M\\) é o valor máximo de \\(f(.)\\). Embora encontrar \\(M\\) seja um exercício simples de cálculo, vamos realizar essa tarefa numericamente, utilizando a função optim, pelo método de Brent:\n\n# densidade de interesse\nf &lt;- function(x) 60* x^3 * (1-x)^2\n# menos a densidade de interesse\nmf &lt;- function(x) -f(x)\n\n\n# encontrando o mínimo local\noptim( .5, mf, lower = 0, upper =1, method = 'Brent')\n\n$par\n[1] 0.6\n\n$value\n[1] -2.0736\n\n$counts\nfunction gradient \n      NA       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nPortanto, tomaremos \\(S\\) como\n\\[S=\\{(x,u):0&lt;x&lt;1,0&lt;u&lt;2,0736\\}\\]\nonde pode-se constatar que \\(V\\subset S\\). A função abaixo gera uma observação de \\(X\\):\n\nrf &lt;- function(){\n  x &lt;- NA\n  while( is.na(x) == T){\n    # passo 1: gerando de (y,u)~Uniforme(S)\n      y &lt;- runif(1)\n      u &lt;- runif(1, 0, 2.0736)\n      \n    # passo 2: testando o candidado\n      if( u &lt; f(y)){ x &lt;- y}\n  }\n  return(x)\n}\n\nAbaixo simulamos uma amostra de tamanho 1.000 dessa distribuição\n\nx &lt;- sapply( 1:1000, function(x) rf() )\n\n# histograma dos valores simulados de X\nhist(x, freq = FALSE, main = \"\" ,ylim=c(0,2.0736), xlab = \"x\")\ncurve(f(x), add = T, lwd = 2)\n\n\n\n\n\n\n\n\n\n\nExercício Utilize o método da uniforme multivariada com marginais não uniformes para obter um simulador para a função densidade\n\\[f(x)=\\frac{1}{\\pi^2}x\\cos(x)^2,\\] onde \\(x(0,2\\pi)\\). Implemente o simulador e obtenha uma amostra de tamanho 5.000.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Método da uniforme multivariada com marginais não uniformes</span>"
    ]
  },
  {
    "objectID": "provas.html",
    "href": "provas.html",
    "title": "7  Provas",
    "section": "",
    "text": "Todas as provas possuem as seguintes regras:\n\nNão é permitido o uso de dispositivos eletrônicos\nNão é permitido deixar a sala de aula sem entregar a prova\n\n\n8 Revisão para a Prova 1\n\nQuestão 1. Considere o problema de encontrar a raiz de \\(f(x)\\) no intervalo \\([a,b]\\). Explique porque ter \\(f(a)f(b)&lt;0\\) não é suficiente para garantir que há uma única raiz no intervalo \\([a,b]\\). Sugestão. Esboce um gráfico de uma função periódica, como \\(\\sin(x)\\).\n\n\nQuestão 2. Seja \\(f(x)=x^2-4\\). Considerando o problema de encontra a raiz de \\(f(x)\\) e o ponto inicial \\(x_0=1\\), faça uma iteração do método de Newton-Raphson.\n\n\nQuestão 3. Prove que nem todo minimizador local é um minimizador global.Sugestão. Você pode esboçar um gráfico para criar seu argumento.\n\n\nQuestão 4. Seja \\(f(x)=x^4-1\\). Considerando o problema de minimização de \\(f(x)\\), o ponto inicial \\(x_0=1\\) e a taxa de aprendizagem \\(\\alpha=0,1\\), faça uma iteração do método do gradiente e retorne o valor de \\(x_1\\).\n\n\nQuestão 5. Considere a função \\(f(x,y)=x^2+y^2\\). Começando com com o simplex \\((1,1),(2,1),(2,3)\\), faça uma iteração do método de Nelder Mead e apresente o novo simplex.\n\n\nQuestão 6. Aproxime a integral \\[\\int_0^1 x^2dx\\] utilizando a regra de Newton-Cotes, considerando dois subintervalos e a regra do ponto médio.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Provas</span>"
    ]
  }
]